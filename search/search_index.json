{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Welcome to my new blog, which is generated via MkDocs extended with Material-MkDocs Theme. My previous short-lived blog is built on jekyll but it took too much effort for configuring and maintaining.</p>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/","title":"Automate Blog","text":"<p>About how my blog site is built and future roadmap </p> <p>Jekyll</p> <p>mkdocs</p> <p>What about this theme</p> <p>https://mmclassification.readthedocs.io/en/latest/</p>"},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/","title":"Boardgame Collection","text":""},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/#from-kickstarter","title":"From Kickstarter","text":""},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/#euthia-resurrected","title":"Euthia Resurrected :","text":"<ul> <li>https://www.kickstarter.com/projects/steamforged/euthia-resurrected</li> <li>Expected delivery - Dec 2023</li> </ul>"},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/#monster-hunter-world-the-board-game","title":"Monster Hunter World: The Board Game:","text":"<ul> <li>https://www.kickstarter.com/projects/steamforged/monster-hunter-world-the-board-game</li> </ul>"},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/#sheol","title":"Sheol","text":"<ul> <li>https://www.kickstarter.com/projects/lunaroakstudio/sheol-board-game</li> </ul> <p>Destinies</p>"},{"location":"Boardgame%20Collection%20040c01d316a441e686288c7d5e720aca/#-httpsboardgamegeekcomboardgame285192destinies","title":"- https://boardgamegeek.com/boardgame/285192/destinies","text":"<p>To buy</p> <ul> <li> <p>Final girl</p> <p></p> </li> <li> <p>ISS *Vanguard (Bought)*</p> <ul> <li> <p>https://boardgamegeek.com/boardgame/325494/iss-vanguard</p> <p></p> </li> <li> <p>https://gamefound.com/projects/awaken-realms/iss-vanguard#/section/project-story</p> </li> <li>Commander\u2019s - $179</li> <li>Commander\u2019s pledge sundrop - $238</li> <li>All in (4 box) - $249</li> <li>All in (4 box) sundrop - $324</li> <li>Real ALL in sundrop - $535</li> <li>DLC<ul> <li>Deadly Frontier campaign (sundrop) - $75</li> <li>Section boxes (sundrop) - $45</li> <li>ISS Vanguard Dice Upgrade - $25</li> </ul> </li> <li>Mage Knight</li> </ul> </li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/","title":"Farming","text":"<p>\u571f\u4f5c\u308a</p> <p>\u6c34\u3084\u308a</p> <p>Plant Treatment by types</p>"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/","title":"Lang","text":"<p>JaPaNeSe</p> <p>English</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/","title":"Lesson","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/#note","title":"Note:","text":"<p>This section is for temporarily storing lesson notes. Afterwards, the notes should be turned into guides in other sections. </p> <p>NLP Book</p> <p>Google ML Bootcamp</p> <p>Coursera - Andrew Ng ML</p> <p>AWS MLU - ML university</p> <p>Google cloud skill boost</p> <p>Daily Bnomial</p> <p>Kaggle</p> <p>DL free course</p> <p>AWS innovate - Data edition 2022</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/","title":"Tech","text":"<p>A large collection of technology notes including ML, Programming, AWS, Networking\u2026etc</p> <p>ML</p> <p>Python</p> <p>AWS</p> <p>Heroku Hosting</p> <p>Network</p> <p>Docker</p> <p>Git</p> <p>Node / JS</p> <p>Mac</p> <p>Certificate</p> <p>Linux/Unix</p>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/","title":"To Read","text":""},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/#notes","title":"Notes:","text":"<p>After Reading, the materials should move to other category </p> <p>Pending material</p> <p>Daily Check</p> <p>Research Paper</p> <p>Reading Material</p>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/Jekyll%209f478d44e1a34021a146c6a1515900aa/","title":"Jekyll","text":""},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/Jekyll%209f478d44e1a34021a146c6a1515900aa/#ref","title":"Ref","text":"<p>https://aymanbagabas.com/blog/2022/03/29/import-notion-pages-to-jekyll.html</p> <p>https://github.com/echo724/notion2jekyll</p>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/Jekyll%209f478d44e1a34021a146c6a1515900aa/#solution","title":"Solution","text":"<ul> <li>~~Exporting into HTML seems better~~<ul> <li>~~No need jekyll at all\u2026~~</li> <li>~~But cannot control anything, e.g. style, common header\u2026etc, nothing~~</li> </ul> </li> <li>Exporting manually to md &amp; script to fix links problem<ul> <li>Need to do manually</li> <li>Make script to search links and replace<ul> <li>Easy way:<ul> <li>Get all md file names,</li> <li>Search in all md for these file names, replace as decoded string (e.g. %20 \u2192 space)</li> </ul> </li> <li>~~use custom plugin like this https://github.com/benbalter/jekyll-relative-links/blob/main/lib/jekyll-relative-links/generator.rb~~</li> </ul> </li> </ul> </li> <li>~~Exporting via API programmatically~~<ul> <li>~~Official API cannot do export~~</li> <li>~~can try this~~<ul> <li>~~https://github.com/yannbolliger/notion-exporter/blob/master/src/NotionExporter.ts~~</li> </ul> </li> </ul> </li> </ul>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/Jekyll%209f478d44e1a34021a146c6a1515900aa/#next-task","title":"Next task","text":"<ul> <li>[x]  Try to use this theme instead, it looks like real tech doc, e.g. iceVision<ul> <li>https://github.com/squidfunk/mkdocs-material</li> <li>[ ]  Solve the unique string filename problem</li> </ul> </li> <li>[ ]  Simplify export &amp; unzip &amp; moving files step, by using maybe https://github.com/yannbolliger/notion-exporter/blob/master/src/NotionExporter.ts</li> <li>[x]  Beautify theme, nav bar\u2026etc<ul> <li>List - https://jamstackthemes.dev/#ssg=jekyll</li> <li>Company feel - https://jamstackthemes.dev/demo/theme/jekyll-serif/</li> <li>Left pic side bar<ul> <li>Large - https://jamstackthemes.dev/demo/theme/not-pure-poole/</li> <li>Minimalist - https://jamstackthemes.dev/demo/theme/jekyll-mr-green/</li> <li>Broken\u2026.https://github.com/zivong/jekyll-theme-hydure</li> </ul> </li> <li>Tailwind - https://github.com/zerostaticthemes/jekyll-atlantic-theme</li> <li>Dark minimalist - https://pages-themes.github.io/midnight/</li> </ul> </li> <li>[x]  Breadcrum</li> <li>[ ]  Use the floating outline from post layout<ul> <li>Make it on the right side</li> </ul> </li> <li>Note list<ul> <li>[ ]  Each md to be 1 white block, instead of merging all together</li> </ul> </li> <li>[ ]  Complete self intro</li> <li>[ ]  Code styling by languages</li> <li>[ ]  Resume (by jekyll resume theme or just pdf ?<ul> <li>[ ]  https://github.com/jekyller/PanelCV</li> <li>[ ]  https://jamstackthemes.dev/theme/jekyll-online-cv/</li> </ul> </li> <li>[ ]  Check how to write blog on notion \u2192 import to jekyll easily (with date)</li> </ul>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/mkdocs%20a83cd2377d464c2eb3bf8d9a0e40b568/","title":"mkdocs","text":""},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/mkdocs%20a83cd2377d464c2eb3bf8d9a0e40b568/#to-do","title":"TO-DO","text":"<ul> <li>Left side-bar<ul> <li>[x]  Decide whether use <code>navigation.tabs</code> or not</li> <li>[x]  Show every items under selected tab<ul> <li>The tab is actually a page, different with the folder, need to merge them</li> </ul> </li> </ul> </li> <li>[x]  Right side TOC - cannot detect Heading level 1<ul> <li>[ ]  Enhance - always make the 1st level 1 header as most-top header, and other level 1 header with a tab</li> </ul> </li> <li>[ ]  Complete config of mkdocs</li> <li>[ ]  Make a blog posts list layout like this - https://www.dirigible.io/blogs/2021/11/2/material-blogging-capabilities/<ul> <li>[ ]  https://github.com/vuquangtrong/mkdocs-material-blog/blob/main/overrides/partials/post-list.html</li> </ul> </li> </ul>"},{"location":"Automate%20Blog%20f771cc000fb149438376da8ea2aaf906/mkdocs%20a83cd2377d464c2eb3bf8d9a0e40b568/#code-snippets","title":"Code snippets","text":"<pre><code>{% set split_title_list = nav_item.title.split(\" \") %}\n{% set split_title_last_code = split_title_list | last %}\n{% if split_title_last_code|length == 32 %}\n&lt;div&gt;{{nav_item.title | replace(\" \" ~ split_title_last_code, \"\")}}&lt;/div&gt;\n</code></pre>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/","title":"Plant Treatment by types","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_1","title":"\u30df\u30cb\u30c8\u30de\u30c8","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_2","title":"\u6574\u679d\u3000\u305b\u3044\u3057","text":"<p>Target: grow 2 branches from 1 root</p> <ul> <li>Starting from the root, find the 1st flower vine(\u30c4\u30eb).</li> <li>Under this flower vine, mark the 1st \u308f\u304d\u82bd, it will become the 2nd branch to grow tomato.</li> <li>All other \u308f\u304d\u82bd should be cut. (except flower for sure)</li> </ul> <p></p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_3","title":"\u6458\u5fc3 \u3066\u304d\u3057\u3093","text":"<p>If the branch grow up to too high, we need to cut the main branches, in order to keep the nutrition for tomato. </p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_4","title":"\u304d\u3085\u3046\u308a","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_5","title":"\u6574\u679d","text":"<p>Target - keep 1 main branch only (But allow \u308f\u304d\u82bd to grow 1-2 cucumber)</p> <p></p> <p>If cucumber is super curved, it could be a signal of not enough water or \u80a5\u6599. If take no action, the plant could die. </p> <ul> <li>Cut the 1st to 5th \u308f\u304d\u82bd from the bottom. It is to improve the branches growth and cucumber growth.</li> <li>The 1st or 2nd cucumber(after cutting 5   \u308f\u304d\u82bd) should be cut as early as possible. Otherwise it prevents the branches growing up.</li> <li>Cut the main branch IF it grow up until higher than human.</li> <li>For all \u5b50\u30c5\u30eb, cut the head while keeping 2 leafs.</li> </ul> <p></p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_6","title":"\u80a5\u6599","text":"<p>2 weeks after planting, once every 2 weeks.</p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_7","title":"\u306a\u3059\u3073","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_8","title":"\u80a5\u6599\u30c1\u30a7\u30c3\u30af","text":"<p>Point: if water or \u80a5\u6599 is not enough, it could die easily. </p> <p>I.e. \u80a5\u6599\u3092\u7d76\u5bfe\u5207\u308c\u3055\u305b\u306a\u3044</p> <p></p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_9","title":"\u6574\u679d","text":"<p>Initial target : keep 3 branches</p> <ul> <li>From the 1st flower, keep 1st \u308f\u304d\u82bd above it AND below it.</li> <li> <p>Cut all \u308f\u304d\u82bd under those.</p> <p></p> </li> <li> <p>1st and 2nd eggplant should be cut a bit early than usual. To keep the nutrition.</p> </li> </ul> <p>When main branches grow up:</p> <p>\u6574\u679d\u306f\u53ce\u7a6b\u3068\u95a2\u9023\u3055\u305b\u306a\u304c\u3089\u884c\u3044\u307e\u3059\u3002\u307e\u305a\u3001\u82b1\u304c\u307e\u3060\u857e\u306e\u3068\u304d\u306b\u5074\u679d\u306e\u82b1\u306e\u4e0a\u306e\u8449\u30921\u679a\u6b8b\u3057\u3066\u6458\u82af\u3057\u307e\u3059\u3002\u3053\u306e\u82b1\u304c\u679c\u5b9f\u306b\u306a\u3063\u3066\u53ce\u7a6b\u3057\u305f\u5f8c\u3001\u57fa\u90e82\u82bd\u306e\u3046\u3061\u5f37\u3044\u82bd\u306e\u4e0a\u3067\u5207\u308a\u623b\u3057\u307e\u3059\u3002\u305d\u306e\u82bd\u304c\u5927\u304d\u304f\u306a\u3063\u3066\u857e\u304c\u898b\u3048\u3066\u304d\u305f\u3089\u3001\u524d\u56de\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u857e\u306e\u4e0a\u306e\u8449\u30921\u679a\u6b8b\u3057\u3066\u6458\u82af\u3057\u307e\u3059\u3002\u679c\u5b9f\u306e\u53ce\u7a6b\u3054\u3068\u306b\u3053\u308c\u3089\u306e\u4f5c\u696d\u3092\u7e70\u308a\u8fd4\u3057\u307e\u3059\u3002\u751f\u80b2\u304c\u3088\u304f\u3001\u4e3b\u679d\u304c\u80cc\u4e08\u3088\u308a\u3082\u9ad8\u304f\u306a\u308b\u3088\u3046\u3067\u3042\u308c\u3070\u81ea\u5206\u306e\u624b\u306e\u5c4a\u304f\u7bc4\u56f2\u3067\u6458\u82af\u3092\u884c\u3044\u307e\u3059\u3002 (Source here) </p> <p></p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_10","title":"\u30d4\u30fc\u30de\u30f3","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_11","title":"\u80a5\u6599","text":"<p>Don't make it lack of \u80a5\u6599!</p> <p>Refill \u80a5\u6599 every 2 weeks</p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_12","title":"\u6574\u679d","text":"<p>Target: grow 3 branches. </p> <ul> <li>Cut all the \u308f\u304d\u82bd\u3001except 1 branches  above and under the 1st \u30d4\u30fc\u30de\u30f3.</li> <li> <p>Cut those branches that face toward inside, to keep enough air flow.</p> <p></p> <p></p> </li> <li> <p>Also cut the 2nd or 3rd flower for growing up better later</p> <ul> <li>Src: https://sakata-tsushin.com/yomimono/tokushu/20170223_004938.html</li> </ul> <p></p> </li> <li> <p>If the 3rd / 4th branches are blocking air &amp; sunlight in the middle, cut them:</p> <p>\u7b2c\u4e09\u5206\u679d\u304b\u3089\u767a\u751f\u3059\u308b\u5074\u679d\u306f\u65e9\u3081\u306b\u53d6\u308a\u9664\u304d\u307e\u3059\u3002\u3053\u306e\u5074\u679d\u306f\u61d0\u679d\u3068\u547c\u3070\u308c\u3001\u682a\u306e\u4e2d\u592e\u306b\u5f37\u304f\u4f38\u3073\u3001\u63a1\u5149\u6027\u3092\u60aa\u304f\u3057\u305f\u308a\u3001\u98a8\u901a\u3057\u3092\u59a8\u3052\u3066\u75c5\u6c17\u3092\u52a9\u9577\u3057\u305f\u308a\u3057\u307e\u3059\u3002\u3042\u308b\u7a0b\u5ea6\u751f\u80b2\u3057\u3001\u5074\u679d\u304c\u5f37\u304f\u9577\u304f\u4f38\u3073\u3066\u3057\u307e\u3046\u5834\u5408\u306f\u30012\uff5e3\u7bc0\u3067\u6458\u82af\u3057\u3066\u3082\u3088\u3044\u3067\u3059\u3002\u305f\u3060\u3057\u3001\u904e\u5ea6\u306e\u6458\u82af\u306f\u8449\u679a\u6570\u306e\u4f4e\u4e0b\u3092\u62db\u304d\u8349\u52e2\u304c\u8870\u3048\u3066\u3057\u307e\u3063\u305f\u308a\u3001\u65e5\u713c\u3051\u679c\u3092\u52a9\u9577\u3055\u305b\u305f\u308a\u3059\u308b\u306e\u3067\u6ce8\u610f\u304c\u5fc5\u8981\u3067\u3059. (Source here) </p> <p></p> </li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_13","title":"\u30b9\u30a4\u30ab","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_14","title":"\u6574\u679d","text":"<ul> <li>Cute the main branch when main leafs become more than 6, i.e. 6 leafs remain.</li> <li>Then, only keep 4 branches with the best condition. Cut all other branches of main branch.</li> <li>Target : keep totally 2 watermelon among 4 branches.</li> <li>Cut 1st and 2nd watermelon before growing up.</li> <li>Like the image below, grow between 16- 22</li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_15","title":"\u30aa\u30af\u30e9","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_16","title":"\u80a5\u6599","text":"<ul> <li>\u690d\u3048\u4ed8\u305120\u65e5\u5f8c\u304b\u30892\u301c3\u9031\u9593\u306b1\u56de\u306e\u30da\u30fc\u30b9\u3067\u8ffd\u80a5 \u3057\u307e\u3059\u3002\u755d\u306e\u80a9\u304b\u682a\u9593\u306b\u307e\u3044\u3066\u3044\u304d\u307e\u3059\u3002</li> <li>This pic shows the unhealthy leaf \u2192 \u8ffd\u80a5\u3059\u308b\uff01 (image ref)</li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_17","title":"\u6574\u679d","text":"<ul> <li> <p>Depends on the growth.</p> <p></p> </li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/Plant%20Treatment%20by%20types%204cbba7f9c6ce4e75ab479a37ceff9426/#_18","title":"\u53ce\u7a6b","text":"<p>Special!!!</p> <ul> <li> <p>1\u672c\u53ce\u7a6b\u3059\u308b\u3054\u3068\u306b\u30012\u6bb5\u4e0b\u306e\u8449\u3092\u5207\u308a\u53d6\u308a\u307e\u3059\u3002</p> <p></p> </li> </ul>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E5%9C%9F%E4%BD%9C%E3%82%8A%2010c03deb8084452abac04b1468fb2d02/","title":"\u571f\u4f5c\u308a","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E5%9C%9F%E4%BD%9C%E3%82%8A%2010c03deb8084452abac04b1468fb2d02/#_2","title":"\u77f3\u7070\u3000\u305b\u3063\u304b\u3044","text":"<p>\u9178\u6027\u571f\u58cc\uff08\u3069\u3058\u3087\u3046\uff09\u3092\u4e2d\u548c\u3059\u308b</p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E5%9C%9F%E4%BD%9C%E3%82%8A%2010c03deb8084452abac04b1468fb2d02/#_3","title":"\u5806\u80a5\u3000\u305f\u3044\u3072","text":"<p>\u8150\u8449\u571f\uff08\u3075\u3088\u3046\u3069\uff09\u306a\u3069\u306e\u767a\u9175\u3067\u3001</p> <p>\u571f\u3092\u30d5\u30ab\u30d5\u30ab\u306b\u3059\u308b</p> <p>Help to keep air &amp; water around root</p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E5%9C%9F%E4%BD%9C%E3%82%8A%2010c03deb8084452abac04b1468fb2d02/#_4","title":"\u80a5\u6599","text":"<p>\u6709\u6a5f\u8cea\u3000vs   \u7121\u6a5f\u8cea</p> <p>organic  vs inorganic</p>"},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E5%9C%9F%E4%BD%9C%E3%82%8A%2010c03deb8084452abac04b1468fb2d02/#planning","title":"Planning","text":""},{"location":"Farming%204b9df4201aa544e58b946622de151cb0/%E6%B0%B4%E3%82%84%E3%82%8A%208304e2e227fd4107ac2bf890c141a751/","title":"\u6c34\u3084\u308a","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/English%20e4e04b2288e84852b96e14b3bc71057a/","title":"English","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/English%20e4e04b2288e84852b96e14b3bc71057a/#phraseidom-example","title":"Phrase/Idom Example","text":"Content Usage Rule of thumb I have a rule of thumb that is to xxxxx"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/English%20e4e04b2288e84852b96e14b3bc71057a/#frequently-used","title":"Frequently used","text":"Original word Synonym"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/English%20e4e04b2288e84852b96e14b3bc71057a/#difficult-words","title":"Difficult words","text":"Word Meaning Remarks anaesthesia / anesthesia \u9ebb\u9154 Pronounce : An nes theeee sia in vitro fertilization \u4f53\u5916\u53d7\u7cbe (v tro) Fertilized Egg \u53d7\u7cbe\u5375(juseiran)"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/","title":"JaPaNeSe","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#verb","title":"Verb","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#conjugation","title":"Conjugation","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#example","title":"Example","text":"Word romaji Meaning \u54b2\u304f\u3000\u54b2\u304d\u307e\u3059 saku \u52a9\u304b\u308b\u3000\u52a9\u304b\u308a\u307e\u3059 tasu karu e.g. xxxxx\u3068\u52a9\u304b\u308a\u307e\u3059 \u6271\u3046\u3000\u6271\u3044\u307e\u3059 a tsu ka u = Deal, Use \u9001\u308b okuru \u9045\u308c\u308b okureru \u6012\u308b okoru \u542b\u307e\u308c\u308b fuku mareru \u6e80\u305f\u3059 mitasu to satisfy \u7570\u306a\u308b koto naru \u6025\u304b\u3059 sekasu To hurry e.g. \u6025\u304b\u3057\u3066\u3059\u3044\u307e\u305b\u3093"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#noun","title":"Noun","text":"Word Hiragana Meaning \u91d1\u878d \u304d\u3093\u3086\u3046 \u5730\u56f3 \u3061\u305a \u304a\u624b\u4f1d\u3044 \u304a\u3000\u3066\u3000\u3064\u3060\u3044 \u3055\u3089 \u3055\u3089 new, or can use \u307e\u3063\u3055\u3089 (more new) \u7a7a\u767d \u304f\u3046\u306f\u304f \u500b\u3005 \u3053\u3053 individual e.g. \u3053\u306e\u30af\u30e9\u30b9\u306e\u500b\u3005\u306e\u30e1\u30f3\u30d0\u30fc\u306f\u2026 \u53d6\u308a\u6271\u3044 \u3068\u308a\u3000\u3042\u3064\u304b\u3044 handling e.g. \u643a\u5e2f\u96fb\u8a71\u756a\u53f7\u306e\u53d6\u308a\u6271\u3044\u306b\u3064\u3044\u3066\u3001xxxxxx \u4ed8\u4e0e \u3075\u3088 grant, assign e.g. xxx\u3092\u4ed8\u4e0e\u3057\u307e\u3059"},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#usage-example","title":"Usage Example","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#questions","title":"Questions","text":""},{"location":"Lang%20ff312e341c444427b5529e636fa609b1/JaPaNeSe%20fd42b4aa4f5a416eba1b965eeeca9ec4/#others","title":"Others","text":"<ul> <li>\u3082\u3057\u304b\u3057\u305f\u3089\u3001\u91d1\u878d\u306e\u77e5\u8b58\u304c\u3042\u308b\u30b9\u30bf\u30c3\u30d5\u306a\u3089\u3001\u3082\u3063\u3068\u8a73\u3057\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u306d\u3002</li> <li>\u793e\u540d\u3092\u653f\u5e9c\u306e\u3069\u3053\u304b\u306b\u767b\u9332\u3057\u306a\u3044\u3068\u3002</li> <li>\u53c2\u8003\u306b\u306a\u308a\u307e\u3059\u3002/  \u53c2\u8003\u306b\u306a\u308b\u3068\u601d\u3044\u307e\u3059\u3002</li> <li>\u8a00\u3046\u901a\u308a/\u8a00\u3063\u305f\u901a\u308a<ul> <li>\u79c1\u304c\u8a00\u3063\u305f\u901a\u308a\u306b\u52d5\u3044\u3066\u304f\u3060\u3055\u3044\u3002 ( \u8a00\u3063\u305f\u30bf\u30a4\u30df\u30f3\u30b0\u304c\u904e\u53bb\u3067 )</li> <li>\u79c1\u304c\u8a00\u3046\u901a\u308a\u306b\u52d5\u3044\u3066\u304f\u3060\u3055\u3044\u3002( \u8a00\u3046\u30bf\u30a4\u30df\u30f3\u30b0\u304c\u672a\u6765\u306b\u306a\u308b\u305f\u3081\u3001\u610f\u5473\u304c\u7570\u306a\u308a\u307e\u3059\u3002)</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/AWS%20MLU%20-%20ML%20university%20a8e35c276fde4b16b57c96096f7a920e/","title":"AWS MLU - ML university","text":"<p>https://aws.amazon.com/machine-learning/mlu/</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/AWS%20innovate%20-%20Data%20edition%202022%20be5b64f449304b67b372b1a84c0e8229/","title":"AWS innovate - Data edition 2022","text":"<p>AWS ML redshift &amp; athena is SQL query based.</p> <ul> <li>e.g. can use SQL to do model training, inference\u2026etc</li> <li>Athena data source<ul> <li>Mainly for S3</li> <li>By Athena data source connector(via lambda), almost all AWS services with data</li> <li>By Athena Query Federation SDK, can do more customized thing</li> </ul> </li> <li>Redshift \u2192 from db/data warehouse/S3</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/","title":"Coursera - Andrew Ng ML","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/#toc","title":"ToC","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/#ref","title":"Ref","text":"<ul> <li>ML Specialization - https://www.coursera.org/specializations/machine-learning-introduction#courses</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/#course-1-supervised-machine-learning-regression-and-classification","title":"Course 1 - *Supervised Machine Learning: Regression and Classification*","text":"<p>https://www.coursera.org/learn/machine-learning/home/week/1 </p> <p>Types of algorithm :</p> <ul> <li>Supervised learning</li> <li>Unsupervised learning</li> <li>Recommender systems</li> <li>Reinforcement learning</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/#supervised-learning","title":"Supervised Learning :","text":"<ul> <li>Right answers(data) given</li> <li>e.g. Online Ads platform - Input your data and the ad \u2192 output the probability of you clicking it.</li> </ul> <p>Support Vector machine</p> <ul> <li>There is a mathematical trick to allow computer to deal with infinite number of features</li> </ul> <p>Regression Problem</p> <ul> <li>Trying to predict a continuous (Non-discrete) valued output</li> </ul> <p>Classification Problem</p> <ul> <li>Discrete valued output</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Coursera%20-%20Andrew%20Ng%20ML%201a729708ed93425abb1eac96f4b42a41/#unsupervised-learning","title":"Unsupervised learning :","text":"<ul> <li>Given only the data without label.</li> <li>The goal is to find some pattern, structure, or anything interesting about the data.</li> </ul> <p>Clustering algorithm</p> <ul> <li>Similar to classification BUT it just does clustering data into groups the model found.</li> <li>i.e. We don\u2019t know what are the different types, we let the model find it out</li> <li>e.g. Google news clustering the news and provide related/similar news recommendation</li> <li>e.g. DNA gene clustering, different group of DNA people may have similar behavior</li> <li>e.g. Cluster customers into groups for easier &amp; efficient marketing</li> <li>e.g. Deeplearning.ai cluster their community users, to know the purpose of users why joining courses, subscribing newsletters, attending events\u2026etc</li> </ul> <p>Anomaly Detection</p> <ul> <li>Fraud detection in finance industry</li> </ul> <p>Dimensionality Reduction</p> <ul> <li>Take a large dataset and reduce it into a smaller dataset</li> <li>e.g. file compression</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/DL%20free%20course%20094b805e051a4b988eec58ac80eec98d/","title":"DL free course","text":"<p>https://www.reddit.com/r/MachineLearning/comments/lqrek7/n_20_hours_of_new_lectures_on_deep_learning_and/</p> <p>The assistant professor :</p> <p>https://cwkx.github.io/</p> <p>Chris G. Willcocks</p> <p>Department of Computer Science</p> <p>Durham University</p> <p>Deep Learning\u00a0(playlist)The first five lectures are more theoretical, the second half is more applied.</p> <ul> <li>Lecture 1: Introduction. (slides,\u00a0video)</li> <li>Lecture 2: Mathematical principles and backpropagation. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 3: PyTorch programming:\u00a0coding session. (colab1,\u00a0colab2,\u00a0video) - minor issues with audio, but it fixes itself later.</li> <li>Lecture 4: Designing models to generalise. (slides,\u00a0video)</li> <li>Lecture 5: Generative models. (slides,\u00a0desmos,\u00a0colab,\u00a0video)</li> <li>Lecture 6: Adversarial models. (slides,\u00a0colab1,\u00a0colab2,\u00a0colab3,\u00a0colab4,\u00a0video)</li> <li>Lecture 7: Energy-based models. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 8: Sequential models:\u00a0by u/samb-t. (slides,\u00a0colab1,\u00a0colab2,\u00a0video)</li> <li>Lecture 9: Flow models and implicit networks. (slides,\u00a0SIREN,\u00a0GON,\u00a0video)</li> <li>Lecture 10: Meta and manifold learning. (slides,\u00a0interview,\u00a0video)</li> </ul> <p>Reinforcement Learning\u00a0(playlist)This is based on David Silver's course but targeting younger students within a shorter 50min format (missing the advanced derivations) + more examples and Colab code.</p> <ul> <li>Lecture 1: Foundations. (slides,\u00a0video)</li> <li>Lecture 2: Markov decision processes. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 3: OpenAI gym. (video)</li> <li>Lecture 4: Dynamic programming. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 5: Monte Carlo methods. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 6: Temporal-difference methods. (slides,\u00a0colab,\u00a0video)</li> <li>Lecture 7: Function approximation. (slides,\u00a0code,\u00a0video)</li> <li>Lecture 8: Policy gradient methods. (slides,\u00a0code,\u00a0theory,\u00a0video)</li> <li>Lecture 9: Model-based methods. (slides,\u00a0video)</li> <li>Lecture 10: Extended methods. (slides,\u00a0atari,\u00a0video)</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/","title":"Daily Bnomial","text":"<p>https://today.bnomial.com/ </p> <p>2022-06-10 Psychologists speak another language</p> <p>2022-06-27  The attributes of a tensor</p> <p>2022-07-12 Gradient Descent</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/","title":"Google ML Bootcamp","text":"<p>It is an online program by Google, support applicant by allowing them to finish one Coursera course, out of 2 selected courses,  for FREE, and optionally register 3 selected certificate.</p> <p>https://rsvp.withgoogle.com/events/gdml-japan </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#schedule","title":"Schedule:","text":"<ul> <li>2022-07-20 : Kickoff start briefing</li> <li>2022-11-xx : free exam deadline</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#course-for-choosing","title":"Course for choosing:","text":"<ul> <li>*Deep Learning Specialization(Selected)*</li> <li>*TensorFlow Developer Certification*</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#deep-learning-specialization","title":"Deep Learning Specialization","text":"<ul> <li>Help you understand the capabilities, challenges, and consequences of deep learning</li> <li>Build and train neural network such as CNN, RNN, LSTMs, Transformers</li> <li>Optimize them with technique such as Dropout, BatchNorm, Xavier/He initialization</li> <li>Get ready to master theoretical concepts and their industry applications using Python and TensorFlow</li> <li>Real-world cases such as speech recognition, music synthesis, chatbots, machine translation, natural language processing, and more.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#5-course","title":"5 Course","text":"<p>Neural Networks and Deep Learning</p> <p>Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</p> <p>Structuring Machine Learning Projects</p> <p>Convolutional Neural Networks</p> <p>Sequence Models</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#certificate","title":"Certificate","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/#2","title":"2. \u6a5f\u68b0\u5b66\u7fd2\u306e\u8a8d\u5b9a\u8cc7\u683c\u3092\u53d6\u5f97\u3059\u308b","text":"<p>\u30aa\u30f3\u30e9\u30a4\u30f3\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u304c\u5b8c\u4e86\u3057\u305f\u65b9\u306f\u3001\u6a5f\u68b0\u5b66\u7fd2\u30a8\u30f3\u30b8\u30cb\u30a2\u3068\u3057\u3066\u306e\u8a8d\u5b9a\u8cc7\u683c\u306e\u53d6\u5f97\u306b\u30c1\u30e3\u30ec\u30f3\u30b8\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u6b21\u306e 3 \u3064\u306e\u8a8d\u5b9a\u8cc7\u683c\u3092\u53d7\u9a13\u3059\u308b\u5834\u5408\u306f \u672c\u30d7\u30ed\u30b0\u30e9\u30e0\u3067\u53d7\u9a13\u6599\u3092\u8ca0\u62c5\u3044\u305f\u3057\u307e\u3059\u3002</p> <p>TensorFlow \u30c7\u30d9\u30ed\u30c3\u30d1\u30fc\u8a8d\u5b9a\u8cc7\u683c: \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3084\u6a5f\u68b0\u5b66\u7fd2\uff08ML\uff09\u306e\u554f\u984c\u89e3\u6c7a\u80fd\u529b\u3092\u8a3c\u660e\u3059\u308b\u8cc7\u683c</p> <p>GCP Professional Data Engineer: \u30c7\u30fc\u30bf\u51e6\u7406\u30b7\u30b9\u30c6\u30e0\u306e\u8a2d\u8a08\u3001\u30a2\u30fc\u30ad\u30c6\u30af\u30c8\u30c7\u30fc\u30bf\u306e\u51e6\u7406\u3001\u30b7\u30b9\u30c6\u30e0\u69cb\u7bc9\u3001\u6a5f\u68b0\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u904b\u7528\u3001\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u54c1\u8cea\u78ba\u4fdd\u306a\u3069\u306e\u80fd\u529b\u3092\u8a55\u4fa1\u3059\u308b\u8a8d\u5b9a\u8cc7\u683c</p> <p>GCP Professional ML Engineer: ML\u554f\u984c\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3001\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u8a2d\u8a08\u3001\u30c7\u30fc\u30bf\u6e96\u5099\u304a\u3088\u3073\u51e6\u7406\u30b7\u30b9\u30c6\u30e0\u306e\u8a2d\u8a08\u3001\u30e2\u30c7\u30eb\u306e\u958b\u767a\u3001\u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306e\u81ea\u52d5\u5316\u304a\u3088\u3073\u30aa\u30fc\u30b1\u30b9\u30c8\u30ec\u30fc\u30b7\u30e7\u30f3\u3001ML\u30bd\u30ea\u30e5\u30fc\u30b7\u30e7\u30f3\u306e\u76e3\u8996\u3001\u6700\u9069\u5316\u3001\u304a\u3088\u3073\u4fdd\u5b88\u3092\u884c\u3046\u80fd\u529b\u3092\u8a55\u4fa1\u3059\u308b\u8a8d\u5b9a\u8cc7\u683c</p> <ul> <li>\u8a66\u9a13\u306f\u82f1\u8a9e\u3067\u884c\u308f\u308c\u307e\u3059\u3002</li> <li>\u8cc7\u683c\u306e\u53d6\u5f97\u306f\u53c2\u52a0\u8005\u81ea\u8eab\u3067\u9032\u3081\u3066\u3044\u305f\u3060\u304d\u307e\u3059\u304c\u3001\u30aa\u30f3\u30e9\u30a4\u30f3\u30b0\u30eb\u30fc\u30d7\u3084\u30e1\u30f3\u30bf\u30ea\u30f3\u30b0\u306e\u6a5f\u4f1a\u3092\u63d0\u4f9b\u3057\u30b5\u30dd\u30fc\u30c8\u3057\u307e\u3059\u3002</li> <li>\u8a66\u9a13\u306b\u5408\u683c\u3057\u305f\u53d7\u8b1b\u8005\u306b\u306f\u3001\u4fee\u4e86\u8a18\u5ff5\u54c1\u3092\u8d08\u5448\u3057\u307e\u3059\u3002</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20cloud%20skill%20boost%2016d341ec7e514212934bb008aac7d5c0/","title":"Google cloud skill boost","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20cloud%20skill%20boost%2016d341ec7e514212934bb008aac7d5c0/#main-url","title":"Main URL","text":"<p>https://www.cloudskillsboost.google/catalog?keywords=&amp;locale=&amp;solution[]=any&amp;role[]=ml---developer&amp;skill-badge[]=any&amp;format[]=any&amp;level[]=any&amp;duration[]=any&amp;language[]=any</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20cloud%20skill%20boost%2016d341ec7e514212934bb008aac7d5c0/#how-google-does-machine-learning","title":"How Google Does Machine Learning","text":"<p>https://www.cloudskillsboost.google/course_templates/10 </p> <p>Notes:</p> <p>Backward-looking data = look at historical data ML = make predictive decision base on data</p> <p>Google translate:</p> <ul> <li>e.g. take photo of a sign to translate</li> <li>1 model to find the sign</li> <li>1 to read the sign</li> <li>OCR</li> <li>1 to detect the language</li> <li>1 to translate the sign</li> <li>1 to superimposed translated text</li> <li>maybe 1 to select the font style now using</li> </ul> <p>Google search :</p> <ul> <li>few years ago logic:<ul> <li>a bunch of hand-coded rules, e.g. your location + query text =&gt; which result to show</li> </ul> </li> <li>NOW:<ul> <li>DL</li> <li>RankBrain - a DL network for search ranking</li> <li>WHen you click on a link, it may tell google: this result is what you wanted from the query text</li> </ul> </li> </ul> <p>ML can solve: anything you are writing rules today</p> <p>throw away the heuristics just as soon as you have enough data about user preferences</p> <p></p> <p></p> <p>To frame a ML problem</p> <ul> <li>As an ML problem:<ul> <li>what is being predicted</li> <li>what data do we need</li> </ul> </li> <li>As a software problem:<ul> <li>What does the API look like</li> <li>what is the input &amp; output</li> <li>Who will use this service</li> <li>how others are doing this today</li> </ul> </li> <li>As a data problem:<ul> <li>what kind of data we need to collect</li> <li>What to analyze</li> <li>How to react to the prediction</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Kaggle%2031e9b9c6782241c8a4ace240701e7357/","title":"Kaggle","text":"<p>Spaceship-titanic</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/","title":"NLP Book","text":"<p>NLP core task</p> <p>Language Structure</p> <p>NLP approach</p> <p>General Cycle</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-10%20Psychologists%20speak%20another%20language%20aa6c7b59c06f464b85324df7d54acee4/","title":"2022-06-10 Psychologists speak another language","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-10%20Psychologists%20speak%20another%20language%20aa6c7b59c06f464b85324df7d54acee4/#todays-question","title":"Today's question:","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-10%20Psychologists%20speak%20another%20language%20aa6c7b59c06f464b85324df7d54acee4/#psychologists-speak-another-language","title":"Psychologists speak another language","text":"<p>A group of psychologists is visiting the office, and Scarlett is in charge of showing them around.</p> <p>The first stop will be in the Data Science department. They are very excited about showing them the results of their latest machine learning model.</p> <p>Ten minutes into the presentation, it's painfully apparent that the crew is not fully grasping what is going on. Scarlett decides to summarize her ideas using a familiar language: statistics.</p> <p>In statistics, the notion of statistical error is an integral part of hypothesis testing. There are two types of errors when testing the null hypothesis: type I and type II errors. Scarlett wants to explain their results regarding the latter.</p> <p>Do you remember what the correct definition of a type II error is?</p> <ul> <li>[ ]  A type II error occurs when the null hypothesis is true and is not rejected.</li> <li>[ ]  A type II error occurs when the null hypothesis is true but is rejected.</li> <li>[ ]  A type II error occurs when the null hypothesis is false but is not rejected.</li> <li>[ ]  A type II error occurs when the null hypothesis is false and is rejected.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-10%20Psychologists%20speak%20another%20language%20aa6c7b59c06f464b85324df7d54acee4/#lets-review-the-answer","title":"Let's review the answer:","text":"<p>It makes sense for those who are more used to machine learning terminology to compare type I and type II errors with false positives and false negatives.</p> <p>Type I errors are the same as false positives. For example, if we mark a valid email as spam, we are in the presence of a false positive. Type I errors are the rejection of a true\u00a0null hypothesis\u00a0by mistake.</p> <p>Type II errors are the same as false negatives. For example, if we let a spam message pass as a valid email, we are in the presence of a false negative. This is a type II error because we accept the conclusion of the email being good, even though it is incorrect. Type II errors are the acceptance of a false null hypothesis by mistake.</p> <p>In other words, a type II error is when we incorrectly accept the null hypothesis even though the alternative hypothesis is true. Therefore, The third choice is the correct answer to this question.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-10%20Psychologists%20speak%20another%20language%20aa6c7b59c06f464b85324df7d54acee4/#recommended-reading","title":"Recommended reading","text":"<ul> <li>\"What Is a Null Hypothesis?\"\u00a0covers the basics you need to understand before going into hypothesis testing.</li> <li>Check out\u00a0\"Type I and type II errors\"\u00a0for the definition and examples of each type of error.</li> <li>\"Understanding Null Hypothesis Testing\"\u00a0is an excellent article about hypothesis testing.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-27%20The%20attributes%20of%20a%20tensor%20aa75cc0ea0524abeaa73e5bfd453a3b8/","title":"2022-06-27  The attributes of a tensor","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-27%20The%20attributes%20of%20a%20tensor%20aa75cc0ea0524abeaa73e5bfd453a3b8/#todays-question","title":"Today's question:","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-27%20The%20attributes%20of%20a%20tensor%20aa75cc0ea0524abeaa73e5bfd453a3b8/#the-attributes-of-a-tensor","title":"The attributes of a tensor","text":"<p>A \"tensor\" is one of the most basic data structures used in machine learning systems.</p> <p>Let's go back to basics and focus on the fundamental characteristics of a tensor.</p> <p>Which of the following are valid attributes that represent a tensor?</p> <ul> <li>Its number of axes. This attribute is also called the \"rank\" of the tensor.</li> <li>****Its cardinality. This attribute represents the numerical relationship between the axes of the tensor.</li> <li>Its shape. This attribute represents how many dimensions the tensor has along each axis.</li> <li>Its data type. This attribute represents the type of values contained in the tensor.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-27%20The%20attributes%20of%20a%20tensor%20aa75cc0ea0524abeaa73e5bfd453a3b8/#good-job","title":"Good job!","text":"<p>Three primary attributes define a tensor:</p> <ol> <li>Its rank, or the number of axes.</li> <li>Its shape, or the number of dimensions per axis.</li> <li>Its data type, or the type of data contained in it.</li> </ol> <p>The rank of a tensor refers to the tensor's number of axes.</p> <p>Examples:</p> <ul> <li>Rank of a matrix is 2.</li> <li>Rank of a vector is 1.</li> <li>Rank of a scalar is 0.</li> </ul> <p>The shape of a tensor describes the number of dimensions along each axis.</p> <p>Examples:</p> <ul> <li><code>()</code>\u00a0\u2014 scalar</li> <li><code>(2,)</code>\u00a0\u2014 vector</li> <li><code>(3, 2)</code>\u00a0\u2014 matrix</li> <li><code>(3, 2, 5)</code>\u00a0\u2014 3D tensor</li> </ul> <p>The data type of a tensor refers to the kind of data contained in it.</p> <p>Examples:</p> <ul> <li><code>float32</code></li> <li><code>float64</code></li> <li><code>uint8</code></li> <li><code>int64</code></li> </ul> <p>The second choice mentions \"the cardinality of a tensor\" as \"the numerical relationship between the axes of the tensor.\" This is not a correct answer.</p> <p>In summary, the correct answer to the question is the first, third, and fourth choices.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-06-27%20The%20attributes%20of%20a%20tensor%20aa75cc0ea0524abeaa73e5bfd453a3b8/#recommended-reading","title":"Recommended reading","text":"<ul> <li>Deep Learning with Python, Second Edition\u00a0covers the topic of tensors really well.</li> <li>Check\u00a0\"A Gentle Introduction to Tensors for Machine Learning with NumPy\"\u00a0for a quick introduction to tensors and practical code.</li> </ul> <p>See you tomorrow for another question!</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-07-12%20Gradient%20Descent%20243f345195dd488f85dcf022fbe91203/","title":"2022-07-12 Gradient Descent","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-07-12%20Gradient%20Descent%20243f345195dd488f85dcf022fbe91203/#bettys-surprise","title":"Betty's surprise","text":"<p>Discovering that Gradient Descent has different names based on how many samples it uses to calculate the error was something Betty wasn't expecting.</p> <p>There are three popular variations of the algorithm: Mini-Batch Gradient Descent, Stochastic Gradient Descent, and Batch Gradient Descent.</p> <p>Betty found this fascinating, and although it made sense, she always found that Stochastic Gradient Descent was the hardest for her to define.</p> <p>Which of the following statements is true about this version of the algorithm?</p> <ul> <li>****Stochastic Gradient Descent determines the optimal amount of data required to compute the gradient of the cost function.</li> <li>****Stochastic Gradient Descent uses a single sample of data during every iteration.</li> <li>Stochastic Gradient Descent uses all available data once during every iteration.</li> <li>****Stochastic Gradient Descent uses a batch of data (more than one sample but fewer than the entire dataset) during every iteration.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-07-12%20Gradient%20Descent%20243f345195dd488f85dcf022fbe91203/#this-is-how-we-were-thinking-about-it","title":"This is how we were thinking about it:","text":"<p>Here is a simplified explanation of how Gradient Descent works: We take samples from the training dataset, run them through the model, and determine how far our results are from what we expect. We then use this \"error\" to compute how much we need to update the model weights to improve the results.</p> <p>A critical decision we need to make is how many samples we use on every iteration to run through the model. We have three choices:</p> <ul> <li>Use a single sample of data.</li> <li>Use all of the data at once.</li> <li>Use some of the data.</li> </ul> <p>Using a single sample of data on every iteration is called \"Stochastic Gradient Descent\" or SGD. In other words, the algorithm uses one sample to compute the updates.</p> <p>Using all the data at once is called \"Batch Gradient Descent.\" After processing every sample, the algorithm takes the entire dataset and computes the updates.</p> <p>Finally, using some of the data\u2014more than one sample but fewer than the entire dataset\u2014is called \"Mini-Batch Gradient Descent.\" The algorithm works like Batch Gradient Descent, with the only difference that we use fewer samples.</p> <p>Therefore, the second choice is the correct answer to this question.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Daily%20Bnomial%202b6f6ca3c01a4df5916ae715fdc13c9a/2022-07-12%20Gradient%20Descent%20243f345195dd488f85dcf022fbe91203/#recommended-reading","title":"Recommended reading","text":"<ul> <li>Check\u00a0\"An overview of gradient descent optimization algorithms\"\u00a0for a deep dive into gradient descent and every one of its variants.</li> <li>\"Gradient Descent For Machine Learning\"\u00a0is another great introduction to gradient descent.</li> <li>\"A Gentle Introduction to Mini-Batch Gradient Descent and How to Configure Batch Size\"\u00a0covers Batch and Mini-Batch Gradient Descent.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/","title":"Convolutional Neural Networks","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#cite","title":"Cite","text":"<p>As in copyright:</p> <p>DeepLearning.AI\u00a0makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite\u00a0DeepLearning.AI \u00a0as the source of the slides.</p> <ul> <li>Slides here are from DeepLearning.AI</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#cnn-examples","title":"CNN Examples:","text":"<p>There are a few CNNs mentioned in the course, that was recommended to read the paper directly. </p> <ul> <li>LeNet-5<ul> <li>Medium - https://towardsdatascience.com/understanding-and-implementing-lenet-5-cnn-architecture-deep-learning-a2d531ebc342</li> <li>Original Paper - 1998(46pages) - http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</li> </ul> </li> <li>AlexNet - It is similar to LeNet but bigger and deeper network<ul> <li>original paper (ImageNet Classification with Deep Convolutional Neural Networks) 2012 -https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</li> <li>Medium post - https://towardsdatascience.com/alexnet-the-architecture-that-challenged-cnns-e406d5297951</li> </ul> </li> <li>VGG - 16 - <code>16</code> means 16 layers<ul> <li>original paper (*Very Deep Convolutional Networks for Large-Scale Image Recognition*) 2015 - https://arxiv.org/pdf/1409.1556.pdf</li> <li>Medium - https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918</li> </ul> </li> <li>ResNet (Residual Network)<ul> <li>Original paper (*Deep Residual Learning for Image Recognition)* 2015 - https://arxiv.org/abs/1512.03385</li> <li>Medium post - https://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c</li> </ul> </li> <li>Inception Network<ul> <li>Original paper (*Going Deeper with Convolutions*) 2014 - https://arxiv.org/abs/1409.4842</li> <li>Medium - https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202</li> </ul> </li> <li>MobileNet<ul> <li>there are V1, V2, V3</li> <li>Simple comparison - https://lixinso.medium.com/mobilenet-c08928f2dba7</li> <li>V1<ul> <li>paper 2017 (*MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications)*- https://arxiv.org/abs/1704.04861</li> <li>Medium - https://towardsdatascience.com/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69</li> </ul> </li> <li>V2<ul> <li>paper 2019 (*MobileNetV2: Inverted Residuals and Linear Bottlenecks)*- https://arxiv.org/abs/1801.04381</li> <li>google blog - https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html</li> </ul> </li> <li>V3<ul> <li>paper 2019 (*Searching for MobileNetV3)* - https://arxiv.org/abs/1905.02244</li> <li>Medium - https://towardsdatascience.com/everything-you-need-to-know-about-mobilenetv3-and-its-comparison-with-previous-versions-a5d5e5a6eeaa</li> </ul> </li> </ul> </li> <li>EfficientNet<ul> <li>google blog - https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html</li> <li>paper 2020 (*EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)* - https://arxiv.org/abs/1905.11946</li> </ul> </li> <li>YOLO<ul> <li>1st paper 2016 (*You Only Look Once: Unified, Real-Time Object Detection)* - https://arxiv.org/abs/1506.02640</li> <li>2nd paper 2016 (*YOLO9000: Better, Faster, Stronger)* - https://arxiv.org/abs/1612.08242</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#week-1-foundation-of-convolutional-neural-network","title":"Week 1 - Foundation of Convolutional Neural Network","text":"<p>Or called CNN, it is a type of NNs that specialized at solving Computer Vision Problem, e.g. Image classification, Object detection, segmentation. The CNN can \u201clearn\u201d the image features throughout the layers, such as edges\u2026etc.</p> <p>This week explained fundamental elements and the \u201cblocks\u201d in CNN, such as filter(kernel), padding\u2026etc</p> <p>Note: Convolution in ML is NOT the same as Convolution in Maths. </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#edge-detection","title":"Edge Detection:","text":"<ul> <li> <p>In a given image, we can apply a filter(or kernel) to detect the edges(horizontal or vertical) like the screenshot.</p> <p></p> </li> </ul> <p>Mathematically, below is a simple example:</p> <ul> <li>The input image shows that the \u201cedge\u201d is in the middle, between white &amp; grey area</li> <li>Then apply the 3x3 filter (for vertical edge detection)</li> <li> <p>The result image will show the edge is in the middle</p> <p></p> </li> </ul> <p>Edges filter types:</p> <ul> <li> <p>Vertical or horizontal filter:</p> <p></p> </li> <li> <p>Other filters such as Sobel filter, ..etc</p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#padding","title":"Padding:","text":"<ul> <li>During convolution with filters, the output image will be in a smaller resolution. We can apply padding before the convolution to change this output resolution.</li> <li>There are TWO common padding used in CNN - <code>Valid</code> and <code>Same</code><ul> <li>Valid is actually No Padding</li> <li>Same is a padding method to ensure the output resolution to be equal to the input.<ul> <li>By using a simple formula : p = (f-1)/2</li> <li>e.g. If <code>p=2</code>, that means \\(\\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; 5 \\end{bmatrix} \\longrightarrow  \\begin{bmatrix} 0&amp;0&amp;0&amp;0&amp;0&amp;0 \\\\ 0 &amp; 0 &amp; 2 &amp; 3 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 4 &amp; 5 &amp; 0 &amp; 0 \\\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0 \\end{bmatrix}\\)</li> </ul> </li> </ul> </li> <li> <p>Zero-padding</p> <p></p> </li> <li> <p>Benefit of padding</p> <ul> <li>1st, without necessarily shrinking the height and width of the volumes.<ul> <li>Important for deeper networks, otherwise the height/width would shrink as you go to deeper layers.</li> <li>Special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer.</li> </ul> </li> <li>2nd,  keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels at the edges of an image.</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#stride-convolution","title":"Stride Convolution:","text":"<ul> <li>Stride, it means how \u201cfar\u201d each step of convolution is.</li> <li>E.g. for an 7x7 image convolution with 3x3 filter, if <code>s=1</code>, the output image should be 5x5</li> <li> <p>But for the same image &amp; filter, if <code>s=2</code>, the output image will be 3x3. The filter \u201cmove right side by 2 steps\u201d after applying each time.</p> <p></p> </li> <li> <p>The output dimension can be calculated via <code>(n+2p-f)/s + 1</code> where n is input dimension, f is filter size</p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#convolution-of-volumes","title":"Convolution of volumes:","text":"<p>Let\u2019s take a look the convolution with only ONE filter first.</p> <ul> <li>All example above is only 2-D, i.e. no color channel in images.</li> <li>Each filter will be the same Channel Size as input\u2019s, e.g. RGB = 3.</li> <li> <p>E.g. For 1 filter convolution, the 1st element in output is calculated by convolving each channel of image with the filter at the top-left corner.</p> <p></p> </li> <li> <p>So, the output size is still calculated by <code>(n+2p-f)/s + 1</code>, that ignore how many channels.</p> </li> </ul> <p>Now, let\u2019s take a look the case of multiple filters.</p> <ul> <li> <p>Assuming all filters are the same size(note: we will see different filter size later), the 2-D output is stacked at the end to form a new 3-D output.</p> <p></p> </li> <li> <p>The output size is calculated by <code>(n+2p-f)/s + 1</code> and the channel size will be equal to \u201cthe number of filters\u201d</p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#one-layer-of-cnn","title":"One Layer of CNN:","text":"<ul> <li>In the Conv block, input image is convolved with a number of filters. The parameters <code>w</code>(or weight) are the values inside each filters</li> <li>Then EACH 2-D output is added with a bias <code>b</code> and applied an activation function(e.g. ReLU).</li> <li> <p>Finally each 2-D output is stacked together to form a new 3-D input for next layer.</p> <p></p> </li> </ul> <p>Number of parameters in ONE layer:</p> <ul> <li>e.g. 1 layer of CNN block contains <code>10 filters</code>, each filter is 3x3x3.<ul> <li>num of <code>w</code> = 3x3x3 x 10</li> <li>num of <code>b</code> = 10</li> <li>Total parameters = 280 for ONE layer</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#pooling-layer","title":"Pooling Layer:","text":"<ul> <li>From above, we know below factors decide the output dimension:<ul> <li><code>Padding</code> - can help to keep output as the same dimension of input.</li> <li><code>Number of filter</code> - decide the \u201cchannel\u201d of output (i.e. depth)</li> <li><code>Filter dimension</code> - decide the \u201cresolution\u201d of the output (i.e. w &amp; h)</li> </ul> </li> <li>What if we want to reduce the size? \u2192 Pooling</li> <li> <p>There are 2 type of pooling:</p> <ul> <li>Max Pooling - take the max value in the area</li> <li> <p>Avg Pooling - values are averaged in the area</p> <p></p> </li> </ul> </li> <li> <p>Quick fact - there are NO parameters to train in Pooling layer</p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#hyperparameters-of-cnn","title":"Hyperparameters of CNN:","text":"<ul> <li>For each type of block, there are bunches of thing to tune.</li> <li>Conv block - num of filter, filter size, padding, stride</li> <li>Pooling block - type of pooling, filter size, stride</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#cnn-basic-structure","title":"CNN Basic Structure:","text":"<ul> <li>Let\u2019s take a look a simple basic structure of CNN:</li> <li> <p>There will be several CNN blocks(including Conv, Pooling).  At the end of the network, there will be a layer of flattening the 3-D output, and then fully connected to the output layer such as softmax for classification.</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#why-need-cnn","title":"Why need CNN?","text":"<ul> <li> <p>1st Reason - number of parameters are TOTALLY different</p> <ul> <li> <p>For the same size of input &amp; output, CNN - 156 parameters, Traditional NN - more than 14M</p> <p></p> </li> </ul> </li> <li> <p>Parameter Sharing - A feature detector (such as a vertical edge detector) that\u2019s useful in one part of the image is probably useful in another part of the image.</p> </li> <li>Sparsity of connections - In each layer, each output value depends only on a small number of inputs.<ul> <li> <p>Off-topic - Sparse meaning:</p> <p></p> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#assignment-1","title":"Assignment 1","text":"<ul> <li>Padding<ul> <li>benefit<ul> <li>1st, without necessarily shrinking the height and width of the volumes.<ul> <li>Important for deeper networks, otherwise the height/width would shrink as you go to deeper layers.</li> <li>Special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer.</li> </ul> </li> <li>2nd,  keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels at the edges of an image.</li> </ul> </li> <li>type - zero padding</li> </ul> </li> <li>Convolution<ul> <li>The 2D output of one convolution action is called the feature map</li> </ul> </li> <li>Pooling<ul> <li>gradually reduce the height and width of the input, then summarizing the features in each small region</li> </ul> </li> </ul> <pre><code>x_pad = zero_pad(x, 3)\n# x: (m, n_H, n_W, n_C)\nZ = conv_single_step(a_slice_prev, W, b)\n# a_slice_prev shape = W shape: (f, f, n_C_prev)\n# b : (1, 1, 1)\n# Z : scalar\nZ, cache_conv = conv_forward(A_prev, W, b, hparameters)\n\"\"\"\nArguments:\n    A_prev -- output activations of the previous layer, \n        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n    hparameters -- python dictionary containing \"stride\" and \"pad\"\n    Returns:\n    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n    cache -- cache of values needed for the conv_backward() function\n                        cache = (A_prev, W, b, hparameters)\n\"\"\"\nA, cache = pool_forward(A_prev, hparameters, mode = \"max\")\n\"\"\"\nArguments:\n    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    hparameters -- python dictionary containing \"f\" and \"stride\"\n    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n    Returns:\n    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n                            cache = (A_prev, hparameters)\n\"\"\"\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#optional-back-propagation-of-conv-layer","title":"Optional - back propagation of Conv layer:","text":"<ul> <li>dA_prev - gradient of the cost with respect to the input of the conv layer, shape (m, n_H_prev, n_W_prev, n_C_prev)<ul> <li>in short - gradient of the input (image or the output of previous layer)</li> </ul> </li> <li>dW - gradient of all filters in this conv layer shape (f, f, n_C_prev, n_C)</li> <li>db - gradient of the bias in all filters of this conv layer shape (1, 1, 1, n_C)</li> </ul> <p>Simplified illustration:</p> <p>$$ \\text{Forward : }</p> <p>\\overbrace{ \\begin{bmatrix}  ... &amp; ... &amp; ...  &amp; ... &amp; ... \\  ... &amp; ... &amp; a_1  &amp; a_2 &amp; a_3 \\  ... &amp; ... &amp; a_4  &amp; a_5 &amp; a_6 \\  ... &amp; ... &amp; a_7  &amp; a_8 &amp; a_9 \\  ... &amp; ... &amp; ...  &amp; ... &amp; ...  \\end{bmatrix} }^{\\text{input A}}</p> <ul> <li></li> </ul> <p>\\overbrace{ \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 \\ w_4 &amp; w_5 &amp; w_6 \\ w_7 &amp; w_8 &amp; w_9  \\end{bmatrix} }^{\\text{1 filter W}}</p> <p>\\xrightarrow{\\text{conv with 1 filter}}</p> <p>\\overbrace{ \\begin{bmatrix} ... &amp; ... &amp; ... \\ ... &amp; ... &amp; z_1 \\ ... &amp; ... &amp; ... \\end{bmatrix} }^{\\text{output Z}}</p> <p>$$</p> <p>$$ \\text{Backward : } \\overbrace{\\begin{bmatrix}\u00a0... &amp; ... &amp; ...\u00a0 &amp; ... &amp; ... \\\u00a0... &amp; ... &amp; da_1\u00a0 &amp; da_2 &amp; da_3 \\\u00a0... &amp; ... &amp; da_4\u00a0 &amp; da_5 &amp; da_6 \\\u00a0... &amp; ... &amp; da_7\u00a0 &amp; da_8 &amp; da_9 \\\u00a0... &amp; ... &amp; ...\u00a0 &amp; ... &amp; ...\u00a0\\end{bmatrix}}^{\\text{gradient of input: dA}} *\u00a0 \\overbrace{\\begin{bmatrix}dw_1 &amp; dw_2 &amp; dw_3 \\dw_4 &amp; dw_5 &amp; dw_6 \\dw_7 &amp; dw_8 &amp; dw_9\u00a0\\end{bmatrix}}^{\\text{gradient of 1 filter: dW}}</p> <p>\\xleftarrow{\\text{backprop}} \\overbrace{\\begin{bmatrix}... &amp; ... &amp; ... \\... &amp; ... &amp; dz_1 \\... &amp; ... &amp; ...\\end{bmatrix}}^{\\text{gradient of output: dZ}} $$</p> <p>Note:</p> <ul> <li>bias is ignored here</li> <li>input channel dimension is ignored to simplify the figure</li> <li>math operation above is not accurate, it is just for illustration the \u201cfeeling\u201d</li> <li>Proper <code>da</code> should be <code>da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</code> for all filters <code>c</code><ul> <li>Because the slice <code>a</code> was used to convolve with ALL filters <code>c</code> to produce the output z1, with the depth of (c). So we need to sum all gradient from ALL filters</li> </ul> </li> <li>Proper <code>dW</code> is calculated via <code>dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</code><ul> <li>So for each dW of all filters <code>c</code>, it sums up the gradient, obtained from all slices of input and gradient of output.</li> </ul> </li> <li>Similarly, <code>db</code> is via <code>db[:,:,:,c] += dZ[i, h, w, c]</code></li> </ul> <p>tmp</p> <pre><code>input_img = tf.keras.Input(shape=input_shape):\noutputs = tf.keras.layers.Dense(units=6, activation='softmax')(F)\ntfl.ZeroPadding2D(\npadding=(3,3), input_shape=(64,64,3)\n),\n## Conv2D with 32 7x7 filters and stride of 1\ntfl.Conv2D(32, 7, strides=(1, 1)),\n## BatchNormalization for axis 3\ntfl.BatchNormalization(axis=3),\n## ReLU\ntfl.ReLU(),\n## Max Pooling 2D with default parameters\ntfl.MaxPool2D(),\n## Flatten layer\ntfl.Flatten(),\n## Dense layer with 1 unit for output &amp; 'sigmoid' activation\ntfl.Dense(1, activation=\"sigmoid\"),\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#week-2","title":"Week 2","text":"<p>This week covered several popular models architecture. Again, It is suggested to read the original paper to get more insights.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#classic-networks","title":"Classic Networks:","text":"<ul> <li>LeNet - 5</li> <li>AlexNet - It is similar to LeNet but bigger and deeper network<ul> <li>original paper (ImageNet Classification with Deep Convolutional Neural Networks) 2012 -https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</li> <li>Medium post - https://towardsdatascience.com/alexnet-the-architecture-that-challenged-cnns-e406d5297951</li> </ul> </li> <li>VGG - 16 - <code>16</code> means 16 layers<ul> <li>original paper (*Very Deep Convolutional Networks for Large-Scale Image Recognition*) 2015 - https://arxiv.org/pdf/1409.1556.pdf</li> <li>Medium - https://medium.com/@mygreatlearning/everything-you-need-to-know-about-vgg16-7315defb5918</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#resnet-residual-network","title":"ResNet (Residual Network):","text":"<p>It is an important concept in CNN in recent years. It add a shortcut(a.k.a. skip connection) between layers. </p> <p>What is it?</p> <ul> <li> <p>So the activation <code>a</code> will be from <code>z</code> AND the activation from previous layers.</p> <p></p> </li> <li> <p>The effect of this, is to allow the deeper networks to learn more features</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#why-resnet","title":"Why ResNet?","text":"<p>A Drawback about deep layers network:</p> <ul> <li>The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output).</li> <li> <p>However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow.</p> <p></p> </li> <li> <p>More specifically, during gradient descent, as you back-propagate from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode,\" from gaining very large values).</p> </li> <li>During training, you might therefore see the magnitude (or norm) of the gradient for the shallower layers decrease to zero very rapidly as training proceeds.</li> <li>Therefore, adding the \u201cskip connection\u201d can solve the above problems.</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#learning-identity-function-in-residual-network","title":"\u201cLearning identity function\u201d in residual network:","text":"<ul> <li>The lecture mentioned that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an <code>identity function</code>. This means that you can stack additional ResNet blocks with little risk of harming training set performance.</li> <li>On that note, there is also some evidence that the ease of learning an identity function accounts for ResNets' remarkable performance even more than skip connections help with vanishing gradients.</li> <li> <p>The <code>identity block</code></p> <ul> <li> <p>is the standard block used in ResNets, and corresponds to the case where the input activation (say\u00a0a[l]) has the same dimension as the output activation (say\u00a0a[l+2]). To flesh out the different steps of what happens in a ResNet's identity block, here is an alternative diagram showing the individual steps:</p> <p></p> </li> <li> <p>The upper path is the \"shortcut path.\" The lower path is the \"main path.\" In this diagram, notice the CONV2D and ReLU steps in each layer.</p> </li> <li>To speed up training, a BatchNorm step has been added. e.g. The first BatchNorm is normalizing the 'channels' axis.</li> <li>Another type of ResNet block - <code>Convolutional Block</code></li> <li> <p>You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:</p> <p></p> <ul> <li>\u2022 The CONV2D layer in the shortcut path is used to resize the input\u00a0\ud835\udc65x\u00a0to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix\u00a0\ud835\udc4a\ud835\udc60Ws\u00a0discussed in lecture.)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#1-x-1-conv-filters-or-network-in-network","title":"1 x 1 ConV Filters (or Network in Network) :","text":"<ul> <li> <p>It seems the 1x1 Conv filter will alter the output \u201cchannel\u201d.</p> <p></p> </li> <li> <p>By looking deeper, each filter (yellow stick) will convolve with each pixel in 6x6 input of ALL channels, and output a REAL NUMBER after activation function. If you applied a number of filters to this, it feels like applying a fully connected network between pixels in channels and filters.</p> <p></p> </li> </ul> <p>What is the usage?</p> <ul> <li>A Pooling layer shrink width or height of input.</li> <li> <p>Then 1x1 conv can let you shrink(or increase) the channel dimension</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#inception-network","title":"Inception Network:","text":"<ul> <li> <p>It mixed the filters of different size or even types(ConV or Pool) in one block, let the model pick by itself instead of the need to pick one of them.</p> <ul> <li>Note - although it looks meaningless to use a Max pooling with Same Padding here.</li> <li>Note2 - The max-pool layer is followed by an 1x1 Conv layer because the \u201cChannel\u201d dimension doesn\u2019t match other outputs</li> </ul> <p></p> </li> <li> <p>HOWEVER, the computational cost is quite high here.</p> <ul> <li> <p>A \u201cBottleneck Layer\u201d is applied in the middle to shrink the channel dimension first</p> <p></p> </li> </ul> </li> <li> <p>At last, ONE inception block will look like this:</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#mobilenet","title":"MobileNet:","text":"<ul> <li>As the name suggested, this network is specialized at devices with lower computational power.</li> <li>Key concept of MobileNet - <code>Depth-wise separable convolution</code><ul> <li>Recalling a NORMAL convolution<ul> <li>Each channel in each filter (e.g. 5x5 x3) is convolved with each channel in the input (e.g. 10x10 x3)  \u2192 output <code>1 channel</code> ONLY (e.g. 6x6 x1)</li> </ul> </li> <li> <p>What about Depth-wise separable convolution? It is split into 2 steps:</p> <ul> <li> <p>Depth-wise convolution</p> <ul> <li>Similar to normal convolution, EXCEPT the output is not <code>1 channel</code>. It holds the input channel dimension e.g. 10x10 x3 * 5x5 x3 \u21d2 6x6 x3</li> <li> <p>Each channel has their own convolution into output</p> <p></p> </li> </ul> </li> <li> <p>Point-wise convolution</p> <ul> <li> <p>The output above is then convolved with a number of 1 x n filters, each output with 1 channel will be stacked. i.e. the resolution is kept while changing the channel dimension</p> <p></p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Comparison of computational cost:</p> <ul> <li> <p>Normal convolution = \u201cparams in filters\u201d x \u201chow many elements in one output channel\u201d i.e. \u201c3x3x3 x5\u201d x \u201c4x4\u201d = 2160</p> <p></p> </li> <li> <p>Depth-wise Separable convolution:</p> <ul> <li>Depth-wise = \u201cparams in filter\u201d x \u201celements in one channel of output\u201d  i.e. \u201c3x3 x3\u201d x \u201c4x4\u201d = 432</li> <li>Point-wise = \u201cparams in filters\u201d x \u201celements in one channel of output\u201d i.e. \u201c1x3 x5\u201d x \u201c4x4\u201d = 240</li> <li> <p>Sum = 672</p> <p></p> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#version-of-mobilenet-architecture","title":"Version of MobileNet Architecture:","text":"<ul> <li> <p>V1  and V2 is very similar except:</p> <ul> <li> <p>The residual block</p> <p></p> </li> <li> <p>The Bottleneck (expansion) layer</p> <p></p> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#efficientnet","title":"EfficientNet","text":"<ul> <li>The core concept is, a CNN can be fine tuned to optimize for different mobile device, by tuning<ul> <li>the input resolution</li> <li>network depth</li> <li>layers width</li> </ul> </li> <li>The difficult point is how to scale them according to different mobile device spec?</li> <li>EfficientNet is good at this (somehow, don\u2019t know how though)</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#practical-advice-for-conv","title":"Practical advice for Conv:","text":"<p>Reuse Open Source implementation:</p> <ul> <li>It is recommended to use the original implementation in Github or paper to save time and chance of error.</li> </ul> <p>Transfer Learning:</p> <ul> <li> <p>There can be various way of transfer learning:</p> <p></p> <ul> <li>1st, freeze the WHOLE network, except the top output layers(softmax).  We add a new softmax layer with the number of desired classes.<ul> <li>Remove original softmax because original softmax may have thousands of classes and possible that does not include our desired class.</li> <li>Use this method if our dataset is too small.</li> <li>i.e. only training our softmax layers</li> <li>NOTE - a trick to speed up - pre-compute the activation value in the last FROZEN layers by our datasets.<ul> <li>Why ? because the \u201chead\u201d of the network is frozen, the activation value must be the same for ALL epoch.  i.e. no need to pass through the frozen layers multiple times.</li> </ul> </li> </ul> </li> <li>2nd, freeze PARTIAL layers of the network.<ul> <li>IF we have a larger dataset, we can freeze less layers from the head.</li> <li>i.e. Train some of the layers and the output layer</li> </ul> </li> <li>3rd, Train the whole network<ul> <li>IF we have a lots of data</li> </ul> </li> </ul> </li> </ul> <p>Data Augmentation:</p> <ul> <li>this technique can always help to add more data, especially the variety</li> <li>Mirror, Random cropping, Rotation, Shearing</li> <li>Color shifting, PCA color augmentation</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#week-3","title":"Week 3","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#assignment-1_1","title":"Assignment 1:","text":"<ul> <li> <p>The output of object detection network:</p> <p></p> <ul> <li>The <code>p</code> represents the probability of having an object</li> <li>the <code>c</code> here can be stored in 2 representation:<ul> <li>1st type - an integers, representing the class number</li> <li>2nd type - extend to N elements, where N is the number of classes. And each value is a probability</li> </ul> </li> </ul> </li> </ul> <p>YOLO network:</p> <ul> <li>Good performance(accuracy) at inference and able to perform in real time.</li> <li>Rather than passing the portions of an image several times, YOLO only needs it once.</li> <li> <p>A useful visualization during debugging the output - color each cell by the class of the max probability.  (note: it is NOT segmentation or the detection result, just a way to debugging)</p> <p></p> </li> <li> <p>Another way of visualization is to print the bbox directly (but you barely see anything..)</p> <p></p> </li> </ul> <p>Non-Max suppression:</p> <p>The process has 2 steps:</p> <ul> <li>Thresholding - Get rid of boxes with a low score(chosen by ourselves).</li> <li> <p>Select only one box when several boxes overlap with each other and detect the same object, by using the IoU metric.</p> <p></p> </li> </ul> <p>TF usage:</p> <pre><code># box_scores shape: (19, 19, 5, 80), last axis is all probability\n# return the indexes(int) where the elements is max value\n# i.e. output shape (19, 19, 5), each elements are the indexes\nbox_classes = tf.math.argmax(box_scores,axis=-1)\n# Reduce the last axis by selecting the element of max value\n# e.g. input shape: (19,19,5,80)\n# output shape: (19,19,5), each elements are the max value from the \"80\" elements\nbox_class_scores = tf.math.reduce_max(box_classes, axis=-1)\n# Apply the mask to an array\n# e.g. the mask is like [ [True, False, False, ....], ....]\n# filtering_mask shape: (19,19,5)\n# box_class_scores shape: (19,19,5)\n# so the output will be (19, 19) if there is only 1 true\nscores = tf.boolean_mask(box_class_scores, filtering_mask) # 19,19\n# Apply NMS\n# max_boxes_tensor is how many boxes to keep\nmax_boxes_tensor = tf.Variable(5, dtype='int32')\n# boxes shape:  (19,19, 4) (bounding boxes in all cells)\n# scores shape: (19,19) (the scores in each cell)\nnms_indices = tf.image.non_max_suppression(\nboxes,\nscores,\nmax_boxes_tensor,\niou_threshold=0.5\n)\n# Pick up elements from scores (19,19) by the given indices\nscores = tf.gather(scores,nms_indices)\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#assignment-2","title":"Assignment 2:","text":"<p>U-Net:</p> <ul> <li> <p>U-Net builds on</p> <ul> <li>a previous architecture called the Fully Convolutional Network, or FCN, which replaces the dense layers found in a typical CNN with a transposed convolution layer that upsamples the feature map back to the size of the original input image, while preserving the spatial information.</li> <li>Instead of one transposed convolution at the end of the FCN, U-Net uses a match number of ConV in downsampling &amp; upsampling.</li> <li> <p>Plus, it improves FCN by adding skip connections, to retain information that would otherwise become lost during encoding.</p> <p></p> </li> </ul> </li> <li> <p>Reason:  the traditional CNN will lose the track of where each \u201cpixel\u201d was, and this spatial information is important in segmentation task.</p> </li> <li>A bonus of using transpose convolutions is that the input size no longer needs to be fixed, as it does when dense layers are used.</li> </ul> <p>Sparse categorical crossentropy VS categorical crossentropy:</p> <ul> <li>Previous we always used categorical crossentropy as loss function.<ul> <li>i.e. the Label dataset is in the one-hot encoding form - [0, 0, 1, 0, \u2026]</li> </ul> </li> <li>Sparse categorical crossentropy is more efficient than other loss functions when you're dealing with lots of classes.<ul> <li>i.e. the ground truth element in Label dataset is just an int - e.g. 2</li> </ul> </li> </ul> <p>TF code:</p> <pre><code># Return list of target files, useful if input is an expression(glob pattern) such as /abc/*.py\n# e.g. mask_list = [ ..., './data/CameraMask/002128.png', .....]\n# https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files\nmask_list_ds = tf.data.Dataset.list_files(mask_list, shuffle=False)\n# Example of loading image file into tf.Tensor\ndef process_path(image_path):\nimg = tf.io.read_file(image_path)\nimg = tf.image.decode_png(img, channels=3)\n# tf.image.convert_image_dtype with tf.float32 will normalize pixels between 0 and 1 automatically (by 255)\nimg = tf.image.convert_image_dtype(img, tf.float32)\nreturn img\n# Resize image\n# https://www.tensorflow.org/api_docs/python/tf/image/resize\ninput_image = tf.image.resize(image, (96, 128), method='nearest')\nplt.imshow(\n# Convert the tensor back to image?\ntf.keras.preprocessing.image.array_to_img(display_list[i])\n)\n# Sparse categorical crossentropy\ntf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#week-4","title":"Week 4","text":"<p>Question:</p> <ul> <li>For face recognition, so how can the real production model \u201cinfer\u201d WHO is the person in the input image. By query &amp; comparing the encoding with our owned database?</li> </ul> <p>Visualizing what Conv Network is learning inside?</p> <ul> <li>In the lecture video, Andrew showed that nine portions of images with max activation, from a picked neuron unit in a hidden layer.</li> <li>So where do those images come from ? And what this Neuron unit means in a Conv network?<ul> <li>https://community.deeplearning.ai/t/what-are-deep-convnets-learning/39345</li> <li>As explained in the post above, when an area of the input image is convolved with ONE filter, it results in a single output value (activation). This value is the neuron unit claimed above.</li> <li>And by tracking which portion of the input image was used to generated this activation value, we can show the corresponding image portion, that results in max activation.</li> <li>i.e. the image portions in lecture came from the input image, it is NOT the activation output itself.</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#assignment-1_2","title":"Assignment 1:","text":"<ul> <li>Face Verification\u00a0\"Is this the claimed person?\" For example, at some airports, you can pass through customs by letting a system scan your passport and then verifying that you (the person carrying the passport) are the correct person. A mobile phone that unlocks using your face is also using face verification. This is a 1:1 matching problem.</li> <li>Face Recognition\u00a0\"Who is this person?\" For example, the video lecture showed a\u00a0face recognition video\u00a0of Baidu employees entering the office without needing to otherwise identify themselves. This is a 1:K matching problem.</li> </ul> <p>Paper - FaceNet:</p> <ul> <li>FaceNet learns a neural network that encodes a face image into a vector of 128 numbers. By comparing two such vectors, you can then determine if two pictures are of the same person.</li> <li>Paper - https://arxiv.org/pdf/1503.03832.pdf</li> </ul> <p>The Triplet Loss function:</p> <ul> <li> <p>During the training, it tries to bring the encodings of two images of the same person (Anchor and Positive) closer together,  while bringing the encodings of two images of different persons (Anchor, Negative) further apart.</p> <p></p> <ul> <li>The term (1) is the squared distance between the anchor \"A\" and the positive \"P\" for a given triplet; you want this to be small.</li> <li>The term (2) is the squared distance between the anchor \"A\" and the negative \"N\" for a given triplet, you want this to be relatively large.</li> <li><code>\u03b1</code>\u00a0is called the margin. It's a hyperparameter that you pick manually. e.g. \u03b1=0.2.</li> </ul> </li> </ul> <p>TF code:</p> <pre><code># Loads an image into PIL format.\n# https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img\na = tf.keras.preprocessing.image.load_img(\"images/danielle.png\", target_size=(160, 160))\n# then you can display the image in notebook by just typing : a\n# Sometimes the following code will be used together\ninput_arr = tf.keras.utils.img_to_array(a)\ntf.reduce_sum()\ntf.square()\ntf.subtract()\ntf.add()\ntf.maximum()\ntriplet_loss(y_true, y_pred_perfect, 1)\nimg_to_encoding(image_path, model)\nnp.linalg.norm\n# UNQ_C2(UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n# GRADED FUNCTION: verify\ndef verify(image_path, identity, database, model):\n\"\"\"\n    Function that verifies if the person on the \"image_path\" image is \"identity\".\n    Arguments:\n        image_path -- path to an image\n        identity -- string, name of the person you'd like to verify the identity. Has to be an employee who works in the office.\n        database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n        model -- your Inception model instance in Keras\n    Returns:\n        dist -- distance between the image_path and the image of \"identity\" in the database.\n        door_open -- True, if the door should open. False otherwise.\n    \"\"\"\n### START CODE HERE\n# Step 1: Compute the encoding for the image. Use img_to_encoding() see example above. (\u2248 1 line)\nencoding = img_to_encoding(image_path, model)\n# Step 2: Compute distance with identity's image (\u2248 1 line)\ndist = np.linalg.norm(tf.subtract(database[identity], encoding))\n# Step 3: Open the door if dist &lt; 0.7, else don't open (\u2248 3 lines)\nif dist &lt; 0.7:\nprint(\"It's \" + str(identity) + \", welcome in!\")\ndoor_open = True\nelse:\nprint(\"It's not \" + str(identity) + \", please go away\")\ndoor_open = False\n### END CODE HERE        \nreturn dist, door_open\n</code></pre> <p>Questions:</p> <p>In the code, we used <code>np.linalg.norm</code>, but what is this, and what is the relationship of L2 norm vs Euclidean distance</p> <ul> <li>https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance-be-calculated-with-numpy</li> <li>https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Convolutional%20Neural%20Networks%2033fb7882662d4e56b67973a38fec4a4d/#assignment-2_1","title":"Assignment 2:","text":"<p>The pretrained model used here is - VGG-19, a 19-layer version of the VGG network</p> <ul> <li>It was trained on the very large ImageNet Dataset.</li> </ul> <p>Neural Style Transfer:</p> <p>Here are the general flow of the training</p> <ol> <li>Load the content image</li> <li>Load the style image</li> <li>Randomly initialize the image to be generated</li> <li>Load the VGG19 model</li> <li>Compute the content cost</li> <li>Compute the style cost</li> <li>Compute the total cost</li> <li>Define the optimizer and learning rate</li> <li>Update the generated image until \u201ctraining\u201d finish</li> </ol> <p>In the process, we are NOT training the VGG19 network, we are, sort of, borrowing its function to encode the image. So we will actually use its activation values from a list of selected layers. To do this, we readjust the model to make it output a list of layers activation</p> <pre><code># e.g.\nlayer_names = [('block5_conv4', 1), ...]\noutputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\nmodel = tf.keras.Model([vgg.input], outputs)\n# get the output like this\ncontent_target = vgg_model_outputs(content_image)\n</code></pre> <p>Generated Image:</p> <ul> <li>How to prepare the generated image during training?</li> <li> <p>It is, at first, initiated randomly, i.e. a noisy image</p> <ul> <li>tips - to speed up training, initialize the random image FROM content images.</li> </ul> <pre><code># e.g. add a random number (either -ve or +ve)\nnoise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\ngenerated_image = tf.add(generated_image, noise)\n# remember to clip the value by boundary\ngenerated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n</code></pre> </li> </ul> <p>Cost Function:</p> <p>In Neural style transfer, the cost function is to optimize two things:</p> <ul> <li> <p>The generated image CONTENT matches the input content image</p> <ul> <li>To do this, the activation output values of the layer in the MIDDLE of the network are usually used.<ul> <li>Reason - not too deep and not too shallow so it can contains high and low level features.</li> </ul> </li> <li> <p>Then the Cost function of CONTENT will be calculate like this: (note - <code>a</code> is the activation output in the selected layer)</p> <p></p> </li> <li> <p>There is an optional step to flatten the 3D array into 2D for simpler calculation.</p> <p></p> </li> </ul> </li> <li> <p>The generated image STYLE matches the input style image</p> <ul> <li> <p>First, need to calculate a Style Matrix (a.k.a. Gram Matrix)</p> <ul> <li>In linear algebra, the Gram matrix G of a set of vectors \\((v_{1},\\dots ,v_{n})\\) is the matrix of dot products, whose entries are \\({\\displaystyle G_{ij} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j}) }\\).<ul> <li>In other words, \\(G_{ij}\\) compares how similar \\(v_i\\) is to \\(v_j\\).  If they are similar, they have a large dot product, then \\(G_{ij}\\) will be large.</li> </ul> </li> <li> <p>The illustration:</p> <p></p> </li> <li> <p>The value \\(G_{(gram)i,j}\\) measures how similar the activations of filter <code>i</code> are to the activations of filter <code>j</code>,</p> <ul> <li>i.e. it is a correlation</li> </ul> </li> <li>The diagonal elements \\(G_{(gram)i,i}\\) measure how \"active\" the filter\u00a0<code>i</code> is.<ul> <li>i.e. prevalence of patterns or textures</li> <li>e.g. suppose filter\u00a0<code>i</code>\u00a0is detecting vertical textures in the image. Then \\(G_{(gram)i,i}\\)\u00a0measures how common vertical textures are in the image as a whole.</li> <li>If the value is large, this means that the image has a lot of vertical texture.</li> </ul> </li> <li>The Style matrix\u00a0Ggram\u00a0measures the style of an image.<ul> <li>Second, calculate the STYLE cost function for each layers of SOME selected layers</li> </ul> </li> <li> <p>Note, it is different with the content encoding above, which only used ONE layer.</p> <p></p> <p></p> </li> </ul> </li> </ul> </li> <li> <p>At last, combine two cost function</p> <ul> <li> <p>\u2022 <code>\u03b1</code>\u00a0and\u00a0<code>\u03b2</code>\u00a0are hyperparameters that control the relative weighting between content and style.</p> <p></p> </li> </ul> </li> </ul> <p>TF code</p> <pre><code># To retrieve dimensions from a tensor X, use: X.get_shape().as_list()\n# Unroll\n# shape to change from  (\ud835\udc5a,\ud835\udc5b\ud835\udc3b,\ud835\udc5b\ud835\udc4a,\ud835\udc5b\ud835\udc36)  to  (\ud835\udc5a,\ud835\udc5b\ud835\udc3b\u00d7\ud835\udc5b\ud835\udc4a,\ud835\udc5b\ud835\udc36) .\n# tf.reshape(tensor, shape)\n# e.g.  tf.reshape(a_C, shape=[m, n_H * n_W, n_C]) gives the same result as tf.reshape(a_C, shape=[m, -1, n_C]).\n# \n# To re-order the dimensions, you can use tf.transpose(a_C, perm=[0,3,1,2]) \n# to changes the dimensions from  (\ud835\udc5a,\ud835\udc5b\ud835\udc3b,\ud835\udc5b\ud835\udc4a,\ud835\udc5b\ud835\udc36)  to  (\ud835\udc5a,\ud835\udc5b\ud835\udc36,\ud835\udc5b\ud835\udc3b,\ud835\udc5b\ud835\udc4a) .\n# Cost formula\n#tf.reduce_sum, tf.square and tf.subtract.\n# Dot Product\n# e.g. ask the function to do transpose for us\nresult = tf.linalg.matmul(a, b, transpose_b=True)\ncompute_content_cost(a_C, a_G)\nGA = gram_matrix(A)\ncompute_layer_style_cost(a_S, a_G)\nJ_style = compute_style_cost(a_S, a_G)\nJ = total_cost(J_content, J_style)\ntrain_step(generated_image)\ncontent layer\n&lt;tf.Tensor 'block5_conv4/Relu:0' shape=(None, 25, 25, 512) dtype=float32&gt;\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/","title":"Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#cite","title":"Cite","text":"<p>As in copyright:</p> <p>DeepLearning.AI\u00a0makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite\u00a0DeepLearning.AI \u00a0as the source of the slides.</p> <ul> <li>Slides here are from DeepLearning.AI</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#important-deadline","title":"Important Deadline","text":"Week Deadline 1 11 Sep 2 18 Sep 3 25 Sep"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#goal","title":"Goal","text":"<ul> <li>Practical aspects of deep learning</li> <li>Learn about hyperparameter tuning, regularization</li> <li>How to diagnose bias, and variants, and advance optimization algorithms, like momentum, armrest, prop, and the ad authorization algorithm</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#week-1-practical-aspect-of-deep-learning","title":"Week 1 - Practical Aspect of Deep Learning","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#train-dev-test-dataset-splitting","title":"Train / Dev / Test dataset splitting:","text":"<ul> <li>Training set is used for training</li> <li>Dev set (i.e. Hold-out cross validation set), is used to cross validate performance of several models</li> <li>Test set is an unbiased dataset to estimate how well your model is<ul> <li>Difference with Dev set \u2192 Your model actually has bias to the Dev set as well while tuning / cross validation</li> </ul> </li> </ul> <p>Dataset Distribution:</p> <ul> <li>1st rule - keep all split dataset from the same distribution<ul> <li>e.g.</li> </ul> </li> </ul> <p>Splitting ratio:</p> <ul> <li>It is common to split it like 70/30 or 60/20/20, IF THE DATASET IS NOT EXTREMELY HUGE</li> <li>If the dataset is like &gt; 1,000,000, you can also set a max of dev/test dataset to be e.g. 1000 (i.e.  a constant)</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#bias-variance","title":"Bias &amp; Variance:","text":"<ul> <li>High Bias = underfitting</li> <li>High Variance = overfitting</li> <li>Note!!!! It is possible to be high bias &amp; high variance at the same time.</li> </ul> <p>How we determine them from the metrics ?</p> <p>Assume the bass error is ~ 0% (e.g. Human can almost identify if it is a cat image by 100% accuracy)</p> Bias/Variance Train set Error Dev set Error High bias 15% 16% High variance 1% 11% Both High!!! 15% 30%"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#solution-to-high-bias-variance","title":"Solution to high bias / variance :","text":"<p>High Bias </p> <ul> <li>It is <code>**underfitting**</code> to the training set (i.e. Model cannot \u201clearn\u201d enough from the training set features)</li> <li>Use a larger neural network with more layers or units</li> <li>Change algorithm</li> </ul> <p>High variance</p> <ul> <li>It is <code>**overfitting**</code> to the training set</li> <li>Use more data to train the network</li> <li>Regularization</li> <li>Change algorithm</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#overfittingunderfitting-regularization-techniques","title":"Overfitting/underfitting - Regularization Techniques:","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#l2-norm","title":"L2 norm","text":"<ul> <li>Why not L1 norm ?<ul> <li>It is said that L1 norm would make parameters matrix to be <code>Sparse</code> (i.e. many zeros in matrix)</li> </ul> </li> <li>In calculating Cost func, add the L2 \u201cpenality\u201d</li> <li>Then in the back propagation, this is added to calculating gradient</li> <li>The <code>lambda</code> value here is a hyperparameter</li> </ul> <ul> <li>How does it prevent overfitting ????<ul> <li>If lambda is large, the result W will be small (more close to zero)</li> <li>Then, the Z will be fairly small and the activation function is roughly linear</li> <li>Recall from previous course:<ul> <li>if Activation function is linear, the whole network tends to behave as a small network.</li> <li>i.e. It acts like a linear network and avoid overfitting</li> </ul> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#dropout-regularization","title":"Dropout Regularization","text":"<ul> <li>In every propagation, each units are decided randomly (by a configured probability, e.g. 0.5)</li> <li>i.e. each propagation uses a different set of units</li> <li>Note : the probability can be different between each layers e.g. last layer would not be overfitting so its probability is set to 1.0</li> <li>So, the <code>probability</code> here is a hyperparameter</li> </ul> <ul> <li>Cautious !!! The activation value after dropout, need to be divide by the probability Because it is to keep the expected value to be the same level</li> <li>Cautious 2 !!! When doing a test prediction, do not apply dropout i.e. ONLY use dropout when \u201ctraining\u201d the model</li> </ul> <ul> <li>How does it work???<ul> <li>Somehow it forces the unit to NOT rely heavily on a specific features i.e. avoid overfitting</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#other-regularization-methods","title":"Other Regularization Methods","text":"<ul> <li>Data augmentation<ul> <li>To avoid heavy data annotation cost / impossible to get more data</li> <li>It increases the data size in a cheaper way</li> </ul> </li> <li>Early Stopping<ul> <li>The loss of dev dataset drops  \u2192 increase after certain iteration</li> <li>It usually means overfitting takes place</li> </ul> </li> </ul> <p>An additional important concept here :  It is better to tune hyperparameters of the same purpose at a certain time, not all together e.g. Tune those for optimizing Cost Function, then tune those for avoid overfitting\u2026etc </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#optimization-normalize-input-data","title":"Optimization - Normalize Input Data:","text":"<ul> <li>Why need to normalize?<ul> <li>To speed up the training, because the gradient descent can reach global minima faster</li> </ul> </li> </ul> <p>How does it work ?</p> <p>One of the normalization method is called - standard scalar</p> <p>Standardize features by removing the mean and scaling to unit variance. </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#standard-scalar-normalization","title":"Standard Scalar normalization","text":"<ul> <li>1st step - Subtract / Zero out the mean of <code>TRAINING DATA</code><ul> <li>So the input has a Zero mean</li> </ul> </li> </ul> <ul> <li>2nd step - normalize the variance<ul> <li>e.g. x1 has a larger variance than x2</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#some-other-common-normalization","title":"Some other common normalization","text":"<ul> <li>Such as divide an image input by 255 if it is all RGB channel</li> </ul> <p>Cautious !!! Use the calculated mean and sigma in test set or prediction as well Do not calculate a new one</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#optimization-vanishing-exploding-gradient","title":"Optimization - Vanishing / Exploding Gradient","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#what-is-this-problem-about","title":"What is this problem about  ???","text":"<p>It can be proved by a simplified neural network, and we can know the output could be very large or small, depending on the number of layers.</p> <p>Note that activation function was assumed to be nothing here. So in fact the output will not be REALLY large, but it is the concept only.</p> <p>The output is still large enough so the training become very slow.</p> <p>For example of a sigmoid activation function The slope near z=<code>0</code> or near <code>1</code> is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#solution-weight-initialization","title":"Solution - weight initialization","text":"<p>It is just a partial solution but it is still helpful. By using a correct weight initialization technique.</p> <ul> <li>For each layer, init the <code>w</code> by multiply it with square root of <code>1/n</code> , where n is the unit number of previous layer(or input)<ul> <li>Reason - so the initial <code>w</code> value is NOT too small or too large</li> </ul> </li> <li>For ReLU, it is said that using <code>2/n</code> is a better value, it is called <code>He initialization</code>, because it is named for the first author of He et al., 2015</li> <li>For Tanh, <code>1/n</code>, it is also called <code>**Xavier initialization**</code></li> <li>For some other way, for example, <code>2/(n of l-1 + n of l)</code></li> </ul> <p>Also, use <code>np.random.randn</code>, because it produces numbers in a normal distribution.</p> <p>When used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range. </p> <p></p> <p>A conclusion of weight initialization :</p> Model Train accuracy Problem/Comment 3-layer NN with zeros initialization 50% fails to break symmetry 3-layer NN with large random initialization 83% too large weights 3-layer NN with He initialization 99% recommended method"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#gradient-checking","title":"Gradient Checking:","text":"<p>It is a debugging step to ensure the gradient descent step you built is correct (especially back propagation)</p> <p>How it works ???</p> <ul> <li>Calculate the <code>gradient</code> via back propagation<ul> <li>hints: convert them into 1-dim array (dW1, dB1, dW2, dB2\u2026)</li> </ul> </li> <li>Calculate the <code>approximate gradient</code> manually, by using theta + - epsilon For EACH parameters<ul> <li>hints: for each comparison(e.g. <code>dW1</code> vs <code>approx_dW1</code>), keep all other parameters unchanged</li> <li>1st - get TWO Cost <code>J</code> via forward propagation of (W1+/-epsilon, B1, W2, B2\u2026)</li> <li>2nd - calculate the slope(gradient) by \\(\\frac {J(+) - J(-)} { 2 epsilon}\\)</li> </ul> </li> <li> <p>At last, compare both 1-dim array <code>grad</code> vs <code>approx_grad</code> \\(\\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 }\\)</p> <pre><code>numerator = np.linalg.norm( (grad - gradapprox) )\ndenominator = np.linalg.norm( grad ) + np.linalg.norm( gradapprox )\ndifference = numerator / denominator\n</code></pre> </li> </ul> <p></p> <p>Note</p> <ul> <li>It cannot be used in dropout regularization<ul> <li>solution - turn off dropout and do a gradient check first</li> </ul> </li> <li>Only use it for debugging, not in the real training process</li> <li>A very rare case<ul> <li>maybe grad check is correct when parameters are close to zero But incorrect when they are not.</li> <li>So in this case, can do a check after random init And do a new check after some training</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#week-2-optimizing-algorithms","title":"Week 2 - Optimizing Algorithms","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#batch-mini-batch-stochastic-gradient-descent","title":"Batch / Mini-batch / Stochastic gradient descent:","text":"<p>Batch = Calculate gradient every whole training set , e.g. 1000 sample</p> <p>Mini-batch = Calculate it for every subset of the whole training set, e.g. every 500 sample, i.e. 2 batches here</p> <p>Stochastic = Calculate it for every SINGLE sample</p> <p>Batch method only update after whole dataset, so the training speed can become slow.</p> <p>Stochastic update the parameters for every sample, so it cannot make the most of vectorization, i.e. the calculation is slower, and the worst case is that it never reaches global minima.</p> <p>As a result, mini-batch takes the good part from both of them.</p> <p>Normally, the <code>batch size</code> is set to a number of \\(2^{x}\\) (e.g. 64, 128\u2026etc) to fit the CPU/GPU memory</p> <p></p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#fundamental-concept-exponentially-weighted-averages","title":"Fundamental concept - exponentially weighted averages:","text":"<p>This part is a fundamental knowledge before learning other optimization algorithms other than Gradient Descent.</p> <p>For example it is the core concept of Gradient Descent with momentum. </p> <p>How it works:</p> <p>We can use <code>exponentially weighted averages</code> , to draw an average line. (Here is a simplified calculation, without bias correction yet.)</p> <p>e.g. the point at <code>V1</code> is calculated by a weighted sum of previous <code>V0</code> and current real value.</p> <p>\\(V_t = \u03b2 V_{t-1} + (1-\u03b2) \u03b8_t\\)</p> <p></p> <p>How <code>\u03b2</code> affects the averaged values ?</p> <p>So the weight <code>\u03b2</code> here  controls how many past records make influence to the current value. </p> <ul> <li>green line = larger beta, it appears to shift right side because it is \u201cslower to change\u201d</li> <li>yellow = small beta, the past values have less influence</li> </ul> <p></p> <p>Bias Correction:</p> <p>The above simplified calculation has a very small value in the early records. So we sometimes apply a <code>Bias correction</code>.</p> <p>The <code>V</code> value is calculated in the same way but divided by \\((1-\u03b2^{t})\\) at last, where <code>t</code> is the # of the record. So the calculated average value is more close to where they should be.</p> \\[ V_t = \\frac { \u03b2 V_{t-1} + (1-\u03b2) \u03b8_t} {1-\u03b2^t}  \\] <p>When <code>t</code> is small (but not zero), <code>V</code> is larger.</p> <p>When <code>t</code> is larger (i.e. in a later point of data), this value become ~ 1 so it takes very less effect later, which is what we want.</p> <p>Just keep in mind that if the iteration is large, most people will just ignore this bias correction.</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#gradient-descent-with-momentum","title":"Gradient Descent with Momentum:","text":"<p>By applying exponentially weighted average during calculating the derivatives:</p> <p>\\(v_{dW} = \u03b2  v_{dW} + (1-\u03b2)dW \\\\ v_{db} = \u03b2  v_{db} + (1-\u03b2)db \\\\ Then, W = W - \u03b1 v_{dW}, b = b - \u03b1 v_{db}\\)</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#quiz-notes","title":"Quiz notes:","text":"<pre><code>v0 = 0\nv1 = 0.5*v0 + 0.5*10\nv2 = 0.5*v1 + 0.5*25 = 15\n# I thought we should use corrected v1 to calculate v2 but wrong...\ncorrected_v1 = 5 / (1-0.5^1)\ncorrected_v2 = (0.5*corrected_v1 + 0.5*25)/(1-0.5^2)\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#assignment","title":"Assignment:","text":"<ul> <li>Common values for\u00a0\ud835\udefd\u03b2\u00a0range from 0.8 to 0.999. If you don't feel inclined to tune this,\u00a0\ud835\udefd=0.9\u03b2=0.9\u00a0is often a reasonable default.</li> </ul> <pre><code>parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n\"\"\"\nparameters -- python dictionary containing your parameters to be updated:\n                parameters['W' + str(l)] = Wl\n                parameters['b' + str(l)] = bl\ngrads -- python dictionary containing your gradients to update each parameters:\n                grads['dW' + str(l)] = dWl\n                grads['db' + str(l)] = dbl\nlearning_rate -- the learning rate, scalar.\n\"\"\"\n# below codes are provided\nfirst_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\nsecond_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\nmini_batches = random_mini_batches(X, Y, mini_batch_size)\n\"\"\"\nArguments:\nX -- input data, of shape (input size, number of examples)\nY -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\nmini_batch_size -- size of the mini-batches, integer\nReturns:\nmini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n\"\"\"\nv = initialize_velocity(parameters)\n\"\"\"\nInitializes the velocity as a python dictionary with:\n            - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\nArguments:\nparameters -- python dictionary containing your parameters.\n                parameters['W' + str(l)] = Wl\n                parameters['b' + str(l)] = bl\nReturns:\nv -- python dictionary containing the current velocity.\n                v['dW' + str(l)] = velocity of dWl\n                v['db' + str(l)] = velocity of dbl\n\"\"\"\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#week-3","title":"Week 3","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Improving%20Deep%20Neural%20Networks%20Hyperparameter%20Tuni%20b0fd7167ddb34984a40b0dd550166167/#assignment_1","title":"Assignment:","text":"<p>H5PY:</p> <pre><code># How to use h5py\nimport h5py\ntrain_dataset = h5py.File('datasets/train_signs.h5', \"r\")\ntest_dataset = h5py.File('datasets/test_signs.h5', \"r\")\n</code></pre> <p>Tensorflow:</p> <pre><code># Access\n## Tensor not allow you to print content directly, but have to use iterator/for-loop\nprint(next(iter(new_train)))\n## Print info of a tensor\nnew_train.element_spec\n\"\"\"Output\nTensorSpec(shape=(12288,), dtype=tf.float32, name=None)\n\"\"\"\n# Declaration\n## constant cannot be modified later\nX = tf.constant(np.random.randn(3,1), name = \"X\")\n## Variable can be modified\nb = tf.Variable(np.random.randn(4,1), name = \"b\")\ntf.zeros()\ntf.ones()\n# Operation\n## Cast var\nimage = tf.cast(image, tf.float32) / 255.0\n## reshape like numpy\nimage = tf.reshape(image, [-1,])\n## = np_arr.T\ntf.transpose(minibatch)\n## tf cannot do vectorization like numpy, so need to use a map func\nnew_test = x_test.map(normalize_func)\n## Add 2 tensor\nY = tf.add(X, b)\ntf.math.add\n## dot matrix\nC = tf.matmul(W, X)\ntf.linalg.matmul\n## Pair up two tensor\ndataset = tf.data.Dataset.zip((X_train, Y_train))\n# ML func\n## activation func\na = tf.keras.activations.sigmoid(z)\na = tf.keras.activations.softmax(z)\na = tf.keras.activations.relu(z)\n## one-hot encoding\nindices = [0, 1, 2]\ndepth = 3\ntf.one_hot(indices, depth)  \n\"\"\"output: [3 x 3]\n[[1., 0., 0.],\n[0., 1., 0.],\n[0., 0., 1.]]\n\"\"\"\n## Initialize vectors - glorot initializer, it draws numnber from a truncated normal distribution\ninitializer = tf.keras.initializers.GlorotNormal(seed=1)\nW1 = tf.Variable(initializer(shape=(25,12288)))\n## Split dataset into minibatches AND preload for streaming(by prefetch)\nminibatches = dataset.batch(minibatch_size).prefetch(8)\n## Training - calculate backpropagation automatically\nwith tf.GradientTape() as tape:\ny_pred = forward_propagation(X, parameters)   # 1. predict, (the func is custom made)\nminibatch_cost = compute_cost(y_pred, y_true)   # 2. loss, (the compute_cost is custom made, return a cost value)\ntrainable_variables = [W1, b1, W2, b2, W3, b3] # define what will be updated during backprop\ngrads = tape.gradient(minibatch_cost, trainable_variables) # auto-calculate backprop\n## Cost func of category classification (categorical_crossentropy)\nbefore_sum = tf.keras.losses.categorical_crossentropy(\ny_true_onehot, y_pred, from_logits=True # if false, logits should be probability\n)\n## Optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate) # init\noptimizer.apply_gradients(zip(grads, trainable_variables)) # update \n## Metrics\n## The CategoricalAccuracy will track the accuracy for this multiclass problem\ntest_accuracy = tf.keras.metrics.CategoricalAccuracy() # init\ntrain_accuracy.update_state(y_true, y_pred) # update metric\ntrain_accuracy.reset_states() # reset to 0, e.g. starting of each epoch\n</code></pre> <p>Sample of model fitting with tf</p> <pre><code>def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\nnum_epochs = 1500, minibatch_size = 32, print_cost = True):\n\"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.\n    Arguments:\n    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 10 epochs\n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\ncosts = []                                        # To keep track of the cost\ntrain_acc = []\ntest_acc = []\n# Initialize your parameters\n#(1 line)\nparameters = initialize_parameters()\nW1 = parameters['W1']\nb1 = parameters['b1']\nW2 = parameters['W2']\nb2 = parameters['b2']\nW3 = parameters['W3']\nb3 = parameters['b3']\noptimizer = tf.keras.optimizers.Adam(learning_rate)\n# The CategoricalAccuracy will track the accuracy for this multiclass problem\ntest_accuracy = tf.keras.metrics.CategoricalAccuracy()\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy()\ndataset = tf.data.Dataset.zip((X_train, Y_train))\ntest_dataset = tf.data.Dataset.zip((X_test, Y_test))\n# We can get the number of elements of a dataset using the cardinality method\nm = dataset.cardinality().numpy()\nminibatches = dataset.batch(minibatch_size).prefetch(8)\ntest_minibatches = test_dataset.batch(minibatch_size).prefetch(8)\n#X_train = X_train.batch(minibatch_size, drop_remainder=True).prefetch(8)# &lt;&lt;&lt; extra step    \n#Y_train = Y_train.batch(minibatch_size, drop_remainder=True).prefetch(8) # loads memory faster \n# Do the training loop\nfor epoch in range(num_epochs):\nepoch_cost = 0.\n#We need to reset object to start measuring from 0 the accuracy each epoch\ntrain_accuracy.reset_states()\nfor (minibatch_X, minibatch_Y) in minibatches:\nwith tf.GradientTape() as tape:\n# 1. predict\nZ3 = forward_propagation(tf.transpose(minibatch_X), parameters)\n# 2. loss\nminibatch_cost = compute_cost(Z3, tf.transpose(minibatch_Y))\n# We accumulate the accuracy of all the batches\ntrain_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\ntrainable_variables = [W1, b1, W2, b2, W3, b3]\ngrads = tape.gradient(minibatch_cost, trainable_variables)\noptimizer.apply_gradients(zip(grads, trainable_variables))\nepoch_cost += minibatch_cost\n# We divide the epoch cost over the number of samples\nepoch_cost /= m\n# Print the cost every 10 epochs\nif print_cost == True and epoch % 10 == 0:\nprint (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\nprint(\"Train accuracy:\", train_accuracy.result())\n# We evaluate the test set every 10 epochs to avoid computational overhead\nfor (minibatch_X, minibatch_Y) in test_minibatches:\nZ3 = forward_propagation(tf.transpose(minibatch_X), parameters)\ntest_accuracy.update_state(minibatch_Y, tf.transpose(Z3))\nprint(\"Test_accuracy:\", test_accuracy.result())\ncosts.append(epoch_cost)\ntrain_acc.append(train_accuracy.result())\ntest_acc.append(test_accuracy.result())\ntest_accuracy.reset_states()\nreturn parameters, costs, train_acc, test_acc\n</code></pre> <p>One Hot encodings</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/","title":"Neural Networks and Deep Learning","text":"<p>Some images and text are from https://www.deeplearning.ai/ </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#cite","title":"Cite","text":"<p>As in copyright:</p> <p>DeepLearning.AI\u00a0makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite\u00a0DeepLearning.AI \u00a0as the source of the slides.</p> <ul> <li>Slides here are from DeepLearning.AI</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#important-deadline","title":"Important Deadline","text":"Week Deadline 1 14 Aug 2 21 Aug 3 28 Aug 4 4 Sep"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#goal-of-this-course","title":"Goal of this course","text":"<ul> <li>Build neural network, train it with data</li> <li>Example  - recognize cat</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#week-1-introduction-to-deep-learning","title":"Week 1 - Introduction to Deep Learning","text":"<p>ReLU function</p> <ul> <li>rectified linear unit</li> <li>rectify = max( X, 0 ) to ensure always \u2265 0</li> </ul> <p>Densely connected</p> <ul> <li> <p>e.g. All input features are connected to all nodes in the next layer, like below</p> <p></p> </li> </ul> <p>Structure tabular data \u2192 standard neural network architecture</p> <p>Computer vision \u2192 Convolutional neural network CNNs</p> <p>Sequence data (audio, text, time-series\u2026etc) \u2192 recurrent neural network RNNs</p> <p></p> <p>Thanks to DL, computer can understand more unstructured data than the past.</p> <p>(I.e. DL is very good at UNSTRUCTURED data)</p> <p>One of the reasons WHY DL become more popular in recent years</p> <ul> <li>Thanks to lifestyle technology advance, e.g. mobile, camera, sensors\u2026etc</li> <li>i.e. we got much more data</li> <li>(Just a conceptual chart) we can see,  traditional ML algorithm(red) doesn\u2019t know how to handle large size of data. While Neural Network(esp. larger size) are benefited from the large size of data.</li> </ul> <p></p> <ul> <li>So it is a simple chart to understand : if less data, we should go for simpler algorithm, or smaller NNs</li> </ul> <p>Another reason - Faster Computation</p> <ul> <li>Switching from Sigmoid func \u2192 ReLU func, it enables the training to be way more faster (I don\u2019t get it though. Maybe checkout the gradient descent formula, that relates to activation func)</li> </ul> <p></p> <p>So this cycle is faster</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#week-2-neural-network-basics","title":"Week 2 - Neural network basics","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#section-1-logistic-regression-as-a-neural-network","title":"Section 1 - Logistic Regression as a neural network","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#binary-classification","title":"Binary Classification","text":"<ul> <li>can use logistic regression</li> <li> <p>Equation looks like:</p> <p></p> </li> </ul> <p>Notation Example - classification of an image \u2192 cat or not</p> <ul> <li>Let\u2019s say image is 64 x 64 pixel, with RGB 3 channel</li> <li>input X - a feature vector with dimension \\(n_x\\) of  (64x64x3)<ul> <li>e.g. \\(x_1 = \\begin{bmatrix} 23 \\\\ 234 \\\\ 2 \\\\ ... \\end{bmatrix}\\) then, whole \\(X = \\begin{bmatrix} x_1, x_2, ..., x_m \\end{bmatrix}\\), with the shape of \\(( n_x, m )\\)</li> <li>Andrew claimed, put 1 entity as a column instead of a row that will be easier for later implementation</li> </ul> </li> <li>output Y - 1 or 0</li> </ul> <p>Its equation is very similar to linear regression, EXCEPT we want the output to be a probability(i.e. between 0 to 1). So we add a sigmoid function to the result of linear regression equation.</p> <p>i.e. y = sigmoid(wx + b)</p> <p>where w is an n dimension vector, b is a real number</p> <p></p> <p>Alternative notation: </p> <p>Sometimes people notate the equation slightly differently. They put <code>w</code> and <code>b</code> into the same parameters theta. The first element (theta zero) is actually <code>b</code>, and then <code>x</code> become a <code>n+1</code> vector, with the x zero equal to 1.</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#cost-function","title":"Cost function:","text":"<p>For Logistic Regression, we cannot use the general square root error equation. Reason is there will not be easy to find global optima, but a lot of local optima.</p> <p>Instead, we would use some similar equations like this effect</p> <p></p> <p>Diff with <code>Loss function, Error function</code>???</p> <ul> <li>Loss/Error Function is how we measure the loss of ONE SINGLE training sample</li> <li>While Cost Function, is how we measure the ENTIRE training set</li> <li> <p>J(w, b) is the cost function, while L(y, y) is the Loss function</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#gradient-descent","title":"Gradient descent:","text":"<p>It is the process to minimize the cost function to the global optima, by updating parameters <code>w</code> and <code>b</code> with the step calculated by learning rate &amp; its slope</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#computation-graph","title":"Computation Graph:","text":"<ul> <li>Useful to understand the flow and the output we want to optimize.</li> <li>During a <code>forward propagation</code>, we calculate the <code>Cost Function</code></li> <li>During a <code>backward propagation</code>, we yield the <code>derivative</code></li> </ul> <p>For example,</p> <p>Let\u2019s say we want to see how <code>J</code> changes when <code>a</code> changes.</p> <ul> <li><code>J</code> changes when <code>v</code> changes \u2192 \\(\\frac{dJ}{dv}\\)</li> <li><code>v</code> changes when <code>a</code> changes \u2192 \\(\\frac{dv}{da}\\)</li> <li> <p>So, \\(\\frac{dJ}{da} = \\frac{dJ}{dv} . \\frac{dv}{da}\\) , which calls the <code>chain derivatives</code></p> <p></p> </li> <li> <p>Other parameter <code>b</code> or <code>c</code> are calculated in the same way, with 1 more layer though.</p> <ul> <li>\\(\\frac{dJ}{db} = \\frac{dJ}{dv}.\\frac{dv}{du}.\\frac{du}{db}\\) , and <code>c</code> is in the same way</li> </ul> </li> <li> <p>In the <code>Back Propagation</code> phrase, those (partial) derivatives are calculated, then each parameters can be updated, by a value depending on <code>derivatives</code> and <code>learning rate</code></p> <p></p> </li> </ul> <p>Note that the output is a <code>Loss Function</code> here (for ONE sample) By Backward propagation, we can calculate \\(\\frac{dL}{dz}\\) and reuse it, to calculate the derivatives of <code>w1</code>, <code>w2</code>, <code>b</code></p> <p></p> <p>Similarly, for the <code>Cost Function</code>, we just need to take average of the sum of <code>Loss Function</code> above (also same for other derivatives dw1, dw2, db), i.e.</p> \\[ \\frac{dJ}{dw^1} = \\frac{1}{m} \\displaystyle\\sum_{i=1}^m \\frac{dL}{dw^1} \\] <p>Side notes:</p> <p>In the above calculation, we may use a for loop to iterate <code>m</code> samples, and another nested for loop to iterate <code>n</code> parameters (e.g. w1 ~ wn). But in deep learning, we need to avoid for loop as much as we can. So a technique called <code>Vectorization</code> is important to get rid of most for loop.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#section-2-python-and-vectorization","title":"Section 2 - Python and Vectorization","text":"<p>In short, avoid using For loop to process list/matrix whenever possible.</p> <p>For example </p> <ul> <li>use numpy matrix operation (e.g. sum\u2026etc) for list/matrix</li> <li> <p>use Python Broadcasting feature to do calculation</p> <ul> <li>e.g.</li> </ul> <pre><code>A = [ 1, 2, 3 ] + 100\n# A: [ 101, 102, 103 ]\n</code></pre> </li> </ul> <p>Example of vectorizing the basic equation \\(z = w^{T}X + b\\)  in a single node</p> <ul> <li>\\(w =  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ ... \\end{bmatrix}\\) where each <code>w</code> is for multiplying each <code>x</code></li> <li>\\(X =  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\end{bmatrix}\\) where each x refer to each <code>Features</code></li> <li>So, \\(w^{T}X\\) is a scalar value</li> </ul> <p>Example of avoiding for loop in the calculation of <code>derivatives in Logistic regression</code></p> <p></p> <ul> <li>1st loop, looping <code>m</code> training example</li> <li>2nd loop, looping x1, x2, x3\u2026xn features in 1 sample</li> </ul> <p>Then the above 2 loops can be converted into Vectorization as below</p> <p></p> <p>Cautions! When dealing with vectorization operation, always avoid using rank 1 array. But you should explicitly create the proper dimension even it is with the shape <code>1</code>. As shown below </p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#numpy-exercise","title":"Numpy exercise:","text":"<p>math.exp() vs numpy.exp()</p> <ul> <li>We barely use math package in DL, because always handle matrix</li> <li>numpy automatically support input as real number, or vector, matrix\u2026etc</li> </ul> <p>np.reshape(-1)</p> <p>Normalization is important</p> <ul> <li>it allow ML/DL gradient descent to converge faster</li> <li>e.g. via <code>np.linalg.norm(x, axis=1, keepdims=True)</code></li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#week-3-shallow-neural-networks","title":"Week 3 - Shallow Neural Networks","text":"<p>A NN with one hidden layer is generally called <code>2 Layer NN</code></p> <p>Because the output layer is counted but the input layer is not.</p> <p>And input layer <code>X</code> sometimes is referred to <code>a[0]</code> while other layers i are <code>a[i]</code></p> <p></p> <p>In a logistic regression simple NN, each neuron (node) can be presented in 2 parts like this:</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#vectorization-of-one-layer","title":"Vectorization of one layer","text":"<p>From previous week2 lecture we learnt Vectorization of ONE NODE</p> <p>And here is the implementation on the one hidden layer, with multiple nodes</p> <p>superscript square bracket <code>[1]</code> = layer 1 subscript <code>1</code> = node 1 in this layer </p> <p>By recalling the basic vectorization in previous week2.</p> <ul> <li>\\(z^{[1]}_1\\)  is the value in node 1 in layer 1</li> <li>\\(w^{[1]}_1 =  \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ ... \\end{bmatrix}\\) , and   \\(x =  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ ... \\end{bmatrix}\\) , and  \\(b^{[1]}_1\\)  is a scalar</li> <li>So, \\(z^{[1]}_1 = w^{[1]T}_1 x + b^{[1]}_1\\)  is a scalar value</li> </ul> <p>From above image, each line of equation in the top right corner is for each nodes.</p> <p>By applying vectorization to the whole layer, e.g. layer 1 </p> <p></p> <p>$z^{[1]} = W^{[1]} x + b^{[1]} =  \\begin{bmatrix} w^{[1]T}_1 \\ w^{[1]T}_2 \\ ... \\end{bmatrix}</p> <p>\\begin{bmatrix} x_1 \\ x_2 \\ ... \\end{bmatrix}</p> <ul> <li> \\[\\begin{bmatrix} b^{[1]}_1 \\\\ b^{[1]}_2 \\\\ ... \\end{bmatrix}\\] </li> </ul> <p>\\$ \\(= \\overbrace{ \\begin{bmatrix} w^{[1]}_{1,1} &amp; w^{[1]}_{1,2} &amp; ... \\\\ w^{[1]}_{2,1} &amp; w^{[1]}_{2,2} &amp; ... \\\\ ... &amp; ... &amp; ... \\end{bmatrix} }^{W^{[1]}}  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\end{bmatrix} +  \\begin{bmatrix} b^{[1]}_1 \\\\ b^{[1]}_2 \\\\ ... \\end{bmatrix}\\)   , where \\(w^{[1]}_{1,2}\\)  ,for example, is <code>w</code>  in <code>node 1</code> for \\(x_2\\)</p> <p>$=</p> <p>\\begin{bmatrix} w^{[1]}{1,1}x_1 + w^{[1]}x_2 + ... + b^{[1]}1 \\ w^{[1]}x_1 + w^{[1]}_{2,2}x_2 + ... + b^{[1]}_2 \\ ... \\end{bmatrix}$</p> <p>\\(= \\begin{bmatrix} z^{[1]}_1 \\\\ z^{[1]}_2 \\\\ ... \\end{bmatrix}\\)</p> <p>For the shape of above equation:</p> <p>z (4, 1) = W (4, 3) x (3, 1) + b (4, 1)</p> <p>where <code>4</code> is nodes size, <code>3</code>is feature size</p> <p>Thus, the output of <code>layer 1</code> &amp; <code>layer 2</code> are as below:</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#vectorizing-across-the-whole-datasetx","title":"Vectorizing across the whole dataset(<code>X</code>):","text":"<p>From the final few equations above, for layer 1 and layer 2, we could think of a for loop easier like left side below. And then convert into vectorization as right side:</p> <ul> <li>\\(z^{[1](i)}\\)  with the shape (4,1)<ul> <li>\\(Z^{[1]}\\) with the shape (4, m) , where <code>m</code> is the size of dataset <code>X</code></li> </ul> </li> <li>\\(W^{[1]}\\) with the shape (4,3)</li> <li>X with the shape (3, m)</li> <li>\\(b^{[1]}\\) with the shape (4,1)</li> </ul> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#activation-function","title":"Activation function","text":"<p>Sigmoid:</p> <p>For Andrew, he claimed he barely used <code>sigmoid</code> EXCEPT when the output is a binary classification. i.e. 0 \u2264 y \u2264 1. Then the <code>sigmoid</code> is used in the output layer</p> <p>Note: so the activation function CAN be different between layers</p> <p>Tanh:</p> <p>In short, because it goes through the point (0,0), that the mean is near zero, easier for calculation. It is ALMOST alway better than <code>sigmoid</code>, EXCEPT when the output is in the range of (0,1)</p> <p>ReLU:</p> <p>However, both <code>sigmoid</code> &amp; <code>tanh</code> also have a downside. When the z (input of activation function)  is very large / very small, its derivative(slope) is very small.  i.e. the gradient descent is small and backward update will be very slow. In such cases, other activation such as <code>ReLU</code> can perform better. (rectifier linear unit)</p> <p>Or <code>leaky ReLU</code>...</p> <p></p> <p>In conclusion, choose the activation function depends on the output. If not sure which activation function to use, try to use <code>ReLU</code> or <code>tanh</code> first. And the choice of these activation function would bring impact to the training speed.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#why-need-nonlinear-activation-function","title":"Why need nonlinear activation function ?","text":"<p>Put it in short, if use a linear activation function and after factorization of the equation, the final equation of the whole network would be still like \\(y = w^{'}x + b^{'}\\)</p> <p>i.e. no matter how many hidden layers, it is similar to a single layer.</p> <p>So <code>linear activation function</code> is barely used except in output layer for some rare cases.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#gradient-descent-of-neural-network","title":"Gradient descent of Neural Network","text":"<p>In the course, there are lots of derivatives calculation.</p> <p>Here is the summary of the equations, for the example in the slides above, with 2 layer NN.</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#random-initialization","title":"Random Initialization","text":"<p>Avoid initialize weights to zero</p> <p>Otherwise all unit in the same layer will be the same no matter after any times of iteration and update. i.e. meaningless how many units you have</p> <p>Also units are not updated correctly or even not updated</p> <p>Seems, it is called <code>symmetry breaking problem</code> </p> <p>Note: <code>b</code> is okay to be zero </p> <p></p> <p>Correct way - random initialization</p> <p>Note: Below is a simple version. A more practical way, please refer to the content in course 2 here </p> <p>Weights should be initialized randomly AND to be some small  numbers ( Show as the <code>*0.01</code> in the below equation, note that it could be other values, 0.001, \u2026etc)</p> <p>Why ?</p> <ul> <li>if <code>w</code> are too large \u2192 <code>z</code> is too large \u2192 <code>slope of activation</code> ~= 0</li> <li>then the backward propagation update is very very slow</li> </ul> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#assignment","title":"Assignment","text":"<p>Some shapes of variable used in the functions</p> <pre><code>\"\"\"\nX shape: (2, 400) a.k.a (n_x, m)\nY: (1, 400)\n- n_x: the size of the input layer\n- n_h: the size of the hidden layer (**set this to 4, only for this Exercise 2**) \n- n_y: the size of the output layer\n\"\"\"\n(n_x, n_h, n_y) = layer_sizes(t_X, t_Y)\n# 2, 4, 1\nparameters = initialize_parameters(n_x, n_h, n_y)\n\"\"\"\nparameters = {\"W1\": W1, (n_h, n_x)\n              \"b1\": b1, (n_h, 1)\n              \"W2\": W2, (n_y, n_h)\n              \"b2\": b2} (n_y, 1)\n\"\"\"\nA2, cache = forward_propagation(t_X, parameters)\n\"\"\"\nA2 : (1, m)\nZ1:  (n_h, m)\nA1:  (n_h, m)\nZ2:  (1, m)\nA2:  (1, m)\n\"\"\"\ncost = compute_cost(A2, t_Y)\nX : (2, m)\nY : (1, m)\ngrads = backward_propagation(parameters, cache, t_X, t_Y)\n\"\"\"\ngrads = {\"dW1\": dW1, (n_h, 1)\n         \"db1\": db1, (1,)\n         \"dW2\": dW2, (1, n_h)\n         \"db2\": db2} (1,)\n\"\"\"\nparameters = update_parameters(parameters, grads)\ndef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False) -&gt; parameters\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#week-4-deep-neural-network","title":"Week 4 - Deep Neural Network","text":"<p>From previous weeks, we learnt the basic of how neuron / each node works in a shallow NN. And we learnt to calculate:</p> <ul> <li>In forward propagation, vectorization with Z = WX + B and the activation function A = G(Z)</li> <li>In back propagation, vectorization of several derivatives (dA, dZ, dW, dB) and how to update parameters</li> </ul> <p>In this short week 4 content, we just apply above knowledge to a deeper NNs with more layers. And we can observe a general formula</p> <ul> <li>\\(Z^{[l]} = W^{[l]} A^{[l-1]} + B^{[l]}\\), where \\(A^{[0]}\\) is equal to X</li> <li>\\(A^{[l]} = g^{[l]}(Z^{[l]})\\)</li> <li>And the formula of back propagation is shown in the screenshot</li> </ul> <p>Note: It is worth caching the <code>Z</code> &amp; <code>A</code> value during calculating each node, that will be reused in back propagation.</p> <p></p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#why-deeper-layers","title":"Why deeper layers?","text":"<p>Andrew used an example of XOR results of x1, x2, x3, \u2026.xn to illustrate. So it is said that deeper layers is better than more units with shallow layers</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#what-is-hyperparameters","title":"What is Hyperparameters?","text":"<p>They are another kind of parameters, that affect how real parameters (W, B) will be calculated.</p> <p>e.g. Learning rate, iterations, layer size, nodes size, activation function choice, \u2026etc</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#quiz-this-week","title":"Quiz this week:","text":"<ul> <li>Confusion point:<ul> <li> <p>We don\u2019t calculate dA^l from Z^l, why?</p> <ul> <li>$\\frac{dJ} {dA^{[l]}} =  \\frac{dJ} {dZ^{[l+1]}} . \\frac{dZ^{[l+1]}} {dA^{[l]}} =  \\frac{dJ} {dZ^{[l+1]}} .  \\frac{d} {dA^{[l]}} (W^{[l+1]}A^{[l]} + b^{[l+1]})</li> </ul> <p>\\ \\text{ } \\ = dZ^{[l+1]}  W^{[l+1]}$</p> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#assignment-part-1-of-2","title":"Assignment part 1 of 2 :","text":"<p>Here's an outline of the steps in this assignment:</p> <p></p> <ul> <li>Initialize the parameters for a two-layer network and for an L-layer neural network</li> <li>Implement the forward propagation module (shown in purple in the figure below)<ul> <li>Complete the LINEAR part of a layer's forward propagation step (resulting in Z^[l]).</li> <li>The ACTIVATION function is provided for you (relu/sigmoid)</li> <li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li> <li>Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li> </ul> </li> <li>Compute the loss</li> <li>Implement the backward propagation module (denoted in red in the figure below)<ul> <li>Complete the LINEAR part of a layer's backward propagation step</li> <li>The gradient of the ACTIVATE function is provided for you(relu_backward/sigmoid_backward)</li> <li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function</li> <li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li> </ul> </li> <li>Finally, update the parameters</li> </ul> <p>The Network architecture we will implement is:</p> <p></p> <p>Variable shape tracking:</p> <pre><code>parameters = initialize_parameters(3,2,1)\n\"\"\"\nW1 -- weight matrix of shape (n_h, n_x)\nb1 -- bias vector of shape (n_h, 1)\nW2 -- weight matrix of shape (n_y, n_h)\nb2 -- bias vector of shape (n_y, 1)\n\"\"\"\nparameters = initialize_parameters_deep([5,4,3])\n\"\"\" A dict containing all layers param : \n{\n    W1 -- weight matrix of shape (n_h, n_x)\n    b1 -- bias vector of shape (n_h, 1)\n    W2 -- weight matrix of shape (n_y, n_h)\n    b2 -- bias vector of shape (n_y, 1)\n    ...\n}\n\"\"\"\n# Pure Z = WX + b\nt_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n\"\"\"\nt_A : (n_unit_prev, m)\nt_W : (n_unit_now, n_unit_prev)\nt_b : (n_unit, 1)\nt_Z : (n_unit_now, m)\nt_linear_cache : (t_A, t_W, t_b)\n\"\"\"\n# activation_cache contains Z\nA, activation_cache = sigmoid(Z)\nA, activation_cache = relu(Z)\nA, cache = linear_activation_forward(t_A_prev, t_W, t_b, activation = \"sigmoid\") # or relu\n\"\"\"\nA = (n_unit_now, m)\ncache = (linear_cache, activation_cache)\n            = (\n                    (t_A_prev, t_W, t_b),\n                    Z\n                )\n\"\"\"\n# t_parameters from initialize_parameters_deep\nt_AL, t_caches = L_model_forward(t_X, t_parameters)\n\"\"\"\nAL : (1, m) # because last layer unit = 1\ncaches = [\n    (linear_cache, activation_cache) # layer 1 cache (relu layer)\n    ( # 2nd layer here\n        (A1, W2, b2),\n        Z2\n    )\n    ...\n    (linear_cache, activation_cache) # layer L cache (sigmoid output)\n]\n\"\"\"\nt_cost = compute_cost(t_AL, t_Y)\n# cost : float\n# dz: (cur_unit_size, m), linear_cache = (A_prev, W, b)\ndA_prev, dW, db = linear_backward(t_dZ, t_linear_cache)\nt_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\") # or relu\n\"\"\"\ndA_prev: (prev_unit_size, m)\ndW:  (current_unit_size, prev_unit_size)\ndb:  (current_unit_size ,1) \n\"\"\"\n# AL is last layer output, i.e. y_hat\ngrads = L_model_backward(AL, Y_gt, caches)\n\"\"\"\ngrads: {\n    dA0: (layer_0_unit_size, m), i.e. (X_feature_size, m)\n    dW1: (layer_1_unit_size, layer_0_unit_size)\n    db1: (layer_1_unit_size, 1)\n    ...\n}\n\"\"\"\nt_parameters = update_parameters(t_parameters, grads, 0.1)\n\"\"\" A dict containing all layers param : \n{\n    W1 -- weight matrix of shape (n_h, n_x)\n    b1 -- bias vector of shape (n_h, 1)\n    W2 -- weight matrix of shape (n_y, n_h)\n    b2 -- bias vector of shape (n_y, 1)\n    ...\n}\n\"\"\"\n</code></pre> <p>Memo:</p> <p>Implementation of cost function</p> <pre><code>cost = (\nnp.dot(Y, np.log(AL.T)) \n+ np.dot(1-Y, np.log(1-AL.T))\n) / -m\n</code></pre> <p></p> <p>Implementation of derivatives</p> <pre><code>dW = np.dot(dZ, A_prev.T) /m # (current_unit_size, prev_unit_size)\ndb = np.sum(dZ, axis=1, keepdims=True) /m # (current_unit_size ,1)\ndA_prev = np.dot(W.T, dZ) # (prev_unit_size, m)\n</code></pre> <p></p> <p>Implementation of cost derivative</p> <pre><code>dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n</code></pre> \\[ \\frac{dJ} {dA^{[L]}} = - ( \\frac{Y} {A^{[L]}} - \\frac{1-Y} {1-A^{[L]}} ) \\]"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Neural%20Networks%20and%20Deep%20Learning%201df34a2b2d564490beb92ea92ffe9812/#assignment-part-2-of-2","title":"Assignment part 2 of 2:","text":"<p>Shape memo:</p> <pre><code># train_x's shape: (12288, 209)\n# test_x's shape: (12288, 50)\n# train_y shape: (1, 209)\n# test_y shape: (1, 50)\n# Reshape and standardize procedure\n# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n# Standardize data to have feature values between 0 and 1.\ntrain_x = train_x_flatten/255.\ntest_x = test_x_flatten/255.\n# 1. 2-layer nn\n# LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.\n\"\"\"From prev assigment\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n\"\"\"\nparameters, costs = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2, print_cost=False)\n\"\"\"\nparameters : final param of W1, b1, W2, b2\ncosts: list of each calculated cost (float)\n\"\"\"\n</code></pre> <p>Code in the 2-layer func</p> <pre><code>parameters = initialize_parameters(n_x,n_h,n_y)\n# X: (n_x, m)\n# W1: (n_h, n_x)\n# b1: (n_h, 1)\nA1, cache1 = linear_activation_forward(X, W1, b1, activation = \"relu\") # or relu\n# A1: (n_h, m)\n# W2: (1, n_h)\n# b2: (1, 1)\nA2, cache2 = linear_activation_forward(A1, W2, b2, activation = \"sigmoid\") # or relu\n# A2: (1, m)\ncost = compute_cost(A2, Y)\ndA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\ndA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\") # or relu\ndA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\") # or relu\nparameters = update_parameters(parameters, grads, learning_rate)\n</code></pre>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Sequence%20Models%20ebfd9f6947f042b7a08adb5c89d456ed/","title":"Sequence Models","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Sequence%20Models%20ebfd9f6947f042b7a08adb5c89d456ed/#cite","title":"Cite","text":"<p>As in copyright:</p> <p>DeepLearning.AI\u00a0makes these slides available for educational purposes. You may not use or distribute these slides for commercial purposes. You may make copies of these slides and use or distribute them for educational purposes as long as you cite\u00a0DeepLearning.AI \u00a0as the source of the slides.</p> <ul> <li>Slides here are from DeepLearning.AI</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Sequence%20Models%20ebfd9f6947f042b7a08adb5c89d456ed/#week-1","title":"Week 1","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Sequence%20Models%20ebfd9f6947f042b7a08adb5c89d456ed/#assignment-1","title":"Assignment 1","text":"<p>RNN Architecture:</p> <ul> <li>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have \"memory.\u201d</li> <li> <p>The example of RNN with len(input) = len(output)</p> <p></p> <ul> <li>Each input \\(x^{(t)}\\)can be an one-hot vector, or just a value.  e.g. A language with a 5000-word vocabulary could be one-hot encoded into a vector that has 5000 units. So\u00a0\\(x^{(t)}\\)\u00a0shape = (5000,)</li> <li>The activation\u00a0a\u27e8t\u27e9\u00a0that is passed to the RNN from one time step to another is called a <code>hidden state</code></li> </ul> </li> </ul> <p>RNN Cell:</p> <ul> <li>Think of the recurrent neural network as the repeated use of a single cell</li> <li> <p>The following figure describes the operations for a single time step of an RNN cell:</p> <p></p> </li> </ul> <p>LSTM Cell:</p> <ul> <li>An LSTM is similar to an RNN in that they both use hidden states to pass along information, but an LSTM also uses a cell state, which is like a long-term memory, to help deal with the issue of vanishing gradients</li> <li>An LSTM cell consists of a cell state, or long-term memory, a hidden state, or short-term memory</li> <li> <p>Here is the cell detail:</p> <p></p> </li> <li> <p>The simple explanation of the flow through:</p> <ul> <li>Similar to simple RNN, <code>a(t-1)</code> and <code>x(t)</code> are the inputs (near the bottom left) They are used to calculate:<ul> <li>Forget Gate - A \u201cmask\u201d vector, used for another input <code>c(t-1)</code> calculation, between 0~1</li> <li>Update Gate - A \u201cmask\u201d vector, that decides if the Candidate values <code>\ud835\udc1c\u0303\u00a0\u27e8\ud835\udc61\u27e9</code> can pass into the hidden state <code>c(t)</code> , between 0~1</li> <li>Candidate value - the value calculated from previous activation and current input, between -1~1</li> <li>Output Gate - A \u201cmask\u201d vector, decides what values are passed into Output y, and next activation</li> </ul> </li> <li><code>c(t-1)</code> is another new input here (left side)<ul> <li>With the Forget Gate from above, values are decided to whether pass in or not</li> <li>Then add with the final candidate value, it become the next <code>c(t)</code></li> </ul> </li> </ul> </li> <li> <p>About the <code>Forget Gate</code>:</p> <p></p> </li> <li> <p>About the <code>Candidate Value</code></p> <p></p> </li> <li> <p><code>Update gate</code> :</p> <p></p> </li> <li> <p><code>Output gate</code>:</p> <p></p> </li> <li> <p>At last, final <code>cell state</code></p> <p></p> </li> <li> <p><code>Hidden state</code> for next cell</p> <p></p> </li> <li> <p>Prediction output</p> <p></p> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Sequence%20Models%20ebfd9f6947f042b7a08adb5c89d456ed/#assignment-2","title":"Assignment 2:","text":"<pre><code>x (27, 1)\nWax (100, 27)\nWaa (100, 100)\nb (100, 1)\na (100, 1)\nWya (27, 100)\nby (27, 1)\ny (27, 1)\nchar_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }\nclip(gradients, maxValue)\ndef sample(parameters, char_to_ix, seed):\noptimize(X, Y, a_prev, parameters, learning_rate = 0.01):\ndef rnn_forward(X, Y, a_prev, parameters):\n\"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n    It returns the loss' value as well as a \"cache\" storing values to be used in backpropagation.\"\"\"\n....\nreturn loss, cache\ndef rnn_backward(X, Y, parameters, cache):\n\"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n    to the parameters. It returns also all the hidden states.\"\"\"\n...\nreturn gradients, a\ndef update_parameters(parameters, gradients, learning_rate):\n\"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n...\nreturn parameters\nnp.random.seed(0)\nprobs = np.array([0.1, 0.0, 0.7, 0.2])\nidx = np.random.choice(range(len(probs)), p = probs)\n</code></pre> <p>Generating text:</p> <ul> <li> <p>After the language model is trained, we can use it to generate pieces of text</p> <p></p> <ul> <li>The <code>sample</code> process here is to : sampling a random text in the chosen texts, to prevent generating the same text every time.<ul> <li>How to: With the output of softmax, we have a vector of probability. Then we choose the words by taking its probability.  E.g. word at i-th index = 16%, it might be picked with a chance of 16%</li> </ul> </li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Structuring%20Machine%20Learning%20Projects%200baa3b0bbf32456e8439b1a11741e1eb/","title":"Structuring Machine Learning Projects","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Structuring%20Machine%20Learning%20Projects%200baa3b0bbf32456e8439b1a11741e1eb/#goal","title":"Goal","text":"<ul> <li>Learn how to structure your machine learning project</li> <li>New best practices for splitting train, test, eval datasets</li> <li>Deal with the issue if your training set and your test come from different distributions</li> <li>End-to-end deep learning</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Google%20ML%20Bootcamp%20d92e64efed9d4755af566dba69a61f69/Structuring%20Machine%20Learning%20Projects%200baa3b0bbf32456e8439b1a11741e1eb/#important-deadline","title":"Important Deadline","text":"Week Deadline 1 2 Oct 2 9 Oct"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Kaggle%2031e9b9c6782241c8a4ace240701e7357/Spaceship-titanic%20320480396edc469b8f39d6af1caa6369/","title":"Spaceship-titanic","text":"<p>It is a getting start competition.</p> <p>https://www.kaggle.com/competitions/spaceship-titanic/data</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Kaggle%2031e9b9c6782241c8a4ace240701e7357/Spaceship-titanic%20320480396edc469b8f39d6af1caa6369/#ref","title":"Ref :","text":"<p>https://www.kaggle.com/code/arootda/pycaret-visualization-optimization-0-81</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Kaggle%2031e9b9c6782241c8a4ace240701e7357/Spaceship-titanic%20320480396edc469b8f39d6af1caa6369/#target","title":"Target :","text":"<p>Predict whether a passenger was transported to an alternate dimension during the\u00a0Spaceship Titanic's collision with the spacetime anomaly.</p> <p>i.e. binary classification</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/Kaggle%2031e9b9c6782241c8a4ace240701e7357/Spaceship-titanic%20320480396edc469b8f39d6af1caa6369/#data","title":"Data :","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/","title":"General Cycle","text":"<p>Data Acquisition</p> <p>Data Cleansing</p> <p>Pre-processing</p> <p>Feature Engineering</p> <p>Modeling(Training)</p> <p>Evaluation</p> <p>Deployment</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/","title":"Language Structure","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/#phonemes","title":"Phonemes","text":"<ul> <li>Smallest units of sound</li> <li>Each of them normally no meaning</li> <li>[EN] can be a letter, or combination of letters<ul> <li>i.e. consonant vs vowel</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/#morphemes","title":"Morphemes","text":"<ul> <li>Smallest unit that has meaning</li> <li>Formed by Phonemes</li> <li>** Could be not a word, e.g. prefix, suffix</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/#lexemes","title":"Lexemes","text":"<ul> <li>The structural variations of morphemes</li> <li>e.g. <code>Run</code> vs <code>Running</code> = belong to the same lexeme form</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/#syntax","title":"Syntax","text":"<ul> <li>A set of rules to construct sentence (with correct grammar)</li> <li>Can represent in Parse Tree</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/Language%20Structure%20c7e651ebae714ac59086f64fc3941764/#context","title":"Context","text":"<ul> <li>Form a particular meaning by various parts.</li> <li>Includes long-term reference(<code>He</code> \u2192 a guy mentioned in last paragraph) World Knowledge, Common Sense</li> <li>Generally, context composes of <code>semantics</code> &amp; <code>pragmatics</code><ul> <li>Semantics = direct meaning of the words, sentences</li> <li>Pragmatics = external knowledge (e.g. world knowledge, common sense...)</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/NLP%20approach%20c633c348bffc4ede99355961353a2c49/","title":"NLP approach","text":"<p>Always start a new project with simple method instead of diving in a deep network straightaway </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/NLP%20approach%20c633c348bffc4ede99355961353a2c49/#heuristics-based-methods","title":"Heuristics-based Methods","text":"<ul> <li>i.e. Rule-based approach</li> <li>e.g. Count how many positive &amp; negative words to judge a sentiment of the text.</li> <li>using <code>Wordnet</code> - db of words, and semantic relationships between them</li> <li>using regex (i.e. <code>StandfordCoreNLP</code> , <code>TokensRegex</code>, <code>Pregex</code>)</li> <li>Context-Free-Grammar (CFG) - a formal grammar to model natural lang<ul> <li>Earley Parser</li> <li>JAPE (Java Annotation Patterns Engine)</li> </ul> </li> <li>Rule based system -  General Architecture for Text Engineering (GATE)</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/NLP%20approach%20c633c348bffc4ede99355961353a2c49/#machine-learning-methods","title":"Machine Learning Methods","text":"<ul> <li>Classification<ul> <li><code>Naive Bayes</code> - classic algorithm for classification tasks</li> <li><code>Support Vector Machine(SVM)</code> - learn a decision boundary to classify classes</li> </ul> </li> <li>Regression</li> <li><code>Hidden Markov Model</code> - a statistical mdoel<ul> <li>used for POS tagging</li> </ul> </li> <li><code>Conditional Random Field(CRF)</code><ul> <li>POS tagging</li> </ul> </li> </ul> <p>Deep Learning Methods</p> <p>to be continue....</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/NLP%20core%20task%209d09715fcb74434a89c6413f0390167e/","title":"NLP core task","text":"<ul> <li>Language Modeling (Words prediction)</li> <li>Topic Modeling (Categorize topics)</li> <li>Text Summarization</li> <li>Text classification</li> <li>Information Extraction</li> <li>Information Retrieval (find a specific doc responding to an user query)</li> <li>Conversational Agent</li> <li>Question Answering</li> <li>Machine Translation</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Acquisition%2085042b98694f4196a94a113f193d0e6b/","title":"Data Acquisition","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Acquisition%2085042b98694f4196a94a113f193d0e6b/#public-dataset","title":"Public dataset","text":"<ul> <li>compilation by Nicolas Iderhoff</li> <li>Google\u2019s datasets search engine</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Acquisition%2085042b98694f4196a94a113f193d0e6b/#scrape-data-from-online-source","title":"Scrape data from online source","text":"<ul> <li>e.g. wiki, twitter</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Acquisition%2085042b98694f4196a94a113f193d0e6b/#product-intervention","title":"Product Intervention","text":"<ul> <li>i.e. collect data from the product/service itself</li> <li>cons: it takes long time</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Acquisition%2085042b98694f4196a94a113f193d0e6b/#data-augmentation","title":"Data Augmentation","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Data%20Cleansing%20b604bcb04d2e4b7b9bcd0e570c9d21e0/","title":"Data Cleansing","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Deployment%20079a9677912a4738a52ea05bbd6cbf79/","title":"Deployment","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Evaluation%2058c8616c037b45bab44fe9bbf9a733ff/","title":"Evaluation","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Evaluation%2058c8616c037b45bab44fe9bbf9a733ff/#evaluation-2-types","title":"Evaluation - 2 types","text":"<ul> <li>intrinsic<ul> <li>Goal: can do it easily before doing extrinsic</li> <li>e.g. measure model performance, by f1, recall, ...etc</li> </ul> </li> <li>extrinsic<ul> <li>Goal: measure the final object performance</li> <li>e.g. How much can the system speed up the user xxxx action.</li> <li>e.g. Measure how much time an user waste if the spam email went into normal inbox</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Evaluation%2058c8616c037b45bab44fe9bbf9a733ff/#intrinsic-evaluation-type","title":"Intrinsic Evaluation type","text":"Metrics Description Applications Accuracy correct pred / ALL pred classification Precision For a specific class (e.g. blue):"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Evaluation%2058c8616c037b45bab44fe9bbf9a733ff/#cheetsheet","title":"Cheetsheet","text":"<p>Evaluation simple cheatsheet</p> <p></p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Feature%20Engineering%20b8988a456abe48d1ae687f193ac6f770/","title":"Feature Engineering","text":"<p>It is also called feature extraction </p> <p>Purpose: Turn preprocessed text into \u2192 numeric vector (i.e. text representation?) that can be fed into ML </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Feature%20Engineering%20b8988a456abe48d1ae687f193ac6f770/#feature-engineering-in-ml-vs-in-dl","title":"Feature engineering in ML VS in DL","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Feature%20Engineering%20b8988a456abe48d1ae687f193ac6f770/#in-ml","title":"In ML:","text":"<p>Pros:</p> <p>We handcraft the function to do this.</p> <p>i.e. it remains interpretable, we can explain the feature correlation</p> <p>Cons:</p> <p>Handcrafted function is a bottleneck sometimes. It affects the performance</p> <p>Also, the wrong choice of feature could lead to big harm on the model.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Feature%20Engineering%20b8988a456abe48d1ae687f193ac6f770/#in-dl","title":"In DL:","text":"<p>Pros:</p> <p>Pre-processed raw data is directly fed into DL model.</p> <p>So the model performance is higher</p> <p>DL model \u201clearn\u201d the features from data.</p> <p>Cons:</p> <p>HOWEVER, it learns all features from data, it is hard to tell the correlation then.</p> <p>i.e. it loses interpretability.</p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/","title":"Modeling(Training)","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/#handcraft-approach","title":"Handcraft Approach:","text":"<p>At the start, data size is small, we should start with simple methods or rule based system.</p> <ul> <li>start with heuristics method I.e. by trial &amp; error OR loosely defined rules<ul> <li>E.g. An explicit blacklist in email spam task.</li> <li>E.g.2 In E-commerce, sorting &amp; recommend by the top number of purchased items(or its category)</li> </ul> </li> <li>Another example is regular expression<ul> <li>lib: <code>Stanford NLP\u2019s TokensRegex</code> , <code>spaCy\u2019s rule based matches</code></li> </ul> </li> </ul> <p>When data grows, switch to/combine with ML or DL model.</p> <p>There are 2 popular ways:</p> <ul> <li>Create features from heuristics, to train/inference model</li> <li>Use heuristics in particular case and bypass model  (I.e. the heuristics decide it during pre-processing)</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/#using-nlp-service-providers-as-approach","title":"Using  NLP service providers as approach:","text":"<ul> <li>Google Cloud Natural Language</li> <li>Amazon Comprehend</li> <li>Microsoft Azure Cognitive Services</li> <li>IBM Watson Natural Language Understanding</li> </ul> <p>These can also be used as a reference to see if your model performs well enough. If not, then you can use the NLP service directly. </p>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/#model-ensembling-stacking","title":"Model ensembling &amp; stacking","text":"<ul> <li>Common way is to NOT do everything by 1 model. Instead, use multiple model to do each specific task.</li> <li>Ensembling: model in parallel</li> <li>Stacking: model in series</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/#apply-heuristic-after-model-inference","title":"Apply Heuristic after model inference","text":"<ul> <li>it is a good practice to apply rule based / heuristic test after model  final output, as a safe test to ensure the model does not make huge mistake</li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Modeling%28Training%29%20ddbef95f0e5340c8bfd2071e5a84a090/#summary-of-how-to-choose-approach-based-on-data-size","title":"Summary of How to choose approach based on Data size","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Pre-processing%2098feed82a48648cd874d5904df535ab8/","title":"Pre-processing","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Pre-processing%2098feed82a48648cd874d5904df535ab8/#table-of-content","title":"Table of Content","text":""},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Pre-processing%2098feed82a48648cd874d5904df535ab8/#preprocessing","title":"Preprocessing","text":"<p>Note: NOT all of them will be executed </p> <p>Note2: The steps are NOT performed in order. e.g. Not removing punctuation before lemmatization. </p> <p>Useful Lib</p> <ul> <li>All-in-one<ul> <li>NLTK</li> <li>spaCy</li> </ul> </li> <li>Lang detection<ul> <li>Polyglot</li> </ul> </li> <li>POS tagger<ul> <li>Parsey McParseface Tagger</li> </ul> </li> </ul> <p>Preliminaries</p> <ul> <li>Word tokenization</li> <li>Sentence tokenization</li> </ul> <p>Frequent Steps</p> <ul> <li>Remove stop words (not meaningful words, such as <code>an, a, the</code>)</li> <li>Remove punctuation/digits, lowercasing</li> <li> <p>Stemming &amp; lemmatization</p> <p></p> </li> </ul> <p>Others steps</p> <ul> <li>Text Normalization<ul> <li>e.g. phone num in various format \u2192 one canonical representation</li> </ul> </li> <li>Language detection</li> <li>Code Mixing &amp; transliteration<ul> <li>Code mixing means \u2192 one sentence/text piece contains multiple languages</li> </ul> </li> </ul> <p>Advanced steps</p> <ul> <li>POS tagging (NLTK, spaCy)<ul> <li>e.g. The final goal is to extract organization name. POS tagging can be used during pre-processing first, to tag the noun.</li> </ul> </li> </ul>"},{"location":"Lesson%201223ec9b800c44cfb51314ad3aa7de4c/NLP%20Book%20fea6a94656744b71bbd9bbfa8c864740/General%20Cycle%2012d3fecbc8e94eb5a82a746b7453cfb9/Pre-processing%2098feed82a48648cd874d5904df535ab8/#summary","title":"Summary","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/","title":"AWS","text":"<ul> <li> <p>Install jupyter on AWS EC2</p> <p>Run Project Jupyter Notebooks On Amazon EC2</p> <p>Running Jupyter Notebook on an EC2 Server</p> </li> </ul> <p>Step Function</p> <p>IAM</p> <p>Lambda</p> <p>EC2</p> <p>Multi Tenant SaaS</p> <p>ECS</p> <p>SageMaker</p> <p>S3</p> <p>Network</p> <p>CloudWatch</p> <p>AWS CLI</p> <p>boto3</p> <p>Service Quotas</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Certificate%20e104e24d2c6c4a0a8c6d72b15658ab34/","title":"Certificate","text":"<p>TensorFlow Developer Certificate - https://www.tensorflow.org/certificate</p> <p>GCP Professional Data Engineer - https://cloud.google.com/certification/data-engineer</p> <p>GCP Professional Machine Learning Engineer - https://cloud.google.com/certification/machine-learning-engineer</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/","title":"Docker","text":"<p>Removal</p> <p>Access</p> <p>Container control</p> <p>Docker-Compose</p> <p>Dockerfile template</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/","title":"Git","text":"<p>Submodule</p> <p>Github Pages</p> <p>Useful github</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/","title":"Heroku Hosting","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#init","title":"Init","text":"<p>Install CLI: https://devcenter.heroku.com/articles/heroku-cli </p> <p>After installed,</p> <pre><code>heroku login\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#create-a-hosting","title":"Create a hosting","text":"<pre><code>heroku create &lt;app name&gt;\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#deploy","title":"Deploy","text":"<pre><code>git push heroku master\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#quickly-open-the-site","title":"Quickly open the site","text":"<pre><code>heroku open\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#add-config-to-the-site","title":"Add config to the site","text":"<pre><code>heroku config:set WEBHOOK_URL=adwadwdawdaw\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#run-command-remotely","title":"Run command remotely","text":"<pre><code>heroku run \"npm run notify\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Heroku%20Hosting%2009f948d6e101446c91716065bd4720cd/#heroku-add-on","title":"Heroku Add-on","text":"<ul> <li>It can be used as a scheduled task - https://devcenter.heroku.com/articles/scheduler</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Linux%20Unix%20b716d9e06ab94175b241c48d584c5d86/","title":"Linux/Unix","text":"<p>Useful CMD</p> <p>Font issue on linux/unix</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/","title":"ML","text":"<p>Dev Resource</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/#general","title":"General :","text":"<p>Practical Skills</p> <p>Theory</p> <p>Glossary</p> <p>List of Algorithm</p> <p>List of Learning Approaches</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/#domain-specific","title":"Domain specific :","text":"<p>Computer Vision</p> <p>NLP</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/#framework-libraries-notes","title":"Framework / Libraries Notes :","text":"<p>scikit-learn</p> <p>Pytorch</p> <p>Tensorflow / Keras</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/#not-sort-yet","title":"Not sort yet :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Mac%2031a0a500dd1745fdac716d1844b0ecee/","title":"Mac","text":"<p>Some tricks / hard-to-solve issues</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Mac%2031a0a500dd1745fdac716d1844b0ecee/#make-custom-script-to-be-an-app","title":"Make custom script to be an APP","text":"<ul> <li>Open script editor</li> </ul> <pre><code>do shell script \"/usr/local/bin/labelImg\"\n// OR for showing a terminal:\ntell application \"Terminal\"\ndo script with command \"labelImg\"\nend tell\n</code></pre> <ul> <li>Save as <code>application</code></li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/","title":"Network","text":"<p>Generic network notes. For cloud platform-specific, please put in AWS / other collections. </p> <p>ssh/config Setup for jump server</p> <p>Upload files with scp</p> <p>Setup VSCode remote development</p> <p>Stress test</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/","title":"Node / JS","text":"<p>Resources</p> <p>Simple http-server</p> <p>Markdown to Doc</p> <p>ES Module VS CommonJS</p> <p>Datetime</p> <p>File related</p> <p>Vue</p> <p>NPM</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/","title":"Python","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#toc","title":"ToC","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#ref","title":"Ref :","text":"<p>Resources</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#programming-practice","title":"Programming Practice :","text":"<p>Env - anaconda/pip/poetry</p> <p>Vscode Setup</p> <p>Import files - Best practice</p> <p>Logger setup</p> <p>Unit test</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#usage-reference","title":"Usage Reference :","text":"<p>Built-in small module</p> <p>Path / files related</p> <p>Date Time</p> <p>Numpy</p> <p>Pandas</p> <p>Image lib</p> <p>Singleton example</p> <p>Text similarity/comparison</p> <p>Data Structure Skill</p> <p>String Manipulation</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#software","title":"Software :","text":"<p>Parse Arguments</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#web-network","title":"Web &amp; Network :","text":"<p>Web related</p> <p>ORM related</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#ml-jupyter","title":"ML &amp; Jupyter :","text":"<p>Visualization - plotting</p> <p>Scikit-learn</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/#others","title":"Others:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/AWS%20CLI%206e50f755d5dc4c6f9dedab1a44fb2c81/","title":"AWS CLI","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/AWS%20CLI%206e50f755d5dc4c6f9dedab1a44fb2c81/#get-access-id-access-key-after-logged-in-via-sso-auth","title":"Get access id, access key after logged in via SSO auth","text":"<p>So it can be used for boto3</p> <pre><code>aws sso get-role-credentials --profile test-xxx --role-name AdministratorAccess --account-id &lt;long number&gt; --access-token &lt;token&gt; --region ap-northeast-1\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/AWS%20CLI%206e50f755d5dc4c6f9dedab1a44fb2c81/#get-region","title":"Get region","text":"<pre><code>aws configure get region\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/AWS%20CLI%206e50f755d5dc4c6f9dedab1a44fb2c81/#s3","title":"S3","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/AWS%20CLI%206e50f755d5dc4c6f9dedab1a44fb2c81/#list-in-a-bucket","title":"List in a bucket","text":"<pre><code>aws s3 ls s3://bucketxxxx\n</code></pre> <p>Copy all files in a folder</p> <pre><code>aws s3 cp s3://xxxxx/groundtruth_texts . --recursive --profile prod-pfm-aws\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/","title":"CloudWatch","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#ref","title":"Ref:","text":"<ul> <li>App ELB metrics list - https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-cloudwatch-metrics.html</li> <li>Network ELB metrics list - https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-cloudwatch-metrics.html</li> <li>RDS metrics list - https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-metrics.html</li> <li>Step Function metrics list - https://docs.aws.amazon.com/step-functions/latest/dg/procedure-cw-metrics.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#log-filtering","title":"Log filtering","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#in-log-group-filtering","title":"In log group, filtering:","text":"<p>Excluding</p> <pre><code># -&lt;pattern&gt;\n-\"GET / HTTP/1.1\" 200\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#in-log-insight-query","title":"In log insight, query:","text":"<p>Contains string</p> <pre><code>fields @timestamp, @message\n| filter @message like /something here/\n| sort @timestamp desc\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#alarm","title":"Alarm","text":"<p>Guide to create an alarm for HTTP code 5xx</p> <ol> <li>CloudWatch \u2192 Alarm</li> <li>Choose from a metric under a namespace, e.g. <code>HTTPCode_Target_5XX_Count</code> under <code>AWS/ApplicationELB</code> for a specific ALB</li> <li>Select <code>Statistic</code> to <code>Sum</code></li> <li><code>Period</code> set to 1 minute</li> <li>Conditions \u2192 <code>Greater</code> than <code>0</code></li> <li>Notification \u2192 alarm trigger if the state is <code>In alarm</code> (i.e. meet the condition above)</li> <li>Select / Create a SNS topic</li> </ol>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/CloudWatch%207528812bc9c14efcb7b25d17bc791f46/#send-alarm-to-slack-without-lambda","title":"Send Alarm to Slack (Without lambda)","text":"<p>Assume you already made an Alarm </p> <p>We will use AWS ChatBot to do integration with Slack easily</p> <ol> <li> <p>Go to IAM \u2192 Policy \u2192 Create new </p> <ol> <li>It is used as *Channel guardrail policies*</li> <li>Normally we allow the minimum action, put these inside Statements:</li> </ol> <pre><code>{\n\"Action\": [\n\"cloudwatch:Describe*\",\n\"cloudwatch:Get*\",\n\"cloudwatch:List*\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": \"*\"\n}\n</code></pre> </li> <li> <p>AWS ChatBot \u2192 create new \u2192 slack</p> <ol> <li>You will be prompted to allow AWS to access Slack</li> </ol> </li> <li> <p>After creation \u2192 <code>configure Slack Channel</code></p> <ol> <li>You can get the <code>Channel ID</code> from slack channel</li> <li> <p><code>Permission</code> \u2192 <code>Channel IAM role</code></p> <ol> <li>create a new one, with (default) <code>Notification permission</code></li> <li>the permission will be like below</li> </ol> <pre><code>{\n\"Action\": [\n\"cloudwatch:Describe*\",\n\"cloudwatch:Get*\",\n\"cloudwatch:List*\"\n],\n\"Effect\": \"Allow\",\n\"Resource\": \"*\"\n}\n</code></pre> </li> <li> <p><code>Channel guardrail policies</code> \u2192 choose the policy we created above</p> </li> <li><code>Notification</code> \u2192 choose the region and Topic (that is created while creating Alarm)</li> </ol> </li> </ol>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/","title":"EC2","text":"<p>Install jupyter on AWS</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/","title":"ECS","text":"<p>Deployment Type</p> <p>Load balancer</p> <p>Components</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/IAM%20ee5ae89ddcfe4aaa8c4472096717cf15/","title":"IAM","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/IAM%20ee5ae89ddcfe4aaa8c4472096717cf15/#3-important-setting-in-roles","title":"3 Important setting in Roles","text":"<ul> <li>Permission policies</li> <li>Permission boundaries</li> <li>Trust relationships</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/IAM%20ee5ae89ddcfe4aaa8c4472096717cf15/#passrole-assumerole","title":"PassRole &amp; AssumeRole","text":"<ul> <li>PassRole \u2192 For a role A, what other <code>roles</code> it(RoleA) can assign to other <code>resources</code></li> <li>AssumeRole \u2192 For a role A, what <code>resources</code> can \u201cpretend\u201d this roleA</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/IAM%20ee5ae89ddcfe4aaa8c4472096717cf15/#passrole","title":"PassRole","text":"<ul> <li>https://blog.rowanudell.com/iam-passrole-explained/</li> <li> <p>The importance of PassRole:</p> <p></p> </li> <li> <p>It is NOT an action, although listed in Action Block</p> </li> <li> <p>It is to specify, for a <code>user/role</code> , what kind of <code>roles</code> they can assign to other resources.</p> <p>e.g.  You can limit a user or service, only can assign the specific role ARN(as below) to other <code>resources</code> </p> <pre><code>{\n\"Effect\": \"Allow\",\n\"Action\": \"iam:PassRole\",\n\"Resource\": \"arn:aws:::123456789012:role/LimitedAccess\"\n}\n</code></pre> <p></p> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/IAM%20ee5ae89ddcfe4aaa8c4472096717cf15/#assumerole","title":"AssumeRole","text":"<ul> <li>It defines which resources / user can assume this role</li> <li> <p>Here is a sample trusted relationships</p> <p>e.g. This role can only be assumed by EC2 </p> <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"ec2.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n</code></pre> </li> <li> <p>When you switch to use a role temporarily, you get the Power as the same as <code>target role</code> through STS</p> </li> <li>So that the user ARN will become sts,  <code>arn:aws:sts::xxxxxxxx</code><ul> <li>original user should be iam, <code>arn:aws:iam::xxxxxx</code></li> </ul> </li> </ul> <p></p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Lambda%200d34baaf0d3147ceb9d93d7fd099b41e/","title":"Lambda","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Lambda%200d34baaf0d3147ceb9d93d7fd099b41e/#deploy-as-container-image","title":"Deploy as Container image","text":"<p>https://docs.aws.amazon.com/lambda/latest/dg/python-image.html</p> <ul> <li>Required lib: aws-lambda-python-runtime-interface-client (RIC)</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Lambda%200d34baaf0d3147ceb9d93d7fd099b41e/#local-testing","title":"Local testing","text":"<p>https://docs.aws.amazon.com/lambda/latest/dg/images-test.html</p> <ul> <li>Required lib: Lambda Runtime Interface Emulator (RIE)</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Multi%20Tenant%20SaaS%20b26cdf3a5f0a45c9978bad7d5d9c8e96/","title":"Multi Tenant SaaS","text":"<p>Resource</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/","title":"Network","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/#example-of-a-system-setup","title":"Example of a system setup","text":"<ul> <li>Assume:<ul> <li>an API server,</li> <li>an inference instance</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/#route53","title":"Route53","text":"<ul> <li>3 zones<ul> <li>prod.xxxxx.xxx.xxx Public</li> <li>api.prod.xxx.xxx.xxx Private</li> <li>inf.xxx.xxxx.xxxx.xx Private</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/#load-balancer","title":"Load Balancer","text":"<ul> <li>3 LB:<ul> <li>api-alb<ul> <li>application</li> </ul> </li> <li>inference-alb<ul> <li>application</li> </ul> </li> <li>prod-nlb<ul> <li>network</li> </ul> </li> </ul> </li> <li>\u2192 forward to  Target group</li> <li>Target Group with Target type = <code>IP / instance / other load balancer</code></li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/#vpc","title":"VPC","text":"<p>All components are in the same VPC</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Network%20f4064c6e056b4d95b1c4e90d532a95da/#network-routing-example","title":"Network routing example","text":"<ul> <li> <p>From ECS any instance, access API server via internal URL:</p> <pre><code>https://prod.xxx.xxx.xxx.xx\n</code></pre> </li> <li> <p>\u2192 Route53, it points to:</p> <pre><code>dualstack.internal-api-alb-xxxxxx.elb.amazonaws.com.\n</code></pre> </li> <li> <p>\u2192 Load balancer(network): <code>api-alb</code></p> </li> <li>\u2192 listener <code>:443</code> \u2192 Target Group: <code>api-tg</code></li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/S3%20b28ea34c02004fbcaa90bc0af362baa3/","title":"S3","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/S3%20b28ea34c02004fbcaa90bc0af362baa3/#s3_1","title":"S3","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/","title":"SageMaker","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#table-of-contents","title":"\u2014Table of Contents\u2014","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#sagemaker-processing","title":"SageMaker Processing","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#local-mode","title":"Local Mode","text":"<p>If <code>arguments</code> is provided, <code>entrypoint</code> is required, although aws mentioned it is not. </p> <pre><code>processor = Processor(\n...\nentrypoint=[\"python3\", \"evaluation.py\"]\n)\nprocessor.run(\narguments=[\n\"--onnx_filename\", \"xxxx.onnx\",\n],\n...\n)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#processingoutput","title":"ProcessingOutput","text":"<p>For source in processing output, you cannot specify a file, but only directory</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#sagemaker-experiment","title":"SageMaker Experiment","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#download-experiment-result","title":"download experiment result","text":"<pre><code>import pandas as pd\nfrom sagemaker.analytics import ExperimentAnalytics\ndef save_experiment_df(experiment_name: str) -&gt; pd.DataFrame:\ntrial_component_analytics = ExperimentAnalytics(\nexperiment_name=experiment_name,\ninput_artifact_names=[]\n)\ndf = trial_component_analytics.dataframe()\ndf.to_csv(f\"experiment-{experiment_name}.csv\")\nreturn\nif __name__ == \"__main__\":\nsave_experiment_df(\"tune-features-20220324\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/SageMaker%205099f722c04a4bbd97cf0c7e681ad8b2/#sagemaker-neo","title":"SageMaker Neo","text":"<p>\u201cAmazon SageMaker Neo enables developers to optimize machine learning (ML) models for inference on SageMaker in the cloud and supported devices at the edge\u201d \u2026 \u201cAmazon SageMaker Neo automatically optimizes machine learning models to perform up to 25x faster with no loss in accuracy. SageMaker Neo uses the tool chain best suited for your model and target hardware platform while providing a simple standard API for model compilation\u201d </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Service%20Quotas%2016c49ef6494144289864411a92686147/","title":"Service Quotas","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Service%20Quotas%2016c49ef6494144289864411a92686147/#background","title":"Background:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Service%20Quotas%2016c49ef6494144289864411a92686147/#how-to","title":"How to:","text":"<ul> <li>To view current service quotas in your account, go to here<ul> <li>https://ap-northeast-1.console.aws.amazon.com/servicequotas/home/</li> </ul> </li> <li>To view default service quota, refer to individual service page,<ul> <li>e.g. for SageMaker, it is here: https://docs.aws.amazon.com/general/latest/gr/sagemaker.html#limits_sagemaker</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/","title":"Step Function","text":"<p>Context Object (i.e. Variable)</p> <p>step-functions-data-science-sdk-python</p> <p>Call other service\u2019s API</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/boto3%207462af1e708c46b7b0a3f023c741ed0d/","title":"boto3","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/boto3%207462af1e708c46b7b0a3f023c741ed0d/#sso-connection","title":"SSO connection","text":"<p>Use boto3 with configured sso profile</p> <pre><code>import boto3\npath = \"preprocess/output/xxxxxxxx/data.pkl\"\ndes = \"./test_s3/data.pkl\"\nsession = boto3.Session(profile_name='test-xxxx')\ns3 = session.resource(\"s3\")\nfor bucket in s3.buckets.all():\nprint(bucket.name)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/boto3%207462af1e708c46b7b0a3f023c741ed0d/#cloudwatch","title":"CloudWatch","text":"<ul> <li> <p>List alarm</p> <pre><code>import boto3\n# Create CloudWatch client\nsession = boto3.Session(profile_name='test-xxxx')\ncloudwatch = session.client(\"cloudwatch\")\n# List alarms of insufficient data through the pagination interface\npaginator = cloudwatch.get_paginator('describe_alarms')\nall_res = []\nfor response in paginator.paginate(StateValue='OK'):\n# print(response['MetricAlarms'])\nall_res.append(response['MetricAlarms'])\nprint(len(all_res))\n</code></pre> </li> <li> <p>Get alarms detail</p> <pre><code>import boto3\n# Create CloudWatch client\nsession = boto3.Session(profile_name='test-xxxx')\ncloudwatch = session.client(\"cloudwatch\")\nresponse = cloudwatch.describe_alarms(\nAlarmNames=[\n'Keith - Test HTTP 5xx API ALB',\n],\n# AlarmNamePrefix='string',\n# AlarmTypes=[\n#     'CompositeAlarm'|'MetricAlarm',\n# ],\n# ChildrenOfAlarmName='string',\n# ParentsOfAlarmName='string',\n# StateValue='OK'|'ALARM'|'INSUFFICIENT_DATA',\n# ActionPrefix='string',\n# MaxRecords=123,\n# NextToken='string'\n)\npp(response)\n</code></pre> </li> <li> <p>Create Alarm</p> <pre><code>import boto3\n# Create CloudWatch client\nsession = boto3.Session(profile_name='test-xxxxx')\ncloudwatch = session.client(\"cloudwatch\")\n# Create alarm\ncloudwatch.put_metric_alarm(\nAlarmName='Keith - Test HTTP 5xx inference ALB',\nAlarmActions= ['arn:aws:sns:xxxxx'],\nComparisonOperator='GreaterThanThreshold',\nMetricName='HTTPCode_Target_5XX_Count',\nNamespace='AWS/ApplicationELB',\nPeriod=60,\nStatistic='Sum',\nThreshold=0.0,\nDatapointsToAlarm=1,\nActionsEnabled=True,\nAlarmDescription='Just test. Fire alarm if 5xx http code.',\nDimensions=[{\n'Name': 'LoadBalancer',\n'Value': 'app/xxxx-alb/xxxxxxxx'\n}],\nEvaluationPeriods=1,\nTreatMissingData='notBreaching'\n)\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/boto3%207462af1e708c46b7b0a3f023c741ed0d/#s3","title":"S3","text":"<ul> <li>How to track the progress when download file<ul> <li>https://brodan.biz/blog/logging-s3-download-progress-with-python-and-boto3/</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/","title":"Install jupyter on AWS","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#intro","title":"Intro","text":"<p>Because our EC2 instance does not release port other than SSH, also we need to access via jump server, so we need to do below setup if we want to access jupyter hosted on the EC2.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#steps","title":"Steps","text":"<ul> <li>Prerequisite : Assume you already setup .ssh/config according to here</li> <li>Bonus: it is a good idea to create a separate user(e.g. jupyter) to dedicatedly run below service</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#install-anaconda","title":"Install Anaconda","text":"<ul> <li>It is recommended to install anaconda which already included jupyter.</li> <li>SSH to your EC2 host</li> <li> <p>Go to anaconda and find the anaconda installer corresponding to your EC2 architecture</p> <p>ps: you can run uname -m to check the architecture</p> <ul> <li>Copy the download link, run below command and download to your EC2 (current work path) For example, I used this package</li> </ul> <pre><code>wget &lt;https://repo.anaconda.com/archive/Anaconda3-2021.11-Linux-x86_64.sh&gt;\n</code></pre> </li> <li> <p>Install the package by running</p> <pre><code>bash Anaconda3-2021.11-Linux-x86_64.sh\n</code></pre> </li> <li> <p>After installation finished, check the python path is under conda:</p> <pre><code>which python\n## showing something like: /home/ubuntu/anaconda3/bin/python\n</code></pre> </li> <li> <p>Logout your current shell terminal, and relogin again -Reason: becasue it needs to restart your shell to activate the conda env</p> <ul> <li> <p>(If still not already in conda environment) Enter the env now</p> <pre><code>conda activate\n</code></pre> </li> </ul> </li> <li> <p>Now you will see something like <code>(base)</code> front of your command line name. e.g.</p> <pre><code>(base) keith@i-xxxxxxxx:~$\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#generate-a-password-with-sha512-algorithm","title":"Generate a password with sha512 algorithm","text":"<ul> <li> <p>Start python in interactive mode</p> <pre><code>ipython\n</code></pre> </li> <li> <p>Type in below line by line, to encrypt your new password</p> <pre><code>from IPython.lib import passwd\n\npasswd()\n</code></pre> </li> <li> <p>COPY the password generated, which will be used in later step</p> </li> <li>Type <code>exit()</code> to exit the interactive mode</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#configure-jupyter-notebook","title":"Configure Jupyter Notebook","text":"<ul> <li> <p>Create the config by</p> <pre><code>jupyter notebook --generate-config\n</code></pre> </li> <li> <p>The generated config probably is here: <code>~/.jupyter/jupyter_notebook_config.py</code></p> </li> <li> <p>Create a directory as your jupyter workspace, e.g. I am using <code>~/notebook</code> here</p> <pre><code>cd ~/\nmkdir notebook\n</code></pre> </li> <li> <p>Edit the config (by <code>vim jupyter_notebook_config.py</code>, and press <code>i</code> to enter edit mode)</p> </li> <li> <p>And add below to the top</p> <pre><code>c.NotebookApp.ip = 'localhost'\nc.NotebookApp.notebook_dir = '&lt; Put the workspace FULL path here (e.g. /home/keith/notebook/) &gt;'\nc.NotebookApp.open_browser = False\nc.NotebookApp.port = 8888\nc.NotebookApp.password_required = True\nc.NotebookApp.password = u'&lt; Put the encrypted password we generated from above &gt;'\n</code></pre> <ul> <li>Remember to put your owned work space full path, and the encrypted password</li> <li>Save the file by clicking <code>esc</code> then type <code>:wq</code></li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#start-the-jupyter","title":"Start the Jupyter","text":"<ul> <li>Run <code>jupyter notebook</code> to start the service, you should see the message like this:</li> </ul> <pre><code>........\n[I 06:42:59.781 NotebookApp] Serving notebooks from local directory: /home/keith/notebook\n[I 06:42:59.781 NotebookApp] Jupyter Notebook 6.4.5 is running at:\n[I 06:42:59.781 NotebookApp] &lt;http://localhost:8888/&gt;\n[I 06:42:59.781 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#setup-ssh-tunnel","title":"Setup SSH tunnel","text":"<ul> <li>Open another terminal in your PC locally (not in EC2)</li> <li> <p>Run below command</p> <pre><code>ssh -v  -J &lt;username of jump server&gt;@&lt;jump server&gt;:&lt;jump server port&gt; \\\\\n    -N &lt;EC2 user name&gt;@&lt;EC2 host&gt; \\\\\n    -L 8888:localhost:8888\n</code></pre> </li> <li> <p>By using <code>v</code> in above command, we can see actually what the server is doing.</p> </li> <li>You should see some message about forwarding the port 8888</li> <li>PS: you need to keep this terminal opened OR put this as background process</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#open-jupyter-web","title":"Open jupyter web !","text":"<ul> <li>Finally, you can access the notebook in your local PC in localhost:8888</li> <li>It should prompt you to enter the password you setup in previous steps</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#optional-recommened-install-anaconda-extension-for-notebook","title":"(Optional) Recommened!!! Install Anaconda extension for Notebook","text":"<p>Original Jupyter only use your global python kernal to execute the notebooks. But most of the time we do not want to ruin the global environment, or avoid version conflict. Here we will install a few extensions so you can easily manage environments(conda) on jupyter notebook</p> <ul> <li>Login to EC2, with the user account running jupyter</li> <li> <p>Follow the guide to install nb_conda</p> <ul> <li>PS: it should install <code>Notebook Conda Kernels</code> together</li> <li> <p>Or just run below code</p> <pre><code>conda install nb_conda\n</code></pre> </li> </ul> </li> <li> <p>After installed, switch back to main user &amp; restart the jupytrer service</p> <pre><code>sudo systemctl restart &lt;your service name&gt;\n</code></pre> </li> <li> <p>Open your jupyter webpage, now you can see a new tab called <code>Conda</code>. So you can create / manage environment. It is recommended to clone the root env because new env lacks of important package</p> </li> <li>Once you created some environment, you can select a specific environment in a notebook:</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#optional-start-jupyter-automatically-as-service","title":"(Optional) Start jupyter automatically as service","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#configure-the-service","title":"Configure the service","text":"<p>Once you confirmed the above setup is working well, you can configure it to startup automatically when server boot.</p> <ul> <li>Ensure you are now loged in to ec2, with the user that is supposed to run jupyter</li> <li> <p>1st, print out your current $PATH and copy it for later use</p> <pre><code>echo $PATH\n</code></pre> </li> <li> <p>Login to an user account that can run <code>sudo</code></p> </li> <li> <p>Create service config, you can change the name <code>jupyter.service</code> to anything else <code>xxxxxx.service</code></p> <pre><code>sudo touch /lib/systemd/system/jupyter.service\n</code></pre> </li> <li> <p>Edit the service file by running <code>sudo vim /lib/systemd/system/jupyter.service</code>, paste below to the configure file</p> <ul> <li>Reminder: press <code>i</code> to enter edit mode</li> </ul> <pre><code>[Unit]\nDescription=Jupyter Notebook Server\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\n\nEnvironment=\"PATH=&lt; Put the output of `echo $PATH` you run before &gt;\"\n\n# Jupyter Notebook: change PATHs as needed for your system\n# you can check the full path by running `which jupyter`\nExecStart=/home/jupyter/anaconda3/bin/jupyter notebook\n\n# Change the below info to your user detail\nUser=jupyter\nGroup=jupyter\nWorkingDirectory=/home/jupyter\nRestart=always\nRestartSec=10\n#KillMode=mixed\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> </li> <li> <p>Save the config file (esc -&gt; <code>:wq</code>)</p> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/EC2%2032646b8fe8404d7fa9d7ba827e292c67/Install%20jupyter%20on%20AWS%20145b959b7d4643b596a28702894829d8/#setup-the-service-from-config","title":"Setup the service from config","text":"<p>Assuming the service name is <code>jupyter</code></p> <ul> <li> <p>Run below command to set it as auto-start when server boot</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable jupyter\n</code></pre> </li> <li> <p>Start the service (or just do a server reboot)</p> <pre><code>sudo systemctl start ipython-notebook\n</code></pre> </li> <li> <p>You can check the service status by:</p> <pre><code>sudo service jupyter status\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Components%20f77556fe2756432183dfd5f5810d762d/","title":"Components","text":"\ud83d\udca1 ECS  - N x ECS Cluster     - N x ECS Service         - N x Task             - 1 x Task Definition         - 1 of below             - Capacity Provider Strategy                 - N x Capacity provider (Already associated in Cluster)             - Launch type (EC2, Fargate, ...etc)         - 1 x CodeDeploy Deployment group         - 1 x ALB - Target Group (Another 1 x target group for green deployment)     - N x Capacity Providers (Association to this cluster) - N x Task Definition   \ud83d\udca1 EC2  - N x Capacity Providers   \ud83d\udca1 CodeDeploy  - N x CodeDeploy Application     - N x CodeDeploy Deployment group"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Deployment%20Type%20580e5dffac0c43fa920e67957a1fa5a8/","title":"Deployment Type","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Deployment%20Type%20580e5dffac0c43fa920e67957a1fa5a8/#rolling-update","title":"Rolling Update","text":"<p>https://docs.aws.amazon.com/AmazonECS/latest/developerguide/deployment-type-ecs.html</p> <ul> <li>Use the deployment circuit breaker, which can roll back to last completed deployment upon a deployment failure.<ul> <li>Even 1 task is successfully running, it is not a failure....</li> </ul> </li> <li>Explanation with image:</li> </ul> <p></p> <p></p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Deployment%20Type%20580e5dffac0c43fa920e67957a1fa5a8/#bluegreen-deploy","title":"Blue/Green deploy","text":"<ul> <li>Deployment doc https://aws.amazon.com/blogs/compute/bluegreen-deployments-with-amazon-ecs/</li> </ul> <p>https://github.com/aws-samples/ecs-blue-green-deployment</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Deployment%20Type%20580e5dffac0c43fa920e67957a1fa5a8/#automate-deployment-with-codedeploy","title":"Automate Deployment with CodeDeploy","text":"<ul> <li>https://docs.aws.amazon.com/codedeploy/latest/userguide/tutorial-ecs-deployment.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Load%20balancer%20f8127112856d4f79bb7c4349cd841ccd/","title":"Load balancer","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/ECS%20dc875ecdb5d748fc8d154855cf226c61/Load%20balancer%20f8127112856d4f79bb7c4349cd841ccd/#alb","title":"ALB","text":"<ul> <li>AWS resource explanation<ul> <li>1 x ALB<ul> <li>N x Listener \u2192 listen to specific url by defined rules</li> <li>N x Target Group \u2192 point to specific ECS service(s)</li> </ul> </li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Multi%20Tenant%20SaaS%20b26cdf3a5f0a45c9978bad7d5d9c8e96/Resource%201c8e2affec494368b512e3ef4b3e7c51/","title":"Resource","text":"<p>https://aws.amazon.com/partners/programs/saas-factory/tenant-isolation/</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/Call%20other%20service%E2%80%99s%20API%2076f10ffa3a944cab853f5ae9764538f2/","title":"Call other service\u2019s API","text":"<p>https://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html</p> <ul> <li>Via aws sdk intergration (just the name), step function can call the APIs from other services<ul> <li>e.g. STF \u2192 ECS updateService</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/Context%20Object%20%28i%20e%20Variable%29%2093bee75def3b498bbae64490fa41de87/","title":"Context Object (i.e. Variable)","text":"<p>We can use variable in SFN definition (i.e. context object)</p> <ul> <li>https://docs.aws.amazon.com/step-functions/latest/dg/input-output-contextobject.html</li> </ul> <p>Also we can use some limited function called intrinsic function</p> <ul> <li>https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-intrinsic-functions.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/step-functions-data-science-sdk-python%2003d71a42fd4742e8932d34d60e1759fc/","title":"step-functions-data-science-sdk-python","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/step-functions-data-science-sdk-python%2003d71a42fd4742e8932d34d60e1759fc/#link","title":"Link:","text":"<p>https://github.com/aws/aws-step-functions-data-science-sdk-python#overview-of-sdk</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/AWS%200b93b1fd8bed449c99799ce093655abc/Step%20Function%2018d9bbffa2bf4edaa609e181a5035193/step-functions-data-science-sdk-python%2003d71a42fd4742e8932d34d60e1759fc/#quick-fact","title":"Quick Fact:","text":"<p>Step Functions creates workflows out of steps called\u00a0States, and expresses that workflow in the\u00a0Amazon States Language.  When you create a workflow in the AWS Step Functions Data Science SDK, it creates a State Machine representing your workflow and steps in AWS Step Functions. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Access%2069a8e087b8c047fb97dffa66dcc44162/","title":"Access","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Access%2069a8e087b8c047fb97dffa66dcc44162/#image","title":"Image","text":"<p>List everything about images (including intermediate layer)</p> <pre><code>docker images -a\n</code></pre> <p>List Dangling Image </p> <pre><code>docker images -f dangling=true\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Access%2069a8e087b8c047fb97dffa66dcc44162/#containers","title":"Containers","text":"<p>List all</p> <pre><code>docker ps -a\n</code></pre> <p>List containers with filters</p> <pre><code>docker ps -a -f status=exited\n# or more\ndocker ps -a -f status=exited -f status=created\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Container%20control%2089aa2982501e4d59af304096a62cce64/","title":"Container control","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Container%20control%2089aa2982501e4d59af304096a62cce64/#container","title":"Container","text":"<p>Run image &amp; Remove the container upon exit</p> <pre><code>docker run --rm image_name\n</code></pre> <p>Stop ALL container</p> <pre><code>docker stop $(docker ps -a -q)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Docker-Compose%20e1245e590ff94151b379bb4479eec360/","title":"Docker-Compose","text":"<p>It is very useful for local development to startup one/multiple service in a clean environment.</p> <p>Pure docker can help to a certain level. But docker-compose is extremely useful if there is a long list of arguments to startup a docker container, e.g. binding ports, mounting volumes\u2026etc</p> <p>Some more advanced usage such as networking, managing existed container, ENV, \u2026etc</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Docker-Compose%20e1245e590ff94151b379bb4479eec360/#common-usage","title":"Common usage","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Docker-Compose%20e1245e590ff94151b379bb4479eec360/#basic-example","title":"Basic example","text":"<pre><code>version: '3.8'\nservices:\napp:\nbuild: .\n# Use below setting if dockerfile name is changed\n# context: .\n# dockerfile: &lt;specific dockerfile name&gt;\nvolumes:\n- ./src:/app\nports:\n- \"3000:3000\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Dockerfile%20template%2035dd3268923a4f75a8002566e1395a32/","title":"Dockerfile template","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Dockerfile%20template%2035dd3268923a4f75a8002566e1395a32/#common-usage","title":"Common usage","text":"<pre><code>FROM node:16-alpine\n# make the 'app' folder the current working directory\nWORKDIR /app\n# setup ENV\nENV MODE=development\n\n# copy src code\nCOPY . .\n\n# Run command such as apk / apt-get\nRUN apk add --no-cache chromium ca-certificates \\\n# install project dependencies\n  &amp;&amp; npm install \\\n# Run npm script\n  &amp;&amp; npm run xxxxxx \\\n# Add user, chmod, chown...etc\n  &amp;&amp; chmod +x /usr/local/bin/xxxxxx \\\n&amp;&amp; addgroup -S xxxxuser_grp &amp;&amp; adduser -S -G xxxxuser_grp xxxuser \\\n&amp;&amp; chown -R xxxxuser_grp:xxxuser /home/xxxx \\\n# Run as xxxx user\nUSER xxxuser\n# expose certain ports\nEXPOSE 8080 3000\n# entry command\n# Or ENTRYPOINT [ \"/opt/app/run.sh\", \"--port\", \"8080\" ]\nCMD [ \"npm\", \"run\", \"xxxxx\" ]\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Removal%20c33a44b6a7b240dbb714a4e661729c85/","title":"Removal","text":"<p>Docker action always involving a large amount of cache, e.g. layers, docker image\u2026etc</p> <p>But in our development environment, it could accumulate easily after a few projects, which results in the out of storage or not enough memory for docker execution.</p> <p>In this page, there are various docker technique for cleaning useless storage.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Removal%20c33a44b6a7b240dbb714a4e661729c85/#all-in-one","title":"All-in-One","text":"<p>clean up any resources \u2014 images, containers, volumes, and networks </p> <pre><code># \u2014 that are\u00a0*dangling*\u00a0(not tagged or associated with a container):\ndocker system prune\n\n# Or \u2014 all stopped containers and all unused images\ndocker system prune -a\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Removal%20c33a44b6a7b240dbb714a4e661729c85/#images","title":"Images","text":"<p>Single image</p> <pre><code>docker rmi &lt;Image ID&gt; &lt;Image ID&gt;\n</code></pre> <p>All dangling images</p> <pre><code>docker image prune\n</code></pre> <p>ALL images</p> <pre><code>docker rmi $(docker images -a -q)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Docker%20f7dfac3d026d4e2bbe55db4f872ce883/Removal%20c33a44b6a7b240dbb714a4e661729c85/#containers","title":"Containers","text":"<p>Single container</p> <pre><code>docker rm ID_or_Name ID_or_Name\n</code></pre> <p>Remove all exited</p> <pre><code>docker rm $(docker ps -a -f status=exited -q)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Github%20Pages%207bf66f7c179a47a992513bd998fbaf20/","title":"Github Pages","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Github%20Pages%207bf66f7c179a47a992513bd998fbaf20/#background","title":"Background","text":"<ul> <li>https://github.com/keithleungwork/keithleungwork.github.io</li> <li>https://keithleungwork.github.io/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Github%20Pages%207bf66f7c179a47a992513bd998fbaf20/#init","title":"Init","text":"<ul> <li>https://docs.github.com/en/pages/quickstart</li> <li>Fork from here: https://github.com/barryclark/jekyll-now</li> <li>IMPORTANT - the repo name has to be .github.io"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Github%20Pages%207bf66f7c179a47a992513bd998fbaf20/#theme","title":"Theme","text":"<p>https://jekyllrb.com/docs/themes/</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Submodule%20d6a44ac7623d4156ac414f600b6d9d8e/","title":"Submodule","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Submodule%20d6a44ac7623d4156ac414f600b6d9d8e/#add-submodule","title":"Add submodule:","text":"<pre><code>git submodule add &lt;git-url&gt; &lt;optional: path to store&gt;\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Submodule%20d6a44ac7623d4156ac414f600b6d9d8e/#update-submodule","title":"Update submodule:","text":"<pre><code>## sync the url info\ngit submodule sync\n## update &amp;&amp; populate files\ngit submodule update --init\n## REALLY go inside the submodule and pull files\ngit submodule update --init --remote\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Submodule%20d6a44ac7623d4156ac414f600b6d9d8e/#remove-submodule","title":"Remove submodule:","text":"<p>To remove a submodule you need to:</p> <ol> <li>Delete the relevant line from the\u00a0.gitmodules\u00a0file.</li> <li>Delete the relevant section from\u00a0.git/config.</li> <li>Run\u00a0git rm --cached path_to_submodule\u00a0(no trailing slash).</li> <li>Commit the superproject.</li> <li>Delete the now untracked submodule files.</li> <li>rm -rf .git/modules/"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Useful%20github%20cec51c0f6a1c42e8a857db890c798102/","title":"Useful github","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Git%20a4fb38f7907e42089b437b08dc3698d6/Useful%20github%20cec51c0f6a1c42e8a857db890c798102/#global","title":"Global","text":"<ul> <li>gitignore template repo - https://github.com/github/gitignore</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Linux%20Unix%20b716d9e06ab94175b241c48d584c5d86/Font%20issue%20on%20linux%20unix%20aa99881cdee14e94895bb77d91e03834/","title":"Font issue on linux/unix","text":"<p>In container environment, it is quite often to meet issue about font not properly displayed / extracted, e.g. from PDF.</p> <p>It is recommended to install below font lib to allow the OS to read CJK characters.</p> <pre><code># To support Japanese in PDF export - https://wiki.alpinelinux.org/wiki/Fonts\nRUN apk add terminus-font font-noto font-noto-thai \\\nfont-noto-tibetan font-ipa font-sony-misc font-daewoo-misc font-jis-misc\n</code></pre> <p>Or,  another source showing below font libs</p> <pre><code>RUN apt-get install fonts-arphic-ukai fonts-arphic-uming \\\nfonts-ipafont-mincho fonts-ipafont-gothic fonts-unfonts-core\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Linux%20Unix%20b716d9e06ab94175b241c48d584c5d86/Useful%20CMD%20a9ed21d74edb496d80809cff9adabbc4/","title":"Useful CMD","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Linux%20Unix%20b716d9e06ab94175b241c48d584c5d86/Useful%20CMD%20a9ed21d74edb496d80809cff9adabbc4/#get-specific-field-in-a-lines","title":"Get specific field in a line(s)","text":"<pre><code># in xxxx.log : 2022-07-15 17:02:57     107588 xxx.pdf\ncat xxxxx.log | awk '{print $4}'\n# output: xxx.pdf\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/","title":"Computer Vision","text":"<p>IceVision</p> <p>Evaluation Skill</p> <p>Remove specific color</p> <p>Common skills</p> <p>Data augmentation</p> <p>Image data Preprocessing</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/","title":"Dev Resource","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#public-dataset","title":"Public dataset","text":"<ul> <li>Dataset source<ul> <li>Require login - https://public.roboflow.com/</li> </ul> </li> <li>Conll-03</li> <li>MNIST</li> <li>CIFAR-10 - https://www.cs.toronto.edu/~kriz/cifar.html<ul> <li>10 classes labeled images</li> </ul> </li> <li>Coco dataset - https://cocodataset.org/#home</li> <li>LVIS - Large Vocabulary Instance Segmentation<ul> <li>https://www.lvisdataset.org/</li> <li>~2 million high-quality instance segmentation masks for over 1000 entry-level object categories in 164k images</li> </ul> </li> <li>Flower dataset<ul> <li>TF - https://www.tensorflow.org/datasets/catalog/tf_flowers</li> </ul> </li> <li>CARLA - self-driving car dataset<ul> <li>https://paperswithcode.com/dataset/carla</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#packagelibrary","title":"Package/Library \ud83d\udd27","text":"<ul> <li>List of all popular ML package for various fields<ul> <li>https://github.com/josephmisiti/awesome-machine-learning#python-natural-language-processing</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#small-package","title":"Small package :","text":"<ul> <li> <p>deepchecks - https://deepchecks.com/</p> <ul> <li> <p>A very easy to use lib to do model &amp; data checking, e.g. train/test set distribution check</p> <p></p> </li> <li> <p>Very useful seemingly</p> </li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#ml-core-framework","title":"ML core framework :","text":"<ul> <li>Scikit-learn (Based on SciPy)<ul> <li>https://scikit-learn.org/stable/install.html</li> <li>The user guide is too detailed. Best usage if you have a target to search for.<ul> <li>https://scikit-learn.org/stable/user_guide.html</li> </ul> </li> </ul> </li> <li> <p>Pycaret</p> <ul> <li>https://pycaret.gitbook.io/docs/</li> <li>wrapper around several machine learning libraries and frameworks, such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and a few more.</li> </ul> </li> <li> <p>PyTorch</p> <ul> <li>Machine Learning lib</li> <li>https://pytorch.org/get-started/locally/</li> </ul> </li> <li>pytorch lightning (Based on Pytorch)<ul> <li>A high level API allow researcher to build PyTorch model more efficiently</li> <li>i.e. More speedy in coding</li> <li>https://www.pytorchlightning.ai/</li> </ul> </li> <li> <p>Fastai (Based on PyTorch)</p> <ul> <li>Deep Learning lib</li> <li>Allow the control from High, Middle, Low level</li> <li>Domains: NLP, CV, Tabular, (?)collaborative filtering</li> <li>NOT support Mac, so run it on Cloud/Colab</li> </ul> </li> <li> <p>Tensorflow (VS PyTorch)</p> </li> <li>Keras (Based on Tensorflow)</li> <li>Autokeras (Based on Keras)<ul> <li>Deep Learning lib</li> <li>Similar to scikit but more high level api</li> <li>e.g. you can specify a task type, they help to choose the model algorithm</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#domain-specific","title":"Domain-Specific :","text":"<ul> <li>NLP<ul> <li>Scikit-crfsuite<ul> <li>CRF model classifier</li> </ul> </li> <li>MITIE<ul> <li>Train NER model</li> </ul> </li> <li>NCRF++</li> <li>FastText - https://fasttext.cc/docs/en/supervised-tutorial.html<ul> <li>fastText is a library for efficient learning of word representations and sentence classification.****</li> </ul> </li> </ul> </li> <li>Computer vision<ul> <li>Framework<ul> <li>OpenMMLab - https://openmmlab.com/codebase<ul> <li>From China, quite a big framework</li> <li>Wrap up a wide range of application.</li> </ul> </li> <li>Icevision - https://airctic.com/0.12.0/</li> <li>YOLOv5 - https://docs.ultralytics.com/<ul> <li>Quite infamous due to not recognized by original YOLO author</li> <li>try YOLOv4 instead</li> </ul> </li> <li>Detectron</li> <li>Mediapipe</li> </ul> </li> <li>Data<ul> <li>augmentation lib - Albumentations</li> </ul> </li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#tool-software","title":"Tool / Software","text":"<ul> <li>Annotation tool:<ul> <li>LabelImg</li> <li>labelstudio - https://labelstud.io/<ul> <li>can label many types of data(img, audio, text\u2026etc)</li> <li>Extremely useful than labelimg (recommened by colleagues)</li> </ul> </li> <li>CVAT</li> <li>makesense.ai</li> <li>Labelbox</li> <li>Roboflow (not free for business)</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Dev%20Resource%2047b2cee0e4064220a55d5bc696012b07/#resource-list","title":"Resource List","text":"<ul> <li>State-of-the-Art models list - https://paperswithcode.com/sota</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/","title":"Glossary","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#maths","title":"Maths :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#euclidean-distance","title":"Euclidean Distance","text":"<ul> <li>Pronounce: \u201cyu cly di an\u201d</li> <li>Distance between 2 points</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#pythagorean-theorem","title":"Pythagorean theorem","text":"<ul> <li>Pronounce: \u201cpy tha gor ri an\u201d</li> <li>a^2 + b^2 = c^2</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#statistic","title":"Statistic :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#gaussian-distribution-aka-normal-distribution","title":"Gaussian Distribution (a.k.a. Normal Distribution)","text":"<p>(img from wiki)</p> <p></p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#orthogonal","title":"orthogonal","text":"<ul> <li>We say that 2 vectors are orthogonal\u00a0if they are perpendicular to each other . i.e. the dot product of the two vectors is zero.</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#variance","title":"Variance","text":"<ul> <li> <p>variance measures the average degree to which each point differs from the mean.</p> <p></p> </li> <li> <p>Note the diff:</p> <ul> <li>Variance is the average squared deviations from the mean, while standard deviation is the square root of this number</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#sample-with-replacement","title":"Sample with replacement","text":"<ul> <li>It means the sample drawn will be put back to the pool before next drawn</li> <li>i.e. multiple drawn is independent of each others</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#stochastic","title":"Stochastic","text":"<ul> <li>Stochastic refers to the property of being well described by a random probability distribution.</li> <li>Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves</li> <li>But two terms are often used synonymously.</li> <li>~= Random</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#squared-residual","title":"Squared residual","text":"<ul> <li>a.k.a.  Residual sum of squares,  sum of squared residuals, sum of squared estimate of errors</li> <li>It is the sum of the squares of residuals. It is a measure of the discrepancy between the data and an estimation model</li> <li> <p>What is Residual ? (Image from https://www.statology.org/residuals/)</p> <p></p> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#machine-learning","title":"Machine Learning","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#regularizations","title":"Regularizations","text":"<ul> <li>Regularization is a technique that helps overcoming over-fitting problem in machine learning models. It is called Regularization as it helps keeping the parameters regular or normal</li> <li>e.g. the L2 regularization in Ridge Regression</li> <li>https://medium.com/@minions.k/ridge-regression-l1-regularization-method-31b6bc03cbf#:~:text=Regularization is a technique that,the parameters regular or normal.</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Glossary%20dbc83e6c01494cc28b3e4a3d74993f93/#variance_1","title":"Variance","text":"<ul> <li>Note: not the variance in statistic</li> <li> <p>Image from https://medium.com/@6453gobind/bias-variance-trade-off-87986b5b5add</p> <p></p> </li> <li> <p>i.e. high variance = overfit</p> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/","title":"List of Algorithm","text":"<p>Standard Algorithm</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/#nlp-nnsalgorithms","title":"NLP NNs/Algorithms","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/#computer-vision-nnsalgorithms","title":"Computer Vision NNs/Algorithms","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/#yolo","title":"Yolo","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/#r-cnn-fast-r-cnn-faster-r-cnn","title":"R-CNN / Fast R-CNN / Faster R-CNN","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/#vit-vision-transformers","title":"ViT (Vision Transformers)","text":"<ul> <li>https://viso.ai/deep-learning/vision-transformer-vit/</li> <li>ViTDet by facebook - https://ai.facebook.com/blog/efficient-accurate-object-detection-for-hundreds-of-uncommon-object-classes/?r=1ltz65&amp;utm_source=substack&amp;utm_medium=email</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Learning%20Approaches%201bebdeaca31949e29e4db04faf5fe153/","title":"List of Learning Approaches","text":"<p>One-Shot Learning</p> <p>Weak Learning</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/","title":"NLP","text":"<p>Preprocess</p> <p>NER tagging scheme</p> <p>CJK special handling</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/","title":"Practical Skills","text":"<p>These groups of post are by the phrases in a ML project</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#go-through-this-before-starting-a-new-project","title":"Go through this before starting a new project !!!","text":"<p>Procedure of a New ML Project</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#data-collection-analysis","title":"Data collection &amp; analysis:","text":"<p>Annotation</p> <p>Dimensionality Reduction</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#system-design","title":"System design:","text":"<p>Ensemble Model</p> <p>MLOps</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#data-preprocess-feature-engineer","title":"Data preprocess &amp; Feature engineer:","text":"<p>Stratified sampling</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#model-building","title":"Model building:","text":"<p>Tuning &amp; optimization</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#evaluation","title":"Evaluation:","text":"<p>Metrics</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#deployment","title":"Deployment:","text":"<p>Onnx</p> <p>Compile model against target hardware</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#monitoring","title":"Monitoring:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#subsequent-phraseeg-retraining","title":"Subsequent phrase(e.g. retraining):","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/#others","title":"Others:","text":"<p>Few-shot object detection</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Pytorch%2076538c4169bb4344a6c836cc47cb7285/","title":"Pytorch","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Pytorch%2076538c4169bb4344a6c836cc47cb7285/#snippets","title":"Snippets","text":"<p>Check GPU</p> <pre><code># check cuda\nimport torch\ntorch.cuda.is_available(), torch.cuda.device_count()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/","title":"Tensorflow / Keras","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#data","title":"Data","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#split-image-data-into-train-dev-set-with-image_dataset_from_directory","title":"Split image data into train, dev set with <code>image_dataset_from_directory</code>","text":"<ul> <li>ref - https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory</li> </ul> <p>Usage sample :</p> <pre><code>BATCH_SIZE = 32\nIMG_SIZE = (160, 160)\ndirectory = \"dataset/\"\ntrain_dataset = image_dataset_from_directory(directory,\nshuffle=True,\nbatch_size=BATCH_SIZE,\nimage_size=IMG_SIZE,\nvalidation_split=0.2,\nsubset='training',\nseed=42)\nvalidation_dataset = image_dataset_from_directory(directory,\nshuffle=True,\nbatch_size=BATCH_SIZE,\nimage_size=IMG_SIZE,\nvalidation_split=0.2,\nsubset='validation',\nseed=42)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#using-datasetprefetch-to-prevent-memory-bottleneck","title":"Using <code>&lt;dataset&gt;.prefetch</code> to prevent memory bottleneck","text":"<ul> <li>Memory bottleneck that can occur when reading from disk. This method sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.</li> <li>You can set the number of elements to prefetch manually, or you can use\u00a0<code>tf.data.experimental.AUTOTUNE</code>\u00a0to choose the parameters automatically. Autotune prompts\u00a0<code>tf.data</code>\u00a0to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations.</li> </ul> <pre><code>AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#data-augmentation-vision","title":"Data Augmentation (Vision)","text":"<p>Keras' Sequential API offers a straightforward method for these kinds of data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later. Ahh, so convenient!</p> <ul> <li>https://www.tensorflow.org/tutorials/images/data_augmentation</li> </ul> <pre><code># A function for data augmentation\ndef data_augmenter():\n'''\n    Create a Sequential model composed of 2 layers\n    Returns:\n        tf.keras.Sequential\n    '''\ndata_augmentation = tf.keras.Sequential()\ndata_augmentation.add(RandomFlip('horizontal'))\ndata_augmentation.add(RandomRotation(0.2))\nreturn data_augmentation\n## usage\ndata_augmentation = data_augmenter()\n# simple test: augmented_image = data_augmentation( [first_image] )\n# some input layers\ninputs = tf.keras.Input(shape=input_shape)\n# apply data augmentation to the inputs\nx = data_augmentation(inputs)\n# some other layers.....\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#model","title":"Model","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#compile-model","title":"Compile model","text":"<ul> <li>When the model is created, you can compile it for training with an optimizer and loss of your choice.</li> <li>When the string\u00a0<code>accuracy</code>\u00a0is specified as a metric, the type of accuracy used will be automatically converted based on the loss function used. This is one of the many optimizations built into TensorFlow that make your life easier! If you'd like to read more on how the compiler operates, check the docs\u00a0here</li> </ul> <pre><code>created_model.compile(optimizer='adam',\nloss='binary_crossentropy',\nmetrics=['accuracy'])\n# or\nmodel2.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\nloss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\nmetrics=['accuracy'])\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#train-a-model","title":"Train a model","text":"<pre><code>history = happy_model.fit(X_train, Y_train, epochs=10, batch_size=16)\n# below is a common code to visualize history\n# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n# model was trained on. \ndf_loss_acc = pd.DataFrame(history.history)\ndf_loss= df_loss_acc[['loss','val_loss']]\ndf_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\ndf_acc= df_loss_acc[['accuracy','val_accuracy']]\ndf_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\ndf_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\ndf_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#evaluate-a-model","title":"Evaluate a model","text":"<pre><code>happy_model.evaluate(X_test, Y_test)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#_1","title":"Tensorflow / Keras","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#manually-create-neural-network","title":"Manually Create Neural Network","text":"<p>Prerequisite Knowledge:</p> <ul> <li>TF Keras Layers - https://www.tensorflow.org/api_docs/python/tf/keras/layers</li> <li>Sequential API - https://www.tensorflow.org/guide/keras/sequential_model<ul> <li>You can also add layers incrementally to a Sequential model with the\u00a0<code>.add()</code>\u00a0method, or remove them using the\u00a0<code>.pop()</code>\u00a0method, much like you would in a regular Python list.</li> <li>Actually, you can think of a Sequential model as behaving like a list of layers. Like Python lists, Sequential layers are ordered, and the order in which they are specified matters.</li> <li>If your model is non-linear or contains layers with multiple inputs or outputs, a Sequential model wouldn't be the right choice!</li> </ul> </li> <li>Or, Functional API - https://www.tensorflow.org/guide/keras/functional<ul> <li>more flexible than the\u00a0<code>tf.keras.Sequential</code>\u00a0API</li> <li>Can handle NON-linear topology, such as Skip connection(e.g. ResNet)</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#common-layers","title":"Common Layers:","text":"<ul> <li>ZeroPadding2D: pad the image with zero</li> <li>MaxPool2D: Pooling layer</li> <li>Conv2D: CNN layer</li> <li>BatchNormalization: Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.<ul> <li>Importantly, batch normalization works differently during training and during inference.</li> </ul> </li> <li>ReLU</li> <li>Flatten</li> <li>Fully-connected (Dense) layer: Apply a fully connected layer with N neurons and activation (e.g. sigmoid).</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Tensorflow%20Keras%20d3f4513db20e48c79b55c98f61ece187/#build-the-model-architecture","title":"Build the model architecture:","text":"<ul> <li>With sequential API:</li> </ul> <pre><code>model = tf.keras.Sequential([\n#... all layers definition\n])\n</code></pre> <ul> <li>With functional API:</li> </ul> <pre><code># Define Input layer\ninput_img = tf.keras.Input(shape=input_shape)\n# Define all hidden layers\n# e.g. \n# First component of main path\nX = Conv2D(\nfilters = 10, kernel_size = 1, strides = (1,1),\npadding = 'valid', kernel_initializer = initializer(seed=0)\n)(X)\nX = BatchNormalization(axis = 3)(X, training = training) # Default axis\nX = Activation('relu')(X)\n# ...\noutputs = tf.keras.layers.Dense(32)(X)\n# Create model\nmodel = tf.keras.Model(inputs=input_img, outputs=outputs)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/","title":"Theory","text":"<p>Weight &amp; Bias &amp; Activation &amp; Activation Function</p> <p>Cost function &amp; back propagation &amp; gradient</p> <p>Semi-supervised learning</p> <p>Gradient Descent</p> <p>Output layer</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/","title":"scikit-learn","text":"<p>It is a popular framework for </p> <ul> <li>its rich content,</li> <li>wide selection of model algorithms,</li> <li>unified API coding style</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#preprocessing","title":"Preprocessing :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#train_test_split","title":"train_test_split :","text":"<pre><code>from sklearn.model_selection import train_test_split\n# without random_state = 0, the split will be random every time\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#models","title":"Models :","text":"<p>Record the algorithm I\u2019ve used or seen.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#example-of-unified-api","title":"Example of unified API :","text":"<pre><code># import the algorithm you want\n# Only this line varies by the model you choose\nfrom sklearn.tree import DecisionTreeRegressor\n# From here are all similar\nmodel = DecisionTreeRegressor(random_state=0)\n# Train\nmodel.fit(X,y)\n# Predict\npredictions = model.predict(test_data)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#metrics","title":"Metrics :","text":"<ul> <li>i.e. <code>from sklearn.metrics import xxxx</code></li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/scikit-learn%20a657b47a258e48068879ad0460f64378/#mean_absolute_error","title":"mean_absolute_error :","text":"<pre><code>from sklearn.metrics import mean_absolute_error\npred = model.predict(X)\nmean_absolute_error(y, pred)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Common%20skills%20019a0c77fee747208fc09a9bd00e50b4/","title":"Common skills","text":"<p>flattened the RGB images to monochrome (by taking the mean of each channel)</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Common%20skills%20019a0c77fee747208fc09a9bd00e50b4/#morphological-transformations","title":"Morphological Transformations","text":"<p>It is to \u201crestore\u201d  an image broken parts</p> <ul> <li>OpenCV - https://docs.opencv.org/4.x/d9/d61/tutorial_py_morphological_ops.html</li> <li>Blog post - https://pyimagesearch.com/2021/12/01/ocr-passports-with-opencv-and-tesseract/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Data%20augmentation%209620ba2875c14418a71045b110a874df/","title":"Data augmentation","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Evaluation%20Skill%201c640e4292834623a50aba5bce0d80bc/","title":"Evaluation Skill","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Evaluation%20Skill%201c640e4292834623a50aba5bce0d80bc/#iou-intersection-over-union-iou-for-object-detection","title":"IoU (*Intersection over Union (IoU) for object detection)*","text":"<p>Common way to evaluate the performance of the object detection model</p> <p></p> <p>https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/ </p> <p>https://towardsdatascience.com/iou-a-better-detection-evaluation-metric-45a511185be1 </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/IceVision%207277e35f8d3841eda8685134ad8de1bb/","title":"IceVision","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/IceVision%207277e35f8d3841eda8685134ad8de1bb/#quick-fact","title":"Quick Fact :","text":"<ul> <li>https://airctic.com/0.12.0/install/</li> <li>It wraps up fastai, pytorch lightning</li> <li> <p>Its unified API allow to choose libraries \u2192 their models \u2192 their backbones</p> <pre><code># e.g. lib = mmdet, model = retinanet, backbone = resnet50_fpn_1x\nmodel_type = models.mmdet.retinanet\nbackbone = model_type.backbones.resnet50_fpn_1x(pretrained=True)\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/IceVision%207277e35f8d3841eda8685134ad8de1bb/#prerequisite-knowledge","title":"Prerequisite Knowledge :","text":"<p>Basic knowledge of </p> <ul> <li>Fastai / Pytorch Lightning</li> <li>YOLO</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/IceVision%207277e35f8d3841eda8685134ad8de1bb/#frequent-issue","title":"Frequent Issue :","text":"<ul> <li> <p>Issue - Icevision is suck on dependencies, the functional env so far :</p> <pre><code># packages\npython==3.7.13\nicevision[all]==0.11.0\nyolov5-icevision==6.0.0\n\"opencv-python-headless&lt;4.3\"\nsetuptools==61.2.0\n</code></pre> </li> <li> <p>issue - how to do inference with the exported model ?</p> <ul> <li>i.e. how to load trained checkpoints</li> <li> <p>option 1 - official inference method <code>model_from_checkpoint</code></p> <ul> <li> <p>weird, need to install mmcv-full even if you use yolo5:</p> <pre><code>pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cpu/torch1.10.0/index.html\n#mmcv-full-1.5.3.tar.gz\n</code></pre> </li> </ul> </li> <li> <p>option 2 - use fastai / torch to load them manually</p> </li> </ul> </li> <li> <p>issue - training or data loader hanged  locally (CPU)</p> <ul> <li>because CPU env lead to deadlock</li> <li>https://github.com/pytorch/pytorch/issues/1355</li> <li>You need to set num of workers to 0</li> </ul> </li> <li> <p>issue - images / plot not showing</p> <ul> <li>you need to put <code>%matplotlib inline</code> or others in the top of notebook</li> <li>https://github.com/matplotlib/matplotlib/issues/14534</li> </ul> </li> <li> <p>issue - about \u201cimage truncated\u201d, below code can solve it</p> <pre><code>from PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Image%20data%20Preprocessing%20bafb2892b94845318e55bee21e58df40/","title":"Image data Preprocessing","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Image%20data%20Preprocessing%20bafb2892b94845318e55bee21e58df40/#image-dataset-of-various-size","title":"Image dataset of various size","text":"<p>How to handle images of different size - https://wandb.ai/ayush-thakur/dl-question-bank/reports/How-to-Handle-Images-of-Different-Sizes-in-a-Convolutional-Neural-Network--VmlldzoyMDk3NzQ </p> <p>Two large classes of solution</p> <ul> <li>Transformation</li> <li>Inherent network properties</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Computer%20Vision%20d5f9345ff68b4088a0a3ab48b57f7a23/Remove%20specific%20color%20cedda64cb8c84caa81c8d7254bf4030b/","title":"Remove specific color","text":"<p>Use openCV to remove</p> <pre><code>import pytesseract\nimport cv2\npytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe'\nimg = cv2.imread('idText.png')\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nadaptiveThresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 35, 90)\nconfig = '-l eng --oem 1 --psm 3'\ntext = pytesseract.image_to_string(adaptiveThresh, config=config)\nprint(\"Result: \" + text)\ncv2.imshow('original', img)\ncv2.imshow('adaptiveThresh', adaptiveThresh)\ncv2.waitKey(0)\n</code></pre> <p>Or apply <code>morphological transformations</code> (i.e. Erosion)</p> <pre><code>import cv2\nimage = cv2.imread('1.png')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray,105, 255, cv2.THRESH_BINARY_INV)[1]\nthresh = 255 - thresh\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\nresult = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\ncv2.imshow('thresh', thresh)\ncv2.imshow('result', result)\ncv2.imwrite('result.png', result)\ncv2.waitKey()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/","title":"Standard Algorithm","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#linear-models","title":"Linear Models :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#linear-regressionordinary-least-square","title":"Linear Regression(Ordinary Least Square)  :","text":"<p>https://scikit-learn.org/stable/modules/linear_model.html</p> <ul> <li>Basic algorithms in ML.</li> <li>To perform classification with generalized linear models, see\u00a0Logistic regression instead.</li> <li>There are a lot of kinds of linear models. But the fundamental is, output <code>y</code> is linear to features <code>x</code> (i.e. no x^2\u2026etc)</li> </ul> <p>Simplest Linear Regression :</p> <pre><code>from sklearn import linear_model\nreg = linear_model.LinearRegression()\nreg.fit(\n[[0, 0], [1, 1], [2, 2]],\n[0, 1, 2]\n)\n# The coefficient\nreg.coef_\n# output : array([0.5, 0.5]) \n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#ridge-regression","title":"Ridge Regression :","text":"<ul> <li>A Linear regression model with L2 regularization (L2-norm)</li> <li>An explanation - https://medium.com/@minions.k/ridge-regression-l1-regularization-method-31b6bc03cbf#:~:text=Regularization is a technique that,the parameters regular or normal.</li> </ul> <pre><code>from sklearn import linear_model\nreg = linear_model.Ridge(alpha=.5)\nreg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\nreg.coef_\n# array([0.34545455, 0.34545455])\nreg.intercept_\n# 0.13636...\n</code></pre> <p>Q: Why adding L2-norm ?</p> <p>Ans: </p> <p>Simple Linear regression (Ordinary Least Square) is easy to overfit. It performs well in train data but worse in test data and new future data.</p> <p>We use Ridge Regression to find a new line that doesn\u2019t fit the training data well. In other words, we introduce a small bias into how the new line is fit to data and in return we obtain a significant drop in variance. By starting with a slightly worse fit, Ridge regression can provide better long-term predictions.</p> <p>Q: How to choose the alpha value ?</p> <p>Ans:</p> <p>The higher the alpha, the less y being sensitive to feature x</p> <p>To find the optimum values of lambda that results in lowest variance, we use \u201c10-fold Cross Validation Method\u201d. (quoted from here)</p> <pre><code>import numpy as np\nfrom sklearn import linear_model\nreg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))\nreg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n# RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,\n#      1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))\nreg.alpha_\n# 0.01\n# ???? So 0.01 is the best alpha for the given X training data ???\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#ridge-classification","title":"Ridge Classification :","text":"<p>The\u00a0<code>[RidgeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier)</code> can be significantly faster than e.g.\u00a0<code>[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)</code> with a high number of classes because it can compute the projection matrix only once.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#decision-tree","title":"Decision Tree :","text":"<p>https://scikit-learn.org/stable/modules/tree.html</p> <p>The most basic single Decision Tree algorithm. From 1 parent root to N children leaf nodes.</p> <pre><code># Or DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor, export_text\nfrom sklearn import tree\nclf = tree.DecisionTreeRegressor()\nclf = clf.fit(X, y)\n# View the tree visually\ntree.plot_tree(clf) \n# Or in textual way\nr = export_text(clf, feature_names=iris['feature_names'])\nprint(r)\n</code></pre> <p>Pros:</p> <ul> <li>Simple, interpretable, can visualize</li> <li>Speed can be fast, depending on the tree depth</li> </ul> <p>Cons:</p> <ul> <li>Very easy to become overfitting<ul> <li>Solution: Pruning, or set min num of data points within each leaf nodes (i.e. to support)</li> </ul> </li> <li>Unstable, because a small new data can result in a totally different tree</li> <li>Output is NOT continuous nor smooth</li> <li>Data is easy to be biased if some classes are dominate.<ul> <li>Solution: balance the training data across classes</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/List%20of%20Algorithm%205071c5d4cfec46b68085a06b3313eca6/Standard%20Algorithm%202b9d0b4fb83f4dfd8c846481c2590f6b/#random-forest","title":"Random Forest :","text":"<p>It is like ensemble of multiple decision trees, each of tree represent 1 component.</p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/CJK%20special%20handling%20bbb40773352e41ada00522840557a5aa/","title":"CJK special handling","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/CJK%20special%20handling%20bbb40773352e41ada00522840557a5aa/#resource","title":"Resource","text":"<ul> <li>A good site to check a character encoding, block library - https://unicode-table.com/en/23CFE/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/CJK%20special%20handling%20bbb40773352e41ada00522840557a5aa/#mysql","title":"MySQL :","text":"<p>If inserting special character into MySQL.</p> <p>(such as <code>\ud84f\udcfe</code>, it is CJK Unified Ideographs Extension B, according to here)</p> <p>Ensure the column charset is correct.</p> <p>since v5.5, it is available in the\u00a0<code>[utf8mb4](http://dev.mysql.com/doc/en/charset-unicode-utf8mb4.html)</code>,\u00a0<code>[utf16](http://dev.mysql.com/doc/en/charset-unicode-utf16.html)</code>,\u00a0<code>[utf16le](http://dev.mysql.com/doc/en/charset-unicode-utf16le.html)</code>\u00a0and\u00a0<code>[utf32](http://dev.mysql.com/doc/en/charset-unicode-utf32.html)</code>\u00a0character sets.</p> <p>It is not available in MySQL's\u00a0<code>big5</code>\u00a0or\u00a0<code>gbk</code>\u00a0character sets. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/NER%20tagging%20scheme%20ecd78e979f5d4113b25ed98c27974f07/","title":"NER tagging scheme","text":"<p>BIO scheme</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/Preprocess%203c81b05b1a27424ba14222704d2ab813/","title":"Preprocess","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/NLP%20656ed215e92c418fa025bab6ca17a037/Preprocess%203c81b05b1a27424ba14222704d2ab813/#normalize-text-especially-cjk-character","title":"Normalize text (especially CJK character)","text":"<pre><code>text = re.sub(r\"\\xa0\", \" \", text)\ntext = re.sub(r\"\\xA0\", \" \", text)\ntext = re.sub(r\"\\t\", \" \", text)\n</code></pre> <pre><code>import unicodedata\ntext = \"\u682a\u5f0f\u4f1a\u793e\uff2b\uff21\uff24\uff2f\uff2b\uff21\uff37\uff21\\u3000\uff26\uff55\uff54\uff55\uff52\uff45\\u3000\uff30\uff55\uff42\uff4c\uff49\uff53\uff48\uff49\uff4e\uff47\"\ntext = unicodedata.normalize(\"NFKC\", text)\n# it will become \u682a\u5f0f\u4f1a\u793eKADOKAWA Future Publishing\n</code></pre> <ul> <li> <p>For <code>unicodedata.normalize</code>, more explanation is here</p> <pre><code>\uff71\uff72\uff73\uff74\uff75 ==(NFC)==&gt; \uff71\uff72\uff73\uff74\uff75\n\uff71\uff72\uff73\uff74\uff75 ==(NFD)==&gt; \uff71\uff72\uff73\uff74\uff75\n\uff71\uff72\uff73\uff74\uff75 ==(NFKC)==&gt; \u30a2\u30a4\u30a6\u30a8\u30aa\n\uff71\uff72\uff73\uff74\uff75 ==(NFKD)==&gt; \u30a2\u30a4\u30a6\u30a8\u30aa\n\u30d1\u30d4\u30d7\u30da\u30dd ==(NFC)==&gt; \u30d1\u30d4\u30d7\u30da\u30dd\n\u30d1\u30d4\u30d7\u30da\u30dd ==(NFD)==&gt; \u30cf\u309a\u30d2\u309a\u30d5\u309a\u30d8\u309a\u30db\u309a\n\u30d1\u30d4\u30d7\u30da\u30dd ==(NFKC)==&gt; \u30d1\u30d4\u30d7\u30da\u30dd\n\u30d1\u30d4\u30d7\u30da\u30dd ==(NFKD)==&gt; \u30cf\u309a\u30d2\u309a\u30d5\u309a\u30d8\u309a\u30db\u309a\n\uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f ==(NFC)==&gt; \uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f\n\uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f ==(NFD)==&gt; \uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f\n\uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f ==(NFKC)==&gt; \u30d1\u30d4\u30d7\u30da\u30dd\n\uff8a\uff9f\uff8b\uff9f\uff8c\uff9f\uff8d\uff9f\uff8e\uff9f ==(NFKD)==&gt; \u30cf\u309a\u30d2\u309a\u30d5\u309a\u30d8\u309a\u30db\u309a\n\uff41\uff42\uff43\uff21\uff22\uff23 ==(NFC)==&gt; \uff41\uff42\uff43\uff21\uff22\uff23\n\uff41\uff42\uff43\uff21\uff22\uff23 ==(NFD)==&gt; \uff41\uff42\uff43\uff21\uff22\uff23\n\uff41\uff42\uff43\uff21\uff22\uff23 ==(NFKC)==&gt; abcABC\n\uff41\uff42\uff43\uff21\uff22\uff23 ==(NFKD)==&gt; abcABC\n\uff11\uff12\uff13 ==(NFC)==&gt; \uff11\uff12\uff13\n\uff11\uff12\uff13 ==(NFD)==&gt; \uff11\uff12\uff13\n\uff11\uff12\uff13 ==(NFKC)==&gt; 123\n\uff11\uff12\uff13 ==(NFKD)==&gt; 123\n\uff0b\uff0d\uff0e\uff5e\uff09\uff5d ==(NFC)==&gt; \uff0b\uff0d\uff0e\uff5e\uff09\uff5d\n\uff0b\uff0d\uff0e\uff5e\uff09\uff5d ==(NFD)==&gt; \uff0b\uff0d\uff0e\uff5e\uff09\uff5d\n\uff0b\uff0d\uff0e\uff5e\uff09\uff5d ==(NFKC)==&gt; +-.~)}\n\uff0b\uff0d\uff0e\uff5e\uff09\uff5d ==(NFKD)==&gt; +-.~)}\n</code></pre> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Annotation%20ec7b90fdc8524b0bb8b9dd2f1d8f055c/","title":"Annotation","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Annotation%20ec7b90fdc8524b0bb8b9dd2f1d8f055c/#format","title":"Format","text":"<ul> <li>PascalVOC - http://host.robots.ox.ac.uk/pascal/VOC/<ul> <li>the format used by\u00a0ImageNet</li> </ul> </li> <li>YOLO</li> <li>CreateML</li> <li>COCO</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Annotation%20ec7b90fdc8524b0bb8b9dd2f1d8f055c/#voc-to-yolo-converter","title":"VOC to YOLO converter","text":"<p>Below is the code piece to convert Pascal VOC into YOLO format. Original source here:</p> <p>https://towardsdatascience.com/convert-pascal-voc-xml-to-yolo-for-object-detection-f969811ccba5</p> <pre><code>import xml.etree.ElementTree as ET\nimport glob\nimport os\nimport json\nfrom pathlib import Path\ndef xml_to_yolo_bbox(bbox, w, h):\n# xmin, ymin, xmax, ymax\nx_center = ((bbox[2] + bbox[0]) / 2) / w\ny_center = ((bbox[3] + bbox[1]) / 2) / h\nwidth = (bbox[2] - bbox[0]) / w\nheight = (bbox[3] - bbox[1]) / h\nreturn [x_center, y_center, width, height]\ndef yolo_to_xml_bbox(bbox, w, h):\n# x_center, y_center width heigth\nw_half_len = (bbox[2] * w) / 2\nh_half_len = (bbox[3] * h) / 2\nxmin = int((bbox[0] * w) - w_half_len)\nymin = int((bbox[1] * h) - h_half_len)\nxmax = int((bbox[0] * w) + w_half_len)\nymax = int((bbox[1] * h) + h_half_len)\nreturn [xmin, ymin, xmax, ymax]\nclasses = []\nroot_path = Path(\"/xxxxxx\")\ninput_dir = root_path.joinpath(\"annotations/\")\noutput_dir = root_path.joinpath(\"labels/\")\nimage_dir = root_path.joinpath(\"images/\")\n# create the labels folder (output directory)\nif not os.path.isdir(output_dir):\nos.mkdir(output_dir)\n# identify all the xml files in the annotations folder (input directory)\nfiles = glob.glob(os.path.join(input_dir, '*.xml'))\n#print(files)\n# loop through each \nfor fil in files:\nbasename = os.path.basename(fil)\nfilename = os.path.splitext(basename)[0]\n# check if the label contains the corresponding image file\n# !!! use jpg instead in later implementation\nif not os.path.exists(os.path.join(image_dir, f\"{filename}.png\")):\nprint(f\"{filename} image does not exist!\")\ncontinue\nresult = []\n# parse the content of the xml file\ntree = ET.parse(fil)\nroot = tree.getroot()\nwidth = int(root.find(\"size\").find(\"width\").text)\nheight = int(root.find(\"size\").find(\"height\").text)\nfor obj in root.findall('object'):\nlabel = obj.find(\"name\").text\n# check for new classes and append to list\nif label not in classes:\nclasses.append(label)\nindex = classes.index(label)\npil_bbox = [int(x.text) for x in obj.find(\"bndbox\")]\nyolo_bbox = xml_to_yolo_bbox(pil_bbox, width, height)\n# convert data to string\nbbox_string = \" \".join([str(x) for x in yolo_bbox])\nresult.append(f\"{index} {bbox_string}\")\nif result:\n# generate a YOLO format text file for each xml file\nwith open(os.path.join(output_dir, f\"{filename}.txt\"), \"w\", encoding=\"utf-8\") as f:\nf.write(\"\\n\".join(result))\n# generate the classes file as reference\nwith open(root_path.joinpath('classes.txt'), 'w', encoding='utf8') as f:\nf.write(json.dumps(classes))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Compile%20model%20against%20target%20hardware%20cbfdbabda769473f9fbcf9c90e4c0255/","title":"Compile model against target hardware","text":"<p>To optimize the model for a different target hardware.</p> <p>For example, use Sagemaker Neo</p> <p>HOWEVER, it only supports a very limited range of model format and model architecture.</p> <p>List here - https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-cloud.html</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/","title":"Dimensionality Reduction","text":"<p>It reduces the number of input variables in a dataset to find a lower-dimensional representation that still preserves the salient relationships in the data.</p> <p>it is classified as <code>unsupervised learning</code> techniques sometimes, for some subgroups that don\u2019t require labeled data.</p> <ul> <li>ref: https://scikit-learn.org/stable/modules/unsupervised_reduction.html</li> </ul> <p>Many of the\u00a0Unsupervised learning\u00a0methods implement a\u00a0<code>transform</code>\u00a0method that can be used to reduce the dimensionality. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/#pca-principal-component-analysis","title":"PCA - Principal Component Analysis","text":"<p>PCA ([Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis))</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/#ica-independent-component-analysis","title":"ICA - Independent Component Analysis","text":"<p>i.e. Independent Component Analysis\u00a0(ICA)</p> <p>TBC\u2026.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/#t-sne-t-distributed-stochastic-neighbor-embedding","title":"T-SNE - T-distributed Stochastic Neighbor Embedding","text":"<p>T-SNE - T-distributed Stochastic Neighbor Embedding</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/#extended-reading","title":"Extended reading:","text":"<ul> <li>https://machinelearningmastery.com/dimensionality-reduction-for-machine-learning/</li> <li>https://machinelearningmastery.com/lstm-autoencoders/</li> <li>https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020?keywords=Machine+Learning:+A+Probabilistic+Perspective&amp;qid=1580679017&amp;sr=8-1&amp;linkCode=sl1&amp;tag=bnomial-20&amp;linkId=e80497d63d021ac1073a2ed9be092b02&amp;language=en_US&amp;ref_=as_li_ss_tl</li> <li>https://www.knime.com/blog/seven-techniques-for-data-dimensionality-reduction</li> <li>https://en.wikipedia.org/wiki/Principal_component_analysis</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/","title":"Ensemble Model","text":"<p>From scikit learn:</p> <p>The goal of\u00a0ensemble methods\u00a0is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/#concepts","title":"Concepts","text":"<ul> <li>Meta-model - a model we train to average the result of other models</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/#ref","title":"Ref","text":"<ul> <li>https://scikit-learn.org/stable/modules/ensemble.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/#ensemble-methods","title":"Ensemble methods","text":"<ul> <li>Averaging<ul> <li>Build models independently in training</li> <li>In prediction, average their outputs</li> <li>Pros :<ul> <li>Usually better than any single model inside this ensemble model, because the variance is reduced.</li> </ul> </li> <li>Example :<ul> <li>Bagging methods</li> <li>Forests of randomized trees</li> </ul> </li> </ul> </li> <li>Boosting<ul> <li>Build base models sequentially ???</li> <li>Pros :<ul> <li>Combine weak models to become powerful model</li> </ul> </li> </ul> </li> <li>Stacking<ul> <li>advantage of stacking is that it can benefit even from models that don't perform very well</li> </ul> </li> <li>Blending<ul> <li>require that models have a similar, good predictive power.</li> <li>Blending ensembles are a type of stacking where the meta-model is fit using predictions on a <code>holdout validation dataset</code> instead of <code>out-of-fold predictions</code>.</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/#bagging-methods","title":"Bagging Methods","text":"<ul> <li>Bagging methods - https://scikit-learn.org/stable/modules/ensemble.html#bagging</li> </ul> <p>How :</p> <p>It build several models in a black-box Each of the models uses a subset of training data At last, their predictions are aggregated to form a final decision</p> <p>It can be easily implemented with scikit like this:</p> <pre><code>from sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# Implement bagging with K-neighbors algorithm for example\nbagging = BaggingClassifier(\nKNeighborsClassifier(),\n# Max drawn from 50% of samples\nmax_samples=0.5,\n# Max drawn from 50% of features (i.e. fields, columns...)\nmax_features=0.5)\n</code></pre> <p>Pros :</p> <p>Reduce variance of the base models</p> <p>Work best with strong &amp; complex models</p> <p>Example :</p> <ul> <li>Single estimator versus bagging: bias-variance decomposition</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Ensemble%20Model%20b8a30bf59aec436d8f98d75b85fc31dc/#random-forest-classifier","title":"Random Forest Classifier","text":"<ul> <li>scikit-learn forest class - https://scikit-learn.org/stable/modules/ensemble.html#forest</li> </ul> <p>How :</p> <p>Similar to bagging method, the only difference is RandomForest is specialized for Decision Trees.</p> <p>A diverse set of classifiers is created by introducing randomness in the classifier construction.</p> <p>The final decision is averaged among predictions made by sub classifiers.</p> <p>Each decision trees is built from the samples drawn with replacement.</p> <p>Pros:</p> <p>Decrease the variance of the forest estimator</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Few-shot%20object%20detection%20037ae06da07844a98e28deaf2f0eea45/","title":"Few-shot object detection","text":"<ul> <li>Useful if some classes (aka Novel classes) have only few training data</li> <li>Enable the model to predict well for those classes</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/MLOps%207a4c5f02e2204d79831a55eae407a8ae/","title":"MLOps","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/MLOps%207a4c5f02e2204d79831a55eae407a8ae/#pipeline-on-aws-sagemaker","title":"Pipeline On AWS SageMaker","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Metrics%207184ad37e54548249e769ff105103c93/","title":"Metrics","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Metrics%207184ad37e54548249e769ff105103c93/#basic-terms","title":"Basic terms","text":"<ul> <li>Precision = True Positives / (True Positives + False Positives)<ul> <li>For all positive predicted, how many of them are correct.</li> <li>i.e. How many hit you missed</li> </ul> </li> <li>Recall = True Positives / (True Positives + False Negatives)<ul> <li>How many positive is predicted among ALL ground true positive</li> <li>i.e. How many REAL target you hit</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Metrics%207184ad37e54548249e769ff105103c93/#mean-absolute-errormae","title":"Mean Absolute Error(MAE) :","text":"<p>To put it in short, take the absolute value of below error</p> <pre><code>error=actual\u2212predicted\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Onnx%209d3fb9f89acb401db7cd8efba08f18cd/","title":"Onnx","text":"<p>https://github.com/onnx/tensorflow-onnx</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/","title":"Procedure of a New ML Project","text":"<p>We will start by asking ourselves a bunch of questions. After answering all of them, our project design will be almost ready.</p> <p>Also keep in mind, the following phrases are often iterating again and again. It\u2019s different with traditional system development.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#self-questioning","title":"Self-questioning","text":"<ul> <li>What KIND of data needed</li> <li>Where is data from</li> <li>How much data we need<ul> <li>if not enough, what data augmentation skills are needed</li> </ul> </li> <li>How will we LABEL the data (e.g. format, software\u2026etc)</li> <li>The model will be served as REAL TIME or BATCH PROCESS</li> <li>What metrics to evaluate the model<ul> <li>e.g. For f-score, what f value should be used, it depends on the project focus.</li> </ul> </li> <li>After launch, how often we will retrain the model</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#data-collection","title":"Data collection:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#explore-data-eda-exploratory-data-analysis","title":"Explore data - EDA (Exploratory Data Analysis)","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#browse-data","title":"Browse data :","text":"<p>check every column</p> <p>analyze data structure or characteristic</p> <p>distinguish between nominal &amp; continuous field</p> <p>of cause , mark the target/label field</p> <p>Compare data balance between Train / Test / Eval</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#what-data-is-helpful-and-what-is-not","title":"What data is helpful and what is not :","text":"<p>describe the observation of EACH columns / data fields from the analysis above</p> <p>decide what kind of preprocessing</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#feature-engineering","title":"Feature Engineering","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Procedure%20of%20a%20New%20ML%20Project%207fbd86aeb6a044c198b2905b91e009eb/#modeling","title":"Modeling","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Stratified%20sampling%206dc95f66839341799f75bdd0331f4dbe/","title":"Stratified sampling","text":"<p>It is a method of splitting a dataset into training and testing, to produce splits that contain a properly balanced set of samples</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Tuning%20%26%20optimization%207ccfaa0caf144e2bb97ebd727096d94d/","title":"Tuning &amp; optimization","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Tuning%20%26%20optimization%207ccfaa0caf144e2bb97ebd727096d94d/#hyperparameter-tuning","title":"Hyperparameter tuning","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Tuning%20%26%20optimization%207ccfaa0caf144e2bb97ebd727096d94d/#learning-rate","title":"Learning rate","text":"<ul> <li>Learning rate scheduler</li> <li>ReduceLROnPlateau</li> <li>Optuna  https://optuna.org/</li> <li>CosineAnnealingLR https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html</li> <li>How to choose Learning rate scheduler<ul> <li>https://neptune.ai/blog/how-to-choose-a-learning-rate-scheduler</li> <li>https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Tuning%20%26%20optimization%207ccfaa0caf144e2bb97ebd727096d94d/#optimizer-by-hugginface","title":"Optimizer by hugginface","text":"<p>https://huggingface.co/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.SchedulerType</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/","title":"PCA (Principal Component Analysis)","text":"<p>Comment: Developed in 1933\u2026\u2026, seriously ?</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/#implementation","title":"Implementation:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/#to-implement-with-scikit","title":"To implement with scikit:","text":"<pre><code>from sklearn.decomposition import PCA\n# only return 2 components\npca = PCA(n_components=2)\npca.fit(flat_x) # the input features shape should be: (samples, features)\nprint(pca.explained_variance_ratio_)\n# out: [0.58640506 0.06278766]\n# transform data for visualization later\nX_r = pca.transform(flat_x)\nX_r.shape\n# out: [samples, 2]\n# OR, you can fit and transform at the same time\nX_r = pca.fit_transform(flat_x)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/#what-is-pca","title":"What is PCA?","text":"<p>TLDR: </p> <p>A linear technique, to reduce the number of variables in a dataset, while preserving as much information as possible. (i.e. reduce the dimensionality of large data sets)</p> <ul> <li>NORMALLY, reducing num of var \u2192 accuracy drops BUT with PCA, it trades a little accuracy for simplicity.</li> <li>Benefit:<ul> <li>easier to explore &amp; visualize &amp; analyze dataset</li> <li>faster process for ML algorithm</li> </ul> </li> </ul> <p>Some visual illustration:</p> <ul> <li> <p>The original 3-dimensional data set. The red, blue, green arrows are the direction of the first, second, and third principal components, respectively. Image by the author.</p> <p></p> <p>[Image from here]</p> </li> <li> <p>After PCA, 3-dim is reduced into 2-dim:</p> <p></p> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/#how-does-pca-work","title":"How does PCA work?","text":"<ol> <li> <p>it understand the datasets first, by math, e.g. variance\u2026.etc</p> <ul> <li>Goal: understand how important it is for each variables (i.e. does it hold more info?)</li> <li>e.g. The greater the variance, the more the information. Vice versa.</li> <li>Reason: e.g. every data hold a variable of similar value(i.e. small variance), we cannot tell the difference of them by this variable.</li> </ul> </li> <li> <p>Summarizing data (i.e. reduce dim)</p> </li> </ol> <p>It is done by finding those Principal components.</p> <p>Let\u2019s use an example below to explain:</p> <p>When we look closer at our data, the maximum amount of variance lies not in the x-axis, not in the y-axis, but a diagonal line across. The second-largest variance would be a line 90 degrees that cuts through the first.</p> <p></p> <p>Then use the Principal Components as axis (PC1, PC2\u2026etc)</p> <p></p> <p>In this example, PC1 alone can capture the total variance of Height and Weight combined. Since PC1 has all the information, we can be very comfortable in removing PC2 and know that our new data is still representative of the original data. (Here is an ideal case)</p> <p>What about PC2, \u2026etc is not 0% of info?</p> <p>In a real-world situation, PC2 is not always 0% of info like in the above example.</p> <p>Performing a PCA will give us N number of principal components, where N is equal to the dimensionality of our original data. Then generally choose the least number of principal components that would explain the most amount of our original data.</p> <p>E.g. by looking at the variance of each Principal Component.  Let\u2019s say we want to keep 90% of info (i.e. 0.9 variance in total), we only need PC1 &amp; PC2 here.</p> <p></p> <p>What data is lost during PCA?</p> <p>Here is the illustration, the red line distance is the data we lost if we only keep this PC1</p> <p></p> <p>When reducing dimensions with PCA, it changes the distances of our data. (the distance between each others)</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/PCA%20%28Principal%20Component%20Analysis%29%206f6d005a20a14022a296f1433a97d9a0/#resources","title":"Resources","text":"<ul> <li>WIki - \u00a0Principal Component Analysis:</li> <li>Scikit-learn - https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca</li> <li>Simple explanation - https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d<ul> <li>a colab to illustrate it - https://colab.research.google.com/drive/1RC_XulRdrqpYRq4h8pRl22cfg9a-_FvS?usp=sharing</li> </ul> </li> <li>*Image Classification with Principal Component Analysis -* https://www.christopherlovell.co.uk/blog/2016/07/04/image-pca-deckchair.html<ul> <li>Q:  Why a monochrome image has color like that?</li> </ul> </li> <li>Use PCA in Image compression - https://towardsdatascience.com/image-compression-using-principal-component-analysis-pca-253f26740a9f</li> <li>Use PCA for MNIST dataset demonstration - https://builtin.com/data-science/tsne-python</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/","title":"T-SNE - T-distributed Stochastic Neighbor Embedding","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#what-is-t-sne","title":"What is T-SNE ?","text":"<ul> <li>Different with PCA, T-SNE is a non-linear technique.</li> <li>Different with PCA, T-SNE CANNOT be reused for new data.  It is only used for data exploration.<ul> <li>Because - \u201ct-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\u201d</li> </ul> </li> <li>Perplexity is a target number of neighbors for the central point. Normally 5~50 i.e. how large is the \u201ccircle\u201d</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#resources","title":"Resources","text":"<ul> <li>visualized t-SNE effect by different graph - https://distill.pub/2016/misread-tsne/</li> <li>A post explained in Chinese - https://mropengate.blogspot.com/2019/06/t-sne.html</li> <li>Guide on Medium - https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1<ul> <li>Too complicated explanation for people without statistic background(like me\u2026)</li> </ul> </li> <li>Another guide on medium - https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a<ul> <li>A lot more chart, visual guide, simplified equation, easier to understand</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#usage","title":"Usage:","text":"<ul> <li>Implement with scikit - https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE</li> <li>A guideline post about t-SNE - https://builtin.com/data-science/tsne-python</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#how-to-implement","title":"How to implement ?","text":"<p>Assumption:</p> <ul> <li>It is good to visualize high dimensional data, BUT we need to reduce the dimension by ourselves as a preprocess. So that the dimension is less than a small number like 50<ul> <li>e.g. apply PCA before inputting the data into TSNE</li> </ul> </li> <li>The number of sample cannot be too large, otherwise the memory/computation time explode</li> </ul> <p>Implementation:</p> <p>It can be as easy as:</p> <pre><code>from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2, learning_rate='auto',\ninit='random', perplexity=20).fit_transform(flat_x)\nX_embedded.shape\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#how-it-works","title":"How it works ?","text":"<p>From the guide on medium, its visualization is great:</p> <p>1st step :</p> <p>For each data points, generate a normal distribution with this point as the mean.</p> <p>And the Euclidean distance as the x-axis in distribution</p> <p>(Note that the distribution is not exactly like this in t-SNE, here is simplified for explanation)</p> <p></p> <p>The original distribution should be like this : (img also from medium post)</p> <p></p> <p>2nd Step :</p> <p>Create a new &amp; low-dimension space.</p> <p>Put all the data points randomly on this space</p> <p>Like in 1st step, for each point create a Student t-distribution\u00a0with a single degree of freedom</p> <p>3rd Step : Gradient descent</p> <p>To optimize the distribution from above (because we put the points randomly), </p> <p>Kullback-Leibler divergence is used, between the 2 distribution we created.</p> <p>This gradient help to \u201cmove\u201d the point to its suitable position</p> <p></p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Practical%20Skills%2043242c52e87648d088e6e64f06770c6e/Dimensionality%20Reduction%20f1c61e866dc34e0f9b681709742f50c6/T-SNE%20-%20T-distributed%20Stochastic%20Neighbor%20Embeddin%2049b3989aa2c44947af3754d1dcf1edb6/#usage-example","title":"Usage Example :","text":"<p>For visualizing &amp; exploring parameters inside a CNN network, that always seems a blackbox, because its parameters in hidden layers are too high-dimensional.</p> <p>But t-SNE can be used here to visualize the parameters.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Cost%20function%20%26%20back%20propagation%20%26%20gradient%2051bcfa8206734d7c85f58e4c630f3c1e/","title":"Cost function &amp; back propagation &amp; gradient","text":"<p>Ref: https://mlfromscratch.com/neural-networks-explained/#/ </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Cost%20function%20%26%20back%20propagation%20%26%20gradient%2051bcfa8206734d7c85f58e4c630f3c1e/#loss-cost-function","title":"Loss (cost) Function :","text":"<ul> <li>categorical cross-entropy loss function quantifies the difference between two probability distributions<ul> <li>good at multi-class classification</li> </ul> </li> <li>Binary cross-entropy is the\u00a0typical loss function\u00a0for the\u00a0binary classification</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Gradient%20Descent%20c3760f05e6c349b5bf3ce9a514b4fdce/","title":"Gradient Descent","text":"<p>Different types by the amount of samples used to calculate error: </p> <ul> <li>Mini-Batch Gradient Descent</li> <li>Stochastic Gradient Descent</li> <li>Batch Gradient Descent.</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Output%20layer%201e7f79463dec4a6c939fbd91446a4725/","title":"Output layer","text":"<p>Softmax uses the output score to produce a probability for each category.</p> <p>Sigmoid converts the output scores to a value between 0 and 1.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Semi-supervised%20learning%20225b63faf5d741449e85aa4d9eabb7f0/","title":"Semi-supervised learning","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Semi-supervised%20learning%20225b63faf5d741449e85aa4d9eabb7f0/#what-is-it","title":"What is it ?","text":"<p>It involves a small number of labeled examples and a large number of unlabeled examples. This situation is challenging for either supervised or unsupervised learning to be effective. And this learning algorithm sits between supervised and unsupervised learning.</p> <p>These algorithms can perform well when we have a very small amount of labeled points and a large amount of unlabeled points.</p> <p>Ref :</p> <ul> <li>Resources list - https://machinelearningmastery.com/what-is-semi-supervised-learning/</li> <li>Scikit learn - https://scikit-learn.org/stable/modules/semi_supervised.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Semi-supervised%20learning%20225b63faf5d741449e85aa4d9eabb7f0/#additional-resources","title":"Additional Resources","text":"<p>Some examples of good review papers on semi-supervised learning include:</p> <ul> <li>Semi-Supervised Learning Literature Survey, 2005.</li> <li>Introduction to Semi-Supervised Learning, 2009.</li> <li>An Overview of Deep Semi-Supervised Learning, 2020.</li> </ul> <p>The scikit-learn Python machine learning library provides a few graph-based semi-supervised learning algorithms that you can try:</p> <ul> <li>Section 1.14. Semi-Supervised, Scikit-Learn User Guide.</li> </ul> <p>The Wikipedia article may also provide some useful links for further reading:</p> <ul> <li>Semi-supervised learning, Wikipedia.</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/","title":"Weight &amp; Bias &amp; Activation &amp; Activation Function","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/#weight","title":"Weight","text":"<ul> <li>Each Neuron has some connection to others</li> <li>These connections = weight</li> <li>Type = double , e.g. 2.2, -1.2, 0.4</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/#activation","title":"Activation","text":"<ul> <li>Each Neuron has some activation</li> <li>Between 0 \u2264 a \u2264 1</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/#activation-function","title":"Activation Function","text":"<ul> <li>Example<ul> <li>Sigmoid</li> <li>ReLu</li> <li>Tanh</li> </ul> </li> <li>A function that map the value x to a value y, where 0 \u2264 y \u2264 1 i.e. output y is the activation</li> <li>Todo:<ul> <li>https://mlfromscratch.com/activation-functions-explained/#/</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/#bias","title":"Bias","text":"<ul> <li>just a value adding/subtracting after weight x activation</li> <li>I.e. <code>activation function</code> af(wa + wa + wa +\u2026 + wa + b) = next neuron activation</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/ML%20bcb6f277ce894a65b5e121f784d4a576/Theory%206a11941d908f496ba89144b55adc75b4/Weight%20%26%20Bias%20%26%20Activation%20%26%20Activation%20Function%2037905e2474f445dcb5aa49956b6215e7/#combining-everything","title":"Combining everything","text":"<ul> <li>2 input node \u2192 neuron A in next layer</li> <li>Let a = <code>activation</code>, w = <code>connection(Weight) to neuron A</code></li> <li>input node 1: a = 0.3, w = 1.1</li> <li>Input node 2: a = 1.0, w = 2.6</li> <li>The value in neuron A = 0.3 x 1.1 + 1.0 x 2.6 = 2.93</li> <li>Activation Function(af) then is wrapped to this new value 2.93</li> <li>I.e. af x 2.93 = a value between 0 to 1</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Setup%20VSCode%20remote%20development%20599e820d9ecd48baa6a0ee07ef714cb1/","title":"Setup VSCode remote development","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Setup%20VSCode%20remote%20development%20599e820d9ecd48baa6a0ee07ef714cb1/#background","title":"Background","text":"<p>A lot of reasons can lead us to use EC2 as development environment. We can use VSCode directly to manipulate the environment, git, code...etc on the EC2 remotely.</p> <p>REF:</p> <ul> <li>https://code.visualstudio.com/docs/remote/ssh-tutorial</li> <li>https://guyernest.medium.com/connecting-vs-code-to-develop-machine-learning-models-in-the-aws-cloud-aa1ebd16f890</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Setup%20VSCode%20remote%20development%20599e820d9ecd48baa6a0ee07ef714cb1/#prerequisite","title":"Prerequisite","text":"<p>Assume you already setup ssh config like this:</p> <pre><code>Host xxxx_jumpserver\nHostname &lt;jump ip...&gt;\nUser keith\nIdentityFile /Users/xxxxx/.ssh/id_rsa\nPort 23422\nForwardAgent Yes\nPubKeyAuthentication Yes\nUseKeychain Yes\nHost ec2sandbox\nHostName &lt;ec2 host / ip&gt;\nUser keith\nProxyCommand ssh -W %h:%p xxxx_jumpserver\n# Only needed if you want to do port forwarding (tunnel)\nLocalForward 8888 localhost:8888\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Setup%20VSCode%20remote%20development%20599e820d9ecd48baa6a0ee07ef714cb1/#procedure","title":"Procedure","text":"<ol> <li>Open the <code>.ssh/config</code><ul> <li>Optional You can install this vscode extension to edit ssh config more easier</li> </ul> </li> <li> <p>And added port forwarding, such as</p> <pre><code>Host ec2sandbox\n....\nLocalForward 8888 localhost:8888\n....\n</code></pre> </li> <li> <p>Install below official VSCode extension</p> <ul> <li>Remote - SSH</li> </ul> </li> <li>You will see a button in bottom left after installing the extension</li> <li>Click \"Connect to Host...\"</li> <li>The ssh config host which you setup before should appear here as an option. Just choose it and everything would be setup automatically.</li> <li>It is done now! You can click File -&gt; open folder to select a remote folder to open as workspace. Also the terminal here would be inside the EC2 already.</li> </ol>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Stress%20test%20d98198f915d84bf8a651f69a4f90b3a5/","title":"Stress test","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Stress%20test%20d98198f915d84bf8a651f69a4f90b3a5/#resource","title":"Resource","text":"<ul> <li>https://github.com/tsenart/vegeta</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Upload%20files%20with%20scp%200e03605db5c241fc872e0f9a321c8f0d/","title":"Upload files with scp","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/Upload%20files%20with%20scp%200e03605db5c241fc872e0f9a321c8f0d/#case-with-jump-host","title":"Case: with jump host","text":"<p>After you setup the tunnel (with command such as <code>ssh -L 1234:C:22 username@destination</code> )</p> <pre><code>scp -P 1234 -pr prj/ keith@localhost:/some/path\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/","title":"ssh/config Setup for jump server","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/#guide","title":"Guide","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/#intro","title":"Intro","text":"<p>Assuming your situation is that connecting to a jump server is a must, in order to access EC2.</p> <p>And now you don't want to connect to jump server manually every time, follow the procedure below to simplify the process.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/#setup-sshconfig","title":"Setup .ssh/config","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/#short-method","title":"Short Method:","text":"<pre><code>Host xxxx_jumpserver\nHostname &lt;jump server ip&gt;\nUser keith\nIdentityFile /Users/xxxx/.ssh/id_rsa\nPort 23422\nForwardAgent Yes\nPubKeyAuthentication Yes\nUseKeychain Yes\nHost ec2sandbox\nHostName &lt;ec2 ip/hostname&gt;\nUser keith\nProxyCommand ssh -W %h:%p xxxx_jumpserver\n# Only needed if you want to do port forwarding (tunnel)\nLocalForward 8888 localhost:8888\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Network%200189ece8aa2d4181b4ad2050fa195932/ssh%20config%20Setup%20for%20jump%20server%2059aaf042f57e4bf4b4fbc58790d70dd9/#long-method","title":"Long method:","text":"<p>Modify the <code>.ssh/config</code> as below</p> <pre><code>... some other lines...\n\nHost &lt;jump server ip address here&gt;\n        ForwardAgent yes\n        IdentityFile &lt;Full path to your key file&gt;\n        UseKeychain yes\n\n... some other lines...\n</code></pre> <p>Replace the <code>&lt;jump server ip address here&gt;</code>  as the jump server ip And replace the  as the FULL path to your key file <p>Then you are done ! Simply type below to connect to ec2</p> <pre><code>ssh  -J &lt;your username&gt;@&lt;jump server ip&gt;:23422 &lt;ec2 username&gt;@&lt;your ec2 host&gt;\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Datetime%2082e105004ea74f238025cc40a7c42e0b/","title":"Datetime","text":"<p>content</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Datetime%2082e105004ea74f238025cc40a7c42e0b/#resource","title":"Resource","text":"<ul> <li>Moment.js - https://momentjs.com/<ul> <li><code>npm i moment</code></li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Datetime%2082e105004ea74f238025cc40a7c42e0b/#timestamp","title":"Timestamp","text":"<pre><code>new Date().getTime().toString()\n// e.g. 1655194529501\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Datetime%2082e105004ea74f238025cc40a7c42e0b/#with-momentjs","title":"With moment.js","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Datetime%2082e105004ea74f238025cc40a7c42e0b/#output-format","title":"Output format","text":"<pre><code>moment().format('MMMM Do YYYY, h:mm:ss a'); // June 15th 2022, 3:03:20 pm\nmoment().format('dddd');                    // Wednesday\nmoment().format(\"MMM Do YY\");               // Jun 15th 22\nmoment().format('YYYY [escaped] YYYY');     // 2022 escaped 2022\nmoment().format();                          // 2022-06-15T15:03:28+09:00\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/ES%20Module%20VS%20CommonJS%20db418a52816d421ba2f22e5e753d62d5/","title":"ES Module VS CommonJS","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/ES%20Module%20VS%20CommonJS%20db418a52816d421ba2f22e5e753d62d5/#definition","title":"Definition :","text":"<p>To put it simply, ES Module is like <code>import xxx from xxxx</code>, while CommonJS is used by default in NodeJs, e.g. <code>const xxx = require(xxxx)</code>.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/","title":"File related","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#browser-download","title":"Browser Download","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#handle-blob-file-saving-in-browser","title":"Handle Blob/ file saving in browser","text":"<ul> <li>Use the lib - https://github.com/eligrey/FileSaver.js/</li> </ul> <pre><code>// Ref from https://medium.com/@fakiolinho/handle-blobs-requests-with-axios-the-right-way-bb905bdb1c04\n\n// In fileSave.js\nimport FileSaver from 'file-saver';\n\nexport default (fileData, fileName) =&gt; FileSaver.saveAs(fileData, fileName);\n\n// In &lt;any API functions&gt;.js\nimport fileSaver from './fileSaver';\nimport moment from \"moment\"\n\nexport async function downloadFile() {\n    const blobData = await someApiRequest()\n    const zip_file_name = moment().format(\"YYYYMMDD-hhmmss\") + \".zip\"\n    await fileSaver(ran_res, zip_file_name);\n}\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#file-createreaddelete","title":"File Create/Read/Delete","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#write-file","title":"Write file","text":"<pre><code>const fs = require('fs');\n\nconst content = 'Some content!'\n\ntry {\n  fs.writeFileSync('/Users/joe/test.txt', content)\n  // file written successfully\n} catch (err) {\n  console.error(err)\n}\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#read-file","title":"Read File","text":"<pre><code>const fs = require('fs')\n\ncss = fs.readFileSync(CSS_FILE_PATH, 'utf8')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#create-directory","title":"Create directory","text":"<pre><code>const fs = require('fs')\n\nfs.mkdirSync(\"xxxx/xxxx\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#remove-directory","title":"Remove directory","text":"<pre><code>const fs = require('fs')\n\nfs.rmSync(dir_path, { recursive: true })\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#create-symlink","title":"Create symlink","text":"<pre><code>const fs = require('fs')\n\nfs.symlinkSync(src_path, link_path, 'dir')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#unlink-symlink","title":"Unlink symlink","text":"<pre><code>const fs = require('fs')\n\nfs.unlinkSync(link_path)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#csv-file","title":"CSV file","text":"<p>Libs:</p> <ul> <li>Papaparse - https://www.papaparse.com/docs</li> <li>https://github.com/adaltas/node-csv</li> </ul> <p>Example of reading a CSV ( with Papaparse )</p> <pre><code>const Papa = require(\"papaparse\")\nconst fs = require(\"fs\")\n\n// file path\nlet path = \"/xxxx.csv\"\n\n// This is async way to read\nfunction asyncWay() {\n  let config = {\n    complete: function(results, file) {\n      console.log(\"Parsing complete:\");\n      console.log(results);\n      console.log(\"-------------------\")\n      console.log(file);\n    }\n  }\n  // Async here !!\n  Papa.parse(\n    fs.createReadStream(path),\n    config)\n}\n\n// This is sync way\nfunction syncWay(){\n  const csv_str = fs.readFileSync(path, \"utf8\")\n  const csvobj = Papa.parse(csv_str)  \n  console.log(csvobj)\n}\n\n//asyncWay()\nsyncWay()\n\n/* The result object looks like this\n{\n  data: [\n    [ 'col_a', 'col_b', 'col_c' ],\n    [\n      '\u682a\u5f0f\u4f1a\u793e\u30af\u30ea\u30fc\u30af\u30fb\u30a2\u30f3\u30c9\u30fb\u30ea\u30d0\u30fc\u793e',\n      'CREEK &amp; RIVER Co.,Ltd.',\n      '\u30ab\u30d6\u30b7\u30ad\u30ac\u30a4\u30b7\u30e3\u30af\u30ea\u30fc\u30af\u30a2\u30f3\u30c9\u30ea\u30d0\u30fc\u30b7\u30e3'\n    ],\n    [ '' ]\n  ],\n  errors: [],\n  meta: {\n    delimiter: ',',\n    linebreak: '\\r\\n',\n    aborted: false,\n    truncated: false,\n    cursor: 84\n  }\n}\n*/\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#path-manipulation","title":"Path Manipulation","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#check-path-exist","title":"Check path exist","text":"<pre><code>const fs = require('fs')\n\nif (!fs.existsSync(entry_server_path)) {\n    console.log(\"Not existed!\")\n}\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#path-string-concat","title":"Path string concat","text":"<pre><code>const path = require(\"path\")\n\npath.join(__dirname, \"../something.js\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#path-resolve","title":"Path resolve","text":"<pre><code>const path = require(\"path\")\n\npath.resolve(path.join(dist_path, '/entry-server.css'))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#get-the-most-right-file-name","title":"Get the most-right file name","text":"<pre><code>const path = require(\"path\")\n\nconsole.log(path.basename(\"/home/user/abc/def/tttt.txt\"))\n// tttt.txt\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#get-the-dir-name","title":"Get the dir name","text":"<pre><code>const path = require(\"path\")\n\nconsole.log(path.dirname(\"/home/user/abc/def/tttt.txt\"))\n// /home/user/abc/def\nconsole.log(path.dirname(\"../abc/def/tttt.txt\"))\n// ../abc/def\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/File%20related%205d36554e134e4d62abd93c012fe3719c/#zip-file","title":"Zip File","text":"<p>Using lib: https://www.archiverjs.com/docs/quickstart</p> <ul> <li>Zip some files: (In NodeJS)</li> </ul> <pre><code>&gt; DO NOT rely on `archive.finalize()`, you should resolve in the \u201cclose/end\u201d listener\n&gt;\n\n```jsx\n// require modules\nconst fs = require('fs');\nconst archiver = require('archiver');\n\n// create a file to stream archive data to.\nconst output = fs.createWriteStream(__dirname + '/example.zip');\nconst archive = archiver('zip', {\n  zlib: { level: 9 } // Sets the compression level.\n});\n\n// listen for all archive data to be written\n// 'close' event is fired only when a file descriptor is involved\noutput.on('close', function() {\n  console.log(archive.pointer() + ' total bytes');\n  console.log('archiver has been finalized and the output file descriptor has closed.');\n});\n\n// This event is fired when the data source is drained no matter what was the data source.\n// It is not part of this library but rather from the NodeJS Stream API.\n// @see: https://nodejs.org/api/stream.html#stream_event_end\noutput.on('end', function() {\n  console.log('Data has been drained');\n});\n\n// good practice to catch warnings (ie stat failures and other non-blocking errors)\narchive.on('warning', function(err) {\n  if (err.code === 'ENOENT') {\n    // log warning\n  } else {\n    // throw error\n    throw err;\n  }\n});\n\n// good practice to catch this error explicitly\narchive.on('error', function(err) {\n  throw err;\n});\n\n// pipe archive data to the file\narchive.pipe(output);\n\n// append a file from stream\nconst file1 = __dirname + '/file1.txt';\narchive.append(fs.createReadStream(file1), { name: 'file1.txt' });\n\n// append a file from string\narchive.append('string cheese!', { name: 'file2.txt' });\n\n// append a file from buffer\nconst buffer3 = Buffer.from('buff it!');\narchive.append(buffer3, { name: 'file3.txt' });\n\n// append a file\narchive.file('file1.txt', { name: 'file4.txt' });\n\n// append files from a sub-directory and naming it `new-subdir` within the archive\narchive.directory('subdir/', 'new-subdir');\n\n// append files from a sub-directory, putting its contents at the root of archive\narchive.directory('subdir/', false);\n\n// append files from a glob pattern\narchive.glob('file*.txt', {cwd:__dirname});\n\n// finalize the archive (ie we are done appending files but streams have to finish yet)\n// 'close', 'end' or 'finish' may be fired right after calling this method so register to them beforehand\narchive.finalize();\n```\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Markdown%20to%20Doc%20ad937648e8604f368dfbb1f257ccaa62/","title":"Markdown to Doc","text":"<p>https://docsify.js.org/#/quickstart</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/NPM%202bb019fde7924d3088c9d6350697902d/","title":"NPM","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/NPM%202bb019fde7924d3088c9d6350697902d/#workspace-ie-nested-package","title":"Workspace (i.e. nested package)","text":"<p>Ref: https://docs.npmjs.com/cli/v8/using-npm/workspaces </p> <p>You can have a file structure like this:</p> <pre><code>.\n+-- package.json\n`-- packages\n   +-- a\n   |   `-- package.json\n   `-- b\n       `-- package.json\n</code></pre> <p>So when you run <code>npm i</code> in root directory, it install packages like this:</p> <pre><code>+-- node_modules\n|  `-- packages/a -&gt; ../packages/a\n+-- package-lock.json\n+-- package.json\n`-- packages\n   +-- a\n   |   `-- package.json\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/NPM%202bb019fde7924d3088c9d6350697902d/#related-command","title":"Related command","text":"<pre><code># Init a workspace (NOTE: assume package.json already created)\nnpm init -w ./packages/a\n\n# Install all packages defined in package.json of each workspaces\nnpm i\n\n# Other command specific to a workspace, e.g. install, uninstall...etc\nnpm install abbrev -w a\n\n# Run a command defined in workspace a\nnpm run test --workspace=a\n\n# Run the commands defined in ALL workspaces\nnpm run test --workspaces\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/","title":"Resources","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/#useful-lib","title":"Useful lib","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/#visualization","title":"Visualization","text":"<ul> <li>List of graph visualization lib (especially for node relation) - https://neo4j.com/developer/tools-graph-visualization/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/#file-manipulation","title":"File Manipulation","text":"<ul> <li>Zip, file/folder creation all-in-one - https://stuk.github.io/jszip/documentation/examples.html</li> <li>Easier zip lib - https://github.com/archiverjs/node-archiver</li> <li>FileSaver.js  - https://github.com/eligrey/FileSaver.js/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/#security","title":"Security","text":"<ul> <li>Nanoid - to generate unique random str - https://github.com/ai/nanoid/<ul> <li>Cautions: v4 cannot support commonJS <code>require</code></li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Resources%207304e264dfcc426aa98410dcfeb8335f/#npm-related","title":"Npm related","text":"<ul> <li>Concurrently - https://github.com/open-cli-tools/concurrently<ul> <li>To startup multiple tasks together, e.g. 2 npm run serve</li> </ul> </li> <li>nodemon - https://www.npmjs.com/package/nodemon<ul> <li>To manually watch folders and rerun task (e.g. reload server).</li> <li>Normally embedded in framework like vue</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Simple%20http-server%20f75ef9aa40184f678aee1dbf59f36713/","title":"Simple http-server","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Simple%20http-server%20f75ef9aa40184f678aee1dbf59f36713/#init-a-npm-project","title":"Init a npm project","text":"<pre><code>npm init\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Simple%20http-server%20f75ef9aa40184f678aee1dbf59f36713/#install-http-server","title":"Install http-server","text":"<pre><code>npm install http-server\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Simple%20http-server%20f75ef9aa40184f678aee1dbf59f36713/#modify-packagejson","title":"Modify package.json","text":"<pre><code>\"scripts\": {\n\"dev\": \"http-server\"\n}\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Simple%20http-server%20f75ef9aa40184f678aee1dbf59f36713/#run-it","title":"Run it","text":"<pre><code>npm run dev\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/","title":"Vue","text":"<p>Dynamic component  <p>Pass all parent props to Child</p> <p>Auto-Global register Components</p> <p>Resource</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/Auto-Global%20register%20Components%201815bb34481248228889d26db3492d7b/","title":"Auto-Global register Components","text":"<p>For some cases, you want to globally / automatically import a bunch of components in a directory.</p> <p>You can follow the guide here</p> <p>Remember that\u00a0global registration must take place before the root Vue instance is created (with\u00a0<code>new Vue</code>).\u00a0Here\u2019s an example\u00a0of this pattern in a real project context. </p> <pre><code>import Vue from 'vue'\nimport upperFirst from 'lodash/upperFirst'\nimport camelCase from 'lodash/camelCase'\n\nconst requireComponent = require.context(\n  // The relative path of the components folder\n  './components',\n  // Whether or not to look in subfolders\n  false,\n  // The regular expression used to match base component filenames\n  /Base[A-Z]\\w+\\.(vue|js)$/\n)\n\nrequireComponent.keys().forEach(fileName =&gt; {\n  // Get component config\n  const componentConfig = requireComponent(fileName)\n\n  // Get PascalCase name of component\n  const componentName = upperFirst(\n    camelCase(\n      // Gets the file name regardless of folder depth\n      fileName\n        .split('/')\n        .pop()\n        .replace(/\\.\\w+$/, '')\n    )\n  )\n\n  // Register component globally\n  Vue.component(\n    componentName,\n    // Look for the component options on `.default`, which will\n    // exist if the component was exported with `export default`,\n    // otherwise fall back to module's root.\n    componentConfig.default || componentConfig\n  )\n})\n</code></pre> <p>Also, can refer to the real world example here</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/Dynamic%20component%20component%209659824ea81c484391f91b30aaebc3f4/","title":"Dynamic component  <p>For vue2:</p> <ul> <li>https://v2.vuejs.org/v2/guide/components.html#Dynamic-Components</li> <li>Explanation of <code>is</code> prop : https://v2.vuejs.org/v2/api/#is</li> </ul> <p>Usage like :</p> <pre><code>&lt;component v-bind:is=\"currentTabComponent\"&gt;&lt;/component&gt;\n</code></pre>","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/Pass%20all%20parent%20props%20to%20Child%2092eda2035dbf49da8d8c2368972ced2f/","title":"Pass all parent props to Child","text":"<p>It is common to make a wrapper component that just pass all parent props to child component.</p> <p>You can use the var <code>$props</code> or <code>$attrs</code></p> <pre><code>&lt;template&gt;\n  &lt;!-- $attrs will include all props from parent EXCEPT those defined in props in this component --&gt;\n    &lt;!-- i.e. contains all props, EXCEPT \"excludeme\" --&gt; \n  &lt;child-comp-1 v-bind=\"$attrs\" /&gt;\n    &lt;!-- $props only contains the field: \"excludeme\" --&gt;\n  &lt;child-comp-2 v-bind=\"$props\" /&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nimport ChildComp1 from \"xxxxxxx\"\nimport ChildComp2 from \"xxxxxxx\"\n\nexport default {\n  name: \"Wrapper\",\n  components: {\n        ChildComp1,\n        ChildComp2\n  },\n  props: {\n    excludeme: {\n      default: 1\n    }\n  },\n}\n&lt;/script&gt;\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/Resource%20cc13fba4dc664ca7a7a9c3380053f804/","title":"Resource","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Node%20JS%2032dcf134f76f43d3bb4c634535045bbf/Vue%20148813f7b2124d85b34081a304efe7e8/Resource%20cc13fba4dc664ca7a7a9c3380053f804/#component-framework","title":"Component Framework","text":"<ul> <li>Vuesax  - https://vuesax.com/docs/guide/</li> <li>Vuetify</li> <li>ElementUI</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Built-in%20small%20module%2054cee416223c4028bdb8ee0d39d4b7fc/","title":"Built-in small module","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Built-in%20small%20module%2054cee416223c4028bdb8ee0d39d4b7fc/#globals","title":"globals","text":"<ul> <li>returns a dictionary with all the global variables and symbols for the current program.</li> </ul> <pre><code>print(globals())\n\"\"\" for example : \n{'In': ['', 'globals()'],\n 'Out': {},\n '_': '',\n '__': '',\n '___': '',\n '__builtin__': &lt;module 'builtins' (built-in)&gt;,\n '__builtins__': &lt;module 'builtins' (built-in)&gt;,\n '__name__': '__main__',\n '_dh': ['/home/repl'],\n '_i': '',\n '_i1': 'globals()',\n '_ih': ['', 'globals()'],\n '_ii': '',\n '_iii': '',\n '_oh': {},\n '_sh': &lt;module 'IPython.core.shadowns' from '/usr/local/lib/python3.5/dist-packages/IPython/core/shadowns.py'&gt;,\n 'exit': &lt;IPython.core.autocall.ExitAutocall at 0x7fbc60ca6c50&gt;,\n 'get_ipython': &lt;bound method InteractiveShell.get_ipython of &lt;IPython.core.interactiveshell.InteractiveShell object at 0x7fbc6478ee48&gt;&gt;,\n 'quit': &lt;IPython.core.autocall.ExitAutocall at 0x7fbc60ca6c50&gt;}\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Built-in%20small%20module%2054cee416223c4028bdb8ee0d39d4b7fc/#_-all-_","title":"_  all _  :","text":"<ul> <li>_  all _  affects the <code>from &lt;module&gt; import *</code> ONLY<ul> <li>i.e. <code>from &lt;module&gt; import &lt;something&gt;</code> still works for all var</li> </ul> </li> </ul> <pre><code># foo.py\n__all__ = ['bar', 'baz']\nwaz = 5\nbar = 10\ndef baz(): return 'baz'\n# main.py\nfrom foo import *\nprint(bar)\nprint(baz)\n# The following will trigger an exception, as \"waz\" is not exported by the module\nprint(waz)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/","title":"Data Structure Skill","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/#dictionary","title":"Dictionary :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/#get-the-key-of-maxmin-val","title":"Get the key of max/min val :","text":"<pre><code>best_tree_size = min(mae_res, key=mae_res.get)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/#type-checking","title":"Type checking","text":"<p>Simple way:</p> <pre><code># But it would not pass for ndarray...etc\nisinstance(some_list, list)\n</code></pre> <p>Or use pandas helper: </p> <p>Danger! List-like can include some unexpected type. Avoid to use pandas helper </p> <pre><code>is_list_like([1, 2, 3])\nis_dict_like({1: 2})\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/#set","title":"Set","text":"<p>Merge list into a set</p> <pre><code>results_list = [[1,2,3], [1,2,4]]\nresults_union = set().union(*results_list)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Data%20Structure%20Skill%2001ad59a6dc2a47ea8f78473f0e93c410/#dict-of-list","title":"Dict of List","text":"<p>Declare a dict of list for adding value easily</p> <pre><code>from collections import defaultdict\nfrom typing import Dict, List\n# so, element is init with a list\ndict_list: Dict[str, List[str]] = defaultdict(list)\n# Then you can do like this without error\ndict_list[\"not_existing_key\"].append(1000)\n#  {'not_existing_key': [1000]}\n</code></pre> <p>Convert Dict of List \u2192 List of Dict (not same length)</p> <pre><code>import numpy as np\nimport itertools\nDL = {\n\"10_ele\": np.ones(10),\n\"5_ele\": np.ones(5),\n\"2_ele\": np.ones(2),\n}\n# {'10_ele': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n# '5_ele': array([1., 1., 1., 1., 1.]),\n# '2_ele': array([1., 1.])}\nv = [dict(zip(DL,t)) for t in itertools.zip_longest(*DL.values(), fillvalue=\"\")]\n\"\"\"output\n[{'10_ele': 1.0, '5_ele': 1.0, '2_ele': 1.0},\n {'10_ele': 1.0, '5_ele': 1.0, '2_ele': 1.0},\n {'10_ele': 1.0, '5_ele': 1.0, '2_ele': ''},\n {'10_ele': 1.0, '5_ele': 1.0, '2_ele': ''},\n {'10_ele': 1.0, '5_ele': 1.0, '2_ele': ''},\n {'10_ele': 1.0, '5_ele': '', '2_ele': ''},\n {'10_ele': 1.0, '5_ele': '', '2_ele': ''},\n {'10_ele': 1.0, '5_ele': '', '2_ele': ''},\n {'10_ele': 1.0, '5_ele': '', '2_ele': ''},\n {'10_ele': 1.0, '5_ele': '', '2_ele': ''}]\n\"\"\"\n</code></pre> <p>Find the largest len of nested list</p> <pre><code>DL_len = max([len(v) for v in DL.values()])\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/","title":"Date Time","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#timezone","title":"Timezone","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#format-datetime-to-iso-8601","title":"Format datetime to ISO 8601","text":"<pre><code>from datetime import datetime\nfrom dateutil import tz\n# Create time without TZ, you cannot print out the \"offset\", i.e. +09:00\nnew_dt = datetime.now()\nnew_dt.isoformat(timespec=\"seconds\")\n# '2022-06-03T15:56:27'\nTZJP = tz.gettz('Asia/Tokyo')\nnew_dt = datetime.now(tz=TZJP)\nnew_dt.isoformat(timespec=\"seconds\")\n# '2022-06-03T15:57:00+09:00'\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#revise-the-tz-info-to-naive-time","title":"Revise the TZ info to naive time","text":"<pre><code>from dateutil import tz\nfrom datetime import datetime\n# tzinfo instance\nTZJP = tz.gettz('Asia/Tokyo')\n# Convert datetime var to string, in ISO 8601 format with timezone value\ndef convert_datetime_to_iso_8601(dt: datetime) -&gt; str:\nnew_dt = dt\n# If no timezone provided, set it to JPT\nif dt.tzinfo is None:\nnew_dt = new_dt.replace(tzinfo=TZJP)\n# So it return datetime in this ISO format: 2022-05-18T13:24:28+09:00\nreturn new_dt.isoformat(timespec=\"seconds\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#formating","title":"Formating","text":"<pre><code>from dateutil import tz\nfrom datetime import datetime\nTZJP = tz.gettz('Asia/Tokyo')\na = datetime.now(tz=TZJP)\nprint(a.isoformat(timespec=\"seconds\"))\n# 2022-05-18T13:24:28+09:00\nprint(a.strftime('%Y-%m-%dT%H:%M:%S%z'))\n# 2022-05-18T13:24:28+0900\n</code></pre> <ul> <li><code>strftime</code> cheatsheet - https://strftime.org/</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#elapse-time-benchmark","title":"Elapse Time / Benchmark","text":"<p>https://realpython.com/python-timer/ </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#by-native-time-package","title":"By native <code>time</code> package","text":"<pre><code>import time\ntic = time.perf_counter()\ntutorial = feed.get_article(0)\ntoc = time.perf_counter()\nprint(f\"Downloaded the tutorial in {toc - tic:0.4f} seconds\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Date%20Time%20d9afbf379d634280acb6096f0fc48851/#customized-contextmanager","title":"Customized ContextManager","text":"<pre><code>from time import time\n# Use as context manager to report time easily\n# Usage:\n#       with TimerManager(\"Example long waiting task\", callback=logger.warning):\n#           sleep(5)\nclass TimerManager(object):\ndef __init__(self, description, *args, callback=None):\nself.description = description\nif callback:\nself.callback = callback\ndef __enter__(self):\nself.start = time()\nself._print_out(f\"{self.description} start now...\")\ndef __exit__(self, type, value, traceback):\nself.end = time()\nself._print_out(f\"{self.description} finished, time elapsed: {self.end - self.start:0.4f}s\")\ndef _print_out(self, content):\nif callable(self.callback):\nself.callback(content)\nelse:\nprint(content)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/","title":"Env - anaconda/pip/poetry","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#pip","title":"Pip","text":"<p>Init an env</p> <pre><code>python3 -m venv ./.venv\n</code></pre> <p>Export to requirements.txt</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> <p>Install from requirements.txt</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#anaconda","title":"Anaconda","text":"<p>Create env in current folder</p> <pre><code>conda create --prefix ./envs\n## Or\nconda create -p ./envs\n## create with a specific python version\nconda create -p .venv python=3.8\n</code></pre> <p>Activate env</p> <pre><code>conda activate ./envs\n</code></pre> <p>Issue: Env name is too long</p> <pre><code>conda config --set env_prompt '({name}) '\n</code></pre> <p>Cancel \u201cauto activate env\u201d</p> <pre><code>conda config --set auto_activate_base false\n</code></pre> <p>List env set</p> <pre><code>conda info --envs\n</code></pre> <p>Share Environment (output to environment.yml)</p> <pre><code>conda env export &gt; environment.yml\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#install-3rd-party-package-in-anaconda","title":"Install 3rd party package in Anaconda","text":"<p>https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-pkgs.html#installing-non-conda-packages</p> <ol> <li>Ensure there is pip installed INSIDE your current anaconda env by running <code>conda list | grep pip</code></li> <li>Run pip like usual to install the package you want</li> </ol> <p>Below method is deprecated</p> <ol> <li>~Run conda create -n venv_name and conda activate venv_name, where venv_name is the name of your virtual environment.~</li> <li>~Run conda install pip. This will install pip to your venv directory.~</li> <li>~Find your anaconda directory, and find the actual venv folder. It should be somewhere like <code>/anaconda/envs/venv_name/</code>~</li> <li>~Install new packages by doing /anaconda/envs/venv_name/bin/pip install package_name.~</li> </ol>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#poetry","title":"Poetry","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#create-new-env","title":"Create new env","text":"<pre><code>poetry shell\n# Or reactivate current venv by source xxxx\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Env%20-%20anaconda%20pip%20poetry%2032cdbfb8b48f4a0f8c100fa1514db400/#install-defined-dependencies","title":"Install defined dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Image%20lib%2083ad45bff029446595a7edc9290efe1f/","title":"Image lib","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Image%20lib%2083ad45bff029446595a7edc9290efe1f/#skimage","title":"Skimage","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Image%20lib%2083ad45bff029446595a7edc9290efe1f/#must-read-inout-datatypes","title":"Must-read - in/out datatypes","text":"<p>Skimage accepts various Input datatypes (or range, e.g. -1 to 1 or 0 to 255)</p> <ul> <li>https://scikit-image.org/docs/dev/user_guide/data_types.html</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Image%20lib%2083ad45bff029446595a7edc9290efe1f/#convert-output-to-other-ranges","title":"Convert output to other ranges:","text":"<pre><code>from skimage.util import img_as_float\nimage = np.arange(0, 50, 10, dtype=np.uint8)\nprint(image.astype(float)) # These float values are out of range.\nprint(img_as_float(image))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Image%20lib%2083ad45bff029446595a7edc9290efe1f/#draw-an-image-with-skimage","title":"Draw an image with skimage:","text":"<pre><code>from skimage import io\np = \"/ssss/xxxx.png\"\nimage = io.imshow(p)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Import%20files%20-%20Best%20practice%2042b1a5a69fe14275add56721ca3ff95c/","title":"Import files - Best practice","text":"<p>It is unbelievably annoying to do <code>import</code> in a python script. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Import%20files%20-%20Best%20practice%2042b1a5a69fe14275add56721ca3ff95c/#resource","title":"Resource","text":"<ul> <li>https://blog.finxter.com/python-how-to-import-modules-from-another-folder</li> <li>https://towardsdatascience.com/understanding-python-imports-init-py-and-pythonpath-once-and-for-all-4c5249ab6355</li> <li>https://peps.python.org/pep-0328/#rationale-for-relative-imports</li> <li>https://stackoverflow.com/questions/4209641/absolute-vs-explicit-relative-import-of-python-module</li> <li>https://stackoverflow.com/questions/16981921/relative-imports-in-python-3</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Import%20files%20-%20Best%20practice%2042b1a5a69fe14275add56721ca3ff95c/#conclusion","title":"Conclusion","text":"<ul> <li>Use absolute import if possible, unless the path is too long.</li> <li>The entry point(which script will be executed) is critical</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Import%20files%20-%20Best%20practice%2042b1a5a69fe14275add56721ca3ff95c/#example","title":"Example:","text":"<p>Let\u2019s say we have below structure:</p> <p></p> <p>We want to access CONST_A ,which is defined in <code>utils/module_share.py</code>, from <code>package_a/module_a.py</code>. However, we will execute <code>project_level.py</code>, which import <code>module_a.py</code>.</p> <p>The main code is as below (note how the CONST_A is imported in different files)</p> <pre><code># src_project/project_level.py\nfrom package_a.module_a import CONST_A\nprint(\" CONST_A is: \", CONST_A)\n# src_project/package_a/module_a.py\nfrom utils.module_share import CONST_A\n# src_project/utils/module_share.py\nCONST_A = \"123123\"\n</code></pre> <p>However, if we import <code>project_level.py</code> in <code>root_level.py</code>, the import path everywhere will just fail.</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Logger%20setup%20eb5f7fe8152840e0ae3d06af63802eb6/","title":"Logger setup","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Logger%20setup%20eb5f7fe8152840e0ae3d06af63802eb6/#logger-common-setup","title":"Logger common setup","text":"<pre><code>import logging\nimport os\ndef get_logger(name):\nlogger = logging.getLogger(name)\nlogger.propagate = False\n# terminal output\nconsole = logging.StreamHandler()\nsimple_formatter = logging.Formatter('%(name)s - %(message)s')\nconsole.setFormatter(simple_formatter)\nlogger.addHandler(console)\n# full logging syntax in the log file\nlog_file_path = os.getenv(\"LOG_FILE\")\nif log_file_path:\nextended_formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\nfh = logging.FileHandler(log_file_path)\nfh.setFormatter(extended_formatter)\nlogger.addHandler(fh)\n# it defines which level to output\nlog_level = os.getenv(\"LOG_LEVEL\")\nif log_level is None:\nraise Exception(\"Global Env: LOG_LEVEL is not set, did you forget to set env file or env variables?\")\nlogger.setLevel(log_level)\nreturn logger\n# can be used like this:\nlogger = get_logger(__name__)\nlogger.info(\"hi\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Logger%20setup%20eb5f7fe8152840e0ae3d06af63802eb6/#coloring-logging","title":"Coloring logging","text":"<pre><code>import logging\nimport os\n# Define a custom formatter\nclass CustomFormatter(logging.Formatter):\nCYAN = '\\033[96m'\nPINK = '\\033[95m'\nPURPLE = '\\033[94m'\nYELLOW = '\\033[93m'\nGREEN = '\\033[92m'\nRED = '\\033[91m'\nGREY = '\\033[90m'\nRESET = '\\033[0m'\nBOLD = '\\033[1m'\nUNDERLINE = '\\033[4m'\n# format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\nformat = \"%(name)s - %(message)s\"\nFORMATS = {\nlogging.DEBUG: GREY + format + RESET,\nlogging.INFO: RESET + format + RESET,\nlogging.WARNING: YELLOW + format + RESET,\nlogging.ERROR: RED + format + RESET,\nlogging.CRITICAL: PINK + format + RESET\n}\ndef format(self, record):\nlog_fmt = self.FORMATS.get(record.levelno)\nformatter = logging.Formatter(log_fmt)\nreturn formatter.format(record)\n# When register logger, use the formatter:\nconsole = logging.StreamHandler()\nconsole.setFormatter(CustomFormatter())\nlogger.addHandler(console)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/","title":"Numpy","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#generation","title":"Generation:","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#create-an-array-of-given-shape","title":"Create an array of given shape","text":"<pre><code># All zeros in a given shape\nZ = np.zeros((m, n_H, n_W, n_C))\n# All ones in a given shape\nZ = np.ones((m, n_H, n_W, n_C))\n# Random values in a given shape.\nx = np.random.rand(3,2)\n# Return a sample (or samples) from the \u201cstandard normal\u201d distribution.\nx = np.random.randn(4, 3, 3, 2)\n# Fill with given value\nx = np.full((2, 2), 10)\n# array([[10, 10],\n#       [10, 10]])\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#permutation-randomly-generate-a-sequence","title":"Permutation - randomly generate a sequence","text":"<p>https://numpy.org/doc/stable/reference/random/generated/numpy.random.permutation.html?</p> <pre><code>np.random.permutation(10)\n# output : array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6]) # random\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#remove-axes-that-the-length-is-1-squeeze","title":"Remove axes that the length is 1 - Squeeze","text":"<p>https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html</p> <pre><code>import numpy as np\na = np.array([[\n[[1],[2],[1]],\n[[2],[3],[4]],\n[[1],[2],[1]]\n]])\nprint(f\"shape: {a.shape}\")\nprint(a.squeeze())\nprint(f\"shape: {a.squeeze().shape}\")\n\"\"\"output:\nshape: (1, 3, 3, 1)\n[[1 2 1]\n [2 3 4]\n [1 2 1]]\nshape: (3, 3)\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#padding","title":"Padding:","text":"<p>Use np.pad. </p> <p>e.g. Pad a list of colored images with zeros. List of images, a of shape\u00a0(100,32,32,3)\u00a0with\u00a0<code>pad = 2</code> \u00a0for the 2nd &amp; 3rd dimension,\u00a0you would do:</p> <pre><code># shape of a : (imgs, H, W, RGB channel)\na= np.pad(\na, \n((0,0), (2,2), (2,2), (0,0)), \nmode='constant', constant_values= (0,0)\n)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#practical-use-case","title":"Practical use case","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#create-a-mask-that-set-true-to-the-max-element","title":"Create a mask that set True to the max element:","text":"<p>e.g. \\(X = \\begin{bmatrix}1 &amp;&amp; 3 \\\\4 &amp;&amp; 2\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}0 &amp;&amp; 0 \\\\1 &amp;&amp; 0\\end{bmatrix}\\)</p> <pre><code>M = (X == np.max(X))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Numpy%207b186b9ab1df444d889a99c71c4ee79c/#distribute-a-value-averagely-into-a-new-array","title":"Distribute a value averagely into a new array:","text":"<p>e.g. \\(dZ = 1 \\quad \\rightarrow\u00a0 \\quad dZ =\\begin{bmatrix}1/4 &amp;&amp; 1/4 \\\\1/4 &amp;&amp; 1/4\\end{bmatrix}\\)</p> <pre><code>shape = (2,2)\ndZ = np.full(shape, dZ / np.prod(shape))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/ORM%20related%20bf844584f7794585bf3bd7587b08f3a0/","title":"ORM related","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/ORM%20related%20bf844584f7794585bf3bd7587b08f3a0/#sqlalchemy","title":"SQLAlchemy","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/ORM%20related%20bf844584f7794585bf3bd7587b08f3a0/#inspect-output-raw-sql","title":"Inspect output raw SQL","text":"<pre><code># No .all() at the end\nsql = db.query(TableA)\nprint(sql)\n# Or \nprint(sql.statement)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/ORM%20related%20bf844584f7794585bf3bd7587b08f3a0/#join-table-in-filter","title":"Join table in filter","text":"<pre><code>db.query(TableA)\n.join(TableA.related_table_b_field_col)\n.filter(\nTableA.col1 == something,\nTableB.status == \"active\"\n)\n.all()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/ORM%20related%20bf844584f7794585bf3bd7587b08f3a0/#use-same-subquery-in-left-right","title":"Use same subquery in LEFT &amp; Right","text":"<ul> <li>Let\u2019s say we want to join 2 identical subquery</li> </ul> <pre><code>subquery = (\ndb.query(TableA, TableB.col1, TableC.col11111)\n.select_from(TableA)\n.join(TableA.related_B_col1)\n.join(TableA.related_C_col1)\n.filter(\nTableB.status == \"active\",\nTableC.status == \"disabled\",\nTableA.id.not_in(some_list)\n)\n.subquery()\n)\n# Set alias for main and join query\nmain_alias = aliased(subquery)\ncompare_alias = aliased(subquery)\n# Build main query at last\nmain_q = (\n# Only select fields from LEFT query\ndb.query(main_alias)\n.select_from(main_alias)\n.join(\ncompare_alias,\n# Here is the manual on clause if necessary\nand_(\nmain_alias.c.col1 == compare_alias.c.col2,\nmain_alias.c.created_at &lt; compare_alias.c.created_at\n),\n# It is a LEFT JOIN\nisouter=True\n)\n# i.e. WHERE compare_alias.id is null\n.filter(compare_alias.c.id.is_(None))\n)\n# Execute SQL, empty array if not found\nraw_res = main_q.all()\n# Convert each rows: sqlalchemy.engine.row.Row into dict\nmain_res = [v._asdict() for v in raw_res]\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/","title":"Pandas","text":"<p>Google Colaboratory</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#data-exploration","title":"Data Exploration","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#paint-cell-by-value","title":"Paint cell by value:","text":"<pre><code>df.head(10).style.background_gradient()\n</code></pre> <p>It will look like : </p> <p></p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#dataframe-info-eg-memory","title":"DataFrame info (e.g. memory) :","text":"<p>Such as memory used</p> <pre><code>all_data.info()\n\"\"\"e.g. \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 12970 entries, 0 to 4276\nData columns (total 14 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   PassengerId   12970 non-null  object \n 1   HomePlanet    12682 non-null  object\n.......\ndtypes: float64(6), object(8)\nmemory usage: 1.5+ MB\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#display-null-value-count","title":"Display null value count :","text":"<pre><code>all_data.isnull().sum()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#display-some-statistic-summary-of-data","title":"Display some statistic summary of data :","text":"<pre><code>all_data.describe()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#display-unique-values-in-a-column","title":"Display unique values in a column :","text":"<pre><code>all_data['HomePlanet'].unique()\n# array(['Europa', 'Earth', 'Mars', nan], dtype=object)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#display-counting-of-unique-values-in-a-column","title":"Display counting of unique values in a column :","text":"<pre><code>all_data['HomePlanet'].value_counts()\n\"\"\"e.g.\nEarth     6865\nEuropa    3133\nMars      2684\nName: HomePlanet, dtype: int64\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#the-number-of-unique-values-in-each-column","title":"The number of unique values in each column :","text":"<pre><code>all_data.nunique(\n# dropna=False\n)\n\"\"\"e.g.\nPassengerId     12970\nHomePlanet          3\nCryoSleep           2\nCabin            9825\nDestination         3\nAge                80\nVIP                 2\nRoomService      1578\nFoodCourt        1953\nShoppingMall     1367\nSpa              1679\nVRDeck           1642\nName            12629\nTransported         2\ndtype: int64\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#display-some-unique-values-in-each-column","title":"Display some unique values in each column :","text":"<p>Note: only show the columns with &lt; 20 unique values</p> <pre><code>all_uniq = { i: all_data[i].unique() for i in all_data.columns if len(all_data[i].unique()) &lt; 20 }\nall_uniq\n</code></pre> <p>Helper func - display multiple DataFrame together</p> <pre><code>from IPython.core.display import HTML\ndef multi_table_vertical(lt) :\nreturn HTML(\" &lt;hr&gt; \".join(table._repr_html_() for table in lt))\n# Usage:\nmulti_table_vertical([all_st, train_st, test_st])\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#data-transform","title":"Data Transform","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#to-list-of-dict","title":"To list of dict :","text":"<pre><code>df.to_dict('records')\n\"\"\"\n[{'customer': 1L, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2L, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3L, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n\"\"\"\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Pandas%20803fe6e421a14a079fcc199c223756a6/#concat-multiple-pandas-df","title":"Concat multiple pandas DF :","text":"<pre><code>all_data = pd.concat([train, test], axis=0)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Parse%20Arguments%20b57395e933ad4ebbaf9e745f1a1553c5/","title":"Parse Arguments","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Parse%20Arguments%20b57395e933ad4ebbaf9e745f1a1553c5/#via-argparse","title":"Via argparse","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Parse%20Arguments%20b57395e933ad4ebbaf9e745f1a1553c5/#prepare-for-argument-input","title":"Prepare for argument input","text":"<pre><code>import argparse\ndef main(args):\n# do something here\npass\nif __name__ == \"__main__\":\nparser = argparse.ArgumentParser()\nparser.add_argument(\n\"--model_name\", type=str, default=\"bert-base-multilingual-cased\"\n)\nparser.add_argument(\"--batch_size\", type=int, default=1)\nparser.add_argument(\"--epochs\", type=int, default=20)\nparser.add_argument(\"--learning_rate\", type=float, default=0.00001)\nargs, _ = parser.parse_known_args()\nprint(f\"model_name: {args.model_name}\")\nprint(f\"batch_size: {args.batch_size}\")\nprint(f\"learning_rate: {args.learning_rate}\")\nprint(f\"epochs: {args.epochs}\")\nmain(args)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Parse%20Arguments%20b57395e933ad4ebbaf9e745f1a1553c5/#via-click","title":"Via <code>click</code>","text":"<p>https://click.palletsprojects.com/en/8.1.x/</p> <p>example:</p> <pre><code>import click\n@click.command()\n@click.option('--count', default=1, help='Number of greetings.')\n@click.option('--name', prompt='Your name',\nhelp='The person to greet.')\ndef hello(count, name):\n\"\"\"Simple program that greets NAME for a total of COUNT times.\"\"\"\nfor x in range(count):\nclick.echo(f\"Hello {name}!\")\nif __name__ == '__main__':\nhello()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/","title":"Path / files related","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#search-within-folder","title":"Search within folder :","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#via-glob","title":"Via <code>glob</code>","text":"<pre><code>import glob\nimport os\ncharacter_json_paths = glob.glob(os.path.join(DIR_JSON, \"*.json\"))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#via-simple-oslistdir","title":"Via simple <code>os.listdir</code>","text":"<pre><code>from pathlib import Path\nfrom typing import List\nimport os\ndef search_files_of_type(\npath: str, suffix: str = \".json\",\nexclude_files_name: List[str] = [],\nexclude_ignore_suffix: bool = True\n) -&gt; list:\n# Remove suffix if True\nif exclude_ignore_suffix and len(exclude_files_name) &gt; 0:\nnew_excl = [Path(v).with_suffix(\"\") for v in exclude_files_name]\n# listdir ONLY list the direct level, NOT all nested path\nreturn [v for v in os.listdir(path) if v.endswith(suffix) and Path(v).with_suffix(\"\") not in new_excl]\nelse:\nreturn [v for v in os.listdir(path) if v.endswith(suffix) and v not in exclude_files_name]\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#path-manipulation","title":"Path manipulation","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#file-suffixie-extension","title":"File suffix(i.e. extension)","text":"<pre><code>v = \"/ada/wdwdw.txt\"\nfilename_no_suffix = str(Path(v).with_suffix(\"\"))\n# /ada/wdwdw\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#joinresolve-a-path","title":"join/resolve a path :","text":"<pre><code>import os\nos.path.join(\"/somewhere/folder\", \"train.npz\")\n# or\nfrom pathlib import Path\nPath(\"/somewhere\").join(\"train.npz\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#calculate-size-of-every-files-in-a-dir-not-subfolders","title":"calculate size of every files in a dir (not subfolders) :","text":"<pre><code>import os\nmodel_path = \"/somewhere\"\nsum(os.path.getsize(f.path) for f in os.scandir(model_path) if f.is_file())\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#temp-folderfile","title":"Temp folder/file","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#via-tempfile","title":"Via <code>tempfile</code> :","text":"<pre><code>with tempfile.TemporaryFile()as fp:\nfp.write(b'Hello world!')\nfp.seek(0)\nfp.read()\n## file is now closed and removed\nwith tempfile.TemporaryDirectory() as path:\nimages_from_path = convert_from_path(\n\"/home/user/example.pdf\", \noutput_folder=path)\n# do something....\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#general-text-file","title":"General - Text File","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#read-file","title":"Read file:","text":"<p>Get all lines into a <code>list</code></p> <pre><code>pdf_list = []\nwith open(\"./pdf_list.log\") as f:\npdf_list = f.readlines()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/#sub-category","title":"Sub category :","text":"<p>PDF skills</p> <p>npz file</p> <p>json file</p> <p>CSV file</p> <p>H5PY</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/","title":"Resources","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/#framework","title":"Framework","text":"<ul> <li>Easy-to-build ML demo site<ul> <li>Gradio</li> <li>Streamlit</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/#useful-small-lib","title":"Useful small lib","text":"<ul> <li>invoke : https://www.pyinvoke.org/</li> <li>Generate fake data, helper<ul> <li>https://faker.readthedocs.io/en/master/providers/faker.providers.ssn.html</li> </ul> </li> <li>Timezone helper, can init timezone info in 1 line<ul> <li>https://dateutil.readthedocs.io/en/stable/</li> </ul> </li> <li>joblib<ul> <li>https://joblib.readthedocs.io/en/latest/index.html</li> <li>Optimize to do pipeline operation</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/#image-processing","title":"Image processing","text":"<ul> <li>skimage (scikit-image)<ul> <li>https://scikit-image.org/</li> </ul> </li> <li>openCV</li> <li>Pillow</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/#visualization-lib","title":"Visualization Lib","text":"<ul> <li>matplotlib<ul> <li>like a base for every other lib</li> <li>enough for simple 2d graph</li> </ul> </li> <li>seaborn<ul> <li>good for heatmap</li> </ul> </li> <li>plotly<ul> <li>great for 3d plot</li> </ul> </li> <li>Graphviz<ul> <li>https://www.graphviz.org/</li> <li>Good for plotting Network style nodes and graph</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Resources%20c9ee2be4e2e54e5cba9af758892789d3/#doc-guide","title":"Doc / Guide","text":"<ul> <li>A better doc than official python doc<ul> <li>https://book.pythontips.com/en/latest/index.html</li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/","title":"Scikit-learn","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#sklearndatasets","title":"sklearn.datasets","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#make_blobs","title":"make_blobs","text":"<p>Generate data</p> <pre><code># Generate data sets\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=1000, centers=3, n_features=3, random_state=0, cluster_std=[1,2,3], center_box=(10,65))\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#sklearnpreprocessing","title":"sklearn.preprocessing","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#standardscaler","title":"StandardScaler","text":"<p>standardize data</p> <pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_blobs\n# Create an array with 3 clusters in 3-dimensions\nX, y = make_blobs(n_samples=1000, centers=3, n_features=3, random_state=0, cluster_std=[1,2,3], center_box=(10,65))\n# Standardize the data\nX = StandardScaler().fit_transform(X)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#sklearndecomposition","title":"sklearn.decomposition","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Scikit-learn%20bc099d81201d4284bc061691b58ecf17/#pca","title":"PCA","text":"<p>Apply Principal component analysis (i.e. dim reduce)</p> <pre><code>from sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\n# NOTE!!! Assume you already did standardization on dataset\n\"\"\"\ndf[col_name] is a dataframe like this:\nx0  x1  x2\n-0.366353   1.022466    1.166899\n-1.179214   1.318905    1.047407\n\"\"\"\n# Perform PCA (w/o limits on n_components)\npca = PCA()\n_ = pca.fit_transform(df[col_name])\nPC_components = np.arange(pca.n_components_) + 1\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Singleton%20example%205717e07575f3446fb29b68aaa35cb21f/","title":"Singleton example","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Singleton%20example%205717e07575f3446fb29b68aaa35cb21f/#use-metaclass","title":"Use <code>metaclass</code>","text":"<pre><code>class Singleton(type):\n_instances = {}\ndef __call__(cls, *args, **kwargs):\nprint(\"singleton called\")\nif cls not in cls._instances:\nprint(\"singleton cls not in _instances\")\ncls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)\nreturn cls._instances[cls]\nclass Inference(metaclass=Singleton):\ndef __init__(self):\nprint(\"inference init called\")\nsuper().__init__()\nself.model = None\n# Example\na = Inference()\n\"\"\"Output\nsingleton called\nsingleton cls not in _instances\ninference init called\n\"\"\"\nb = Inference()\n# singleton called\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Singleton%20example%205717e07575f3446fb29b68aaa35cb21f/#use-_-_-new-_-_","title":"Use <code>_ _ new _ _</code>","text":"<pre><code>class InferenceHelper(object):\n_instance = None\ndef __new__(cls, *args, **kwargs):\n\"\"\"A singleton behavior\n        Returns:\n            InferenceHelper: The instance of InferenceHelper\n        \"\"\"\nif cls._instance is None:\nlogger.info('Initialize a new instance of InferenceHelper.')\ncls._instance = super(InferenceHelper, cls).__new__(cls)\ncls._instance._instace_init(*args, **kwargs)\nelif cls._instance._any_new_config(**kwargs):\n# Only do it when there is different config(s)\ncls._instance._instace_init(*args, **kwargs)\nreturn cls._instance\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Singleton%20example%205717e07575f3446fb29b68aaa35cb21f/#use-self-defined-get_instance","title":"Use self-defined <code>get_instance</code>","text":"<pre><code>class InferenceHelper:\n__instance = None\n@staticmethod\ndef get_instance(*args, **kwargs):\n\"\"\"A singleton behavior\n        Returns:\n            InferenceHelper: The instance of InferenceHelper\n        \"\"\"\nif InferenceHelper.__instance is None:\nlogger.info('Initialize a new instance of InferenceHelper.')\nInferenceHelper(*args, **kwargs)\nreturn InferenceHelper.__instance\ndef __init__(self, *args,  **kwargs):\nif InferenceHelper.__instance is not None:\nraise Exception(\"You cannot manually create an instance of InferenceHelper! Call the get_instance instead\")\nelse:\n# Do init job here....\npass\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/String%20Manipulation%20124f438d6a31491d8dad3fc0302eca85/","title":"String Manipulation","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/String%20Manipulation%20124f438d6a31491d8dad3fc0302eca85/#regex","title":"Regex","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/String%20Manipulation%20124f438d6a31491d8dad3fc0302eca85/#common-pattern","title":"Common pattern","text":"<pre><code>r'_\\d+$' # e.g. xxxxx_12\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/String%20Manipulation%20124f438d6a31491d8dad3fc0302eca85/#replace-pattern-with-xxx","title":"Replace pattern with xxx:","text":"<pre><code>import re\ntarget_text = \"hahahahha_111\"\nre.sub(r'_\\d+$', '', target_text)\n# hahahahha\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/String%20Manipulation%20124f438d6a31491d8dad3fc0302eca85/#search-pattern","title":"Search pattern:","text":"<pre><code>import re\ntarget_text = \"hahahah_111\"\nm = re.search(r'_\\d+$',target_text)\nif m is not None:\nget_str = m.group()\nprint(get_str)\nelse:\nprint(\"no match.\")\n# output: _111\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Text%20similarity%20comparison%20763b399088364bafb235c035a994c357/","title":"Text similarity/comparison","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Text%20similarity%20comparison%20763b399088364bafb235c035a994c357/#native-pacakage-sequencematcher","title":"Native pacakage - SequenceMatcher","text":"<p>https://docs.python.org/3/library/difflib.html</p> <p>There are are many are different string metrics like\u00a0Levenshtein ,\u00a0Damerau-Levenshtein ,\u00a0Hamming distance ,\u00a0Jaro-Winkler \u00a0and\u00a0Strike a match .</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Text%20similarity%20comparison%20763b399088364bafb235c035a994c357/#levenshtein","title":"Levenshtein","text":"<ul> <li>much faster than sequenceMatcher</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Text%20similarity%20comparison%20763b399088364bafb235c035a994c357/#locality-sensitive-hashing","title":"Locality-sensitive hashing","text":"<ul> <li>https://towardsdatascience.com/understanding-locality-sensitive-hashing-49f6d1f6134</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Text%20similarity%20comparison%20763b399088364bafb235c035a994c357/#elasticsearch","title":"Elasticsearch","text":"<ul> <li>It supports fuzzy search - which calculates Levenshtein distance<ul> <li>https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html</li> </ul> </li> <li>Text similarity search with vectors<ul> <li>https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch</li> </ul> </li> <li>Advanced usage - Text similarity with TF models and Elastic search<ul> <li>https://www.ulam.io/blog/text-similarity-search-using-elasticsearch-and-python<ul> <li>Not training a model, but use a pre-trained model to get text embeddings</li> </ul> </li> </ul> </li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Unit%20test%205ad1d2c5e57f4171918ce699c597bd91/","title":"Unit test","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Unit%20test%205ad1d2c5e57f4171918ce699c597bd91/#pytest","title":"Pytest","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Unit%20test%205ad1d2c5e57f4171918ce699c597bd91/#unittest-python-native-module","title":"Unittest Python native module","text":"<p>https://docs.python.org/3/library/unittest.mock.html</p> <p>very useful because you can easily replace the module call within the test target</p> <p>e.g. you can pretend a module/methods return value within the test target, without the need to modify it. </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Unit%20test%205ad1d2c5e57f4171918ce699c597bd91/#setup","title":"Setup","text":"<ol> <li>Better to start with this structure. A separated test folders</li> <li>~~Create an empty <code>__init__.py</code> in <code>proj_root/tests/</code>~~<ol> <li>~~Now you can run pytest and import the src files correctly~~</li> <li>~~BUT, the <code>import</code> inside src files, that you imported, still not work.~~</li> </ol> </li> <li> <p>Create a file <code>[conftest.py](http://conftest.py)</code> in <code>proj_root/tests/</code> with below content:</p> <pre><code>import os\nimport sys\nsys.path.append(\nos.path.abspath(os.path.dirname(os.path.abspath(__file__)) + \"/../src/\")\n)\n</code></pre> <p>So the src folder is also added to sys path</p> </li> </ol>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/","title":"Visualization - plotting","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#common-setup","title":"Common setup","text":"<p>It is common to setup in the beginning of a notebook:</p> <pre><code>%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#background-color","title":"Background color :","text":"<pre><code>fig = plt.figure(1)\nfig.patch.set_facecolor('white')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#title-label","title":"Title, Label","text":"<pre><code>plt.ylabel('log of fqz')\nplt.xlabel('channel val')\nplt.title(title)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#take-logarithm-of-a-axis","title":"Take logarithm of a axis","text":"<pre><code>plt.yscale(\"log\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#subplot-related","title":"Subplot related","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#subplot-in-short","title":"Subplot in short:","text":"<pre><code>fig = plt.figure(1)\n# 211 = 2 row, 1 col, i-th\nplt.subplot(211)\nplt.subplot(212)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#align-x-y-axis-of-subplots","title":"Align x, y axis of subplots:","text":"<pre><code>x = np.min(all_x_data)\ny = np.max(all_y_data)\nplt.ylim(x, y)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#spacing-between-subplots","title":"Spacing between subplots:","text":"<pre><code># set better padding\nplt.subplots_adjust(left=0.1,\nbottom=0.1,\nright=0.9,\ntop=0.9,\nwspace=0.4,\nhspace=0.4)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#advanced-issues","title":"Advanced Issues","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#matplotlib-not-showing-images-in-vscode-notebook","title":"Matplotlib not showing images in vscode / notebook :","text":"<p>In vscode jupyter / normal jupyter notebook, sometimes matplotlib cannot draw image.</p> <p>Try to put one of below in the 1st line of notebook</p> <p>Ref: https://github.com/matplotlib/matplotlib/issues/14534</p> <pre><code># %matplotlib inline - Figures are shown as static png images (optionally svg if configured)\n# %matplotlib notebook or %matplotlib nbagg - Interactive Figures inside the notebook\n# %matplotlib widgets - - Interactive Figures inside the notebook (requires jupyter-matplotlib to be installed)\n# %matplotlib tk or %matplotlib qt etc. - GUI windows show the figure externally to the notebook with the given interactive backend\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#pandas-plotting","title":"Pandas plotting","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#specific-case-visualize-tf-modelfit-history-object","title":"Specific case - visualize TF <code>model.fit</code> history object:","text":"<pre><code># The history is from model.fit like this:\n# history = conv_model.fit(xxxx)\n# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n# model was trained on. \ndf_loss_acc = pd.DataFrame(history.history)\ndf_loss= df_loss_acc[['loss','val_loss']]\ndf_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\ndf_acc= df_loss_acc[['accuracy','val_accuracy']]\ndf_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\ndf_loss.plot(title='Model loss',figsize=(12,8)).set(xlabel='Epoch',ylabel='Loss')\ndf_acc.plot(title='Model Accuracy',figsize=(12,8)).set(xlabel='Epoch',ylabel='Accuracy')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#image","title":"Image","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#store-the-result-into-image","title":"Store the result into image","text":"<pre><code>plt.savefig(\"/xxxxxx/plt_result.png\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#draw-the-image-with-a-numpy-2d-matrix","title":"Draw the image with a numpy 2d matrix:","text":"<p>(it represents an image, each value is between 0,255 (Grey scale)) </p> <pre><code># e.g.  train_images.shape : (100, 100)\n# Each value is 0~255 (because grey scale)\nplt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#plot-multiple-images-in-a-grid","title":"Plot multiple images in a grid:","text":"<pre><code># import numpy as np\n# images_list = np.random.rand(20, 64, 64, 3) * 255\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 10))\nfor i, img in enumerate(images_list):\nax = plt.subplot(5, 5, i + 1)\nplt.imshow(img.astype(\"int\"))\n# Optional\nplt.title(label_list)\nplt.axis(\"off\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#general-plotting","title":"General plotting","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#plot-cost-during-training-1-line","title":"Plot Cost during training (1-line):","text":"<pre><code># Plot the cost\nplt.plot(np.squeeze(costs))\nplt.ylabel('cost')\nplt.xlabel('iterations (per fives)')\nplt.title(\"Learning rate =\" + str(0.0001))\nplt.show()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#other-type-of-plotting","title":"Other type of plotting:","text":"<pre><code># Scatter\nplt.scatter(x, y, label = \"something\", c=\"red\")\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#plot-multiple-accuracy-chart","title":"Plot multiple accuracy chart:","text":"<pre><code># figure 1\nplt.figure(1)\n# subplot of 2 col &amp; 1 row &amp; 1st position\nplt.subplot(211)\n# Plot the train accuracy\nplt.plot(np.squeeze(train_acc))\nplt.ylabel('Train Accuracy')\nplt.xlabel('iterations (per fives)')\nplt.title(\"Learning rate =\" + str(0.0001))\n# subplot of 2 col &amp; 1 row &amp; 1st position\nplt.subplot(212)\n# Plot the test accuracy\nplt.plot(np.squeeze(test_acc))\nplt.ylabel('Test Accuracy')\nplt.xlabel('iterations (per fives)')\nplt.title(\"Learning rate =\" + str(0.0001))\nplt.show()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#plot-multiple-line","title":"Plot multiple line:","text":"<pre><code># create data\nx = [10,20,30,40,50]\ny = [30,30,30,30,30]\n# plot lines\nplt.plot(x, y, label = \"line 1\")\nplt.plot(y, x, label = \"line 2\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#plot-decision-boundary","title":"Plot decision boundary:","text":"<p>https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb </p> <p>It is very useful and common during Coursera courses</p> <pre><code># Helper function to plot a decision boundary.\n# If you don't fully understand this function don't worry, it just generates the contour plot below.\ndef plot_decision_boundary(pred_func):\n# Set min and max values and give it some padding\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nh = 0.01\n# Generate a grid of points with distance h between them\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n# Predict the function value for the whole gid\nZ = pred_func(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n# Plot the contour and training examples\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#3d-plotting","title":"3D plotting","text":"<p>Ref to this post - https://towardsdatascience.com/principal-component-analysis-pca-explained-visually-with-zero-math-1cbf392b9e7d</p> <pre><code>import plotly.express as px\nimport pandas as pd\n# df['cluster_label'] is like 0,1,2\n\"\"\"df looks like:\nx0  x1  x2  cluster_label\n-0.366353   1.022466    1.166899    2\n-1.179214   1.318905    1.047407    2\n0.346441    -1.360488   -0.417740   1\n\"\"\"\n# Visualize our data\ncolors = px.colors.sequential.Plasma\ncolors[0], colors[1], colors[2] = ['red', 'green', 'blue']\nfig = px.scatter_3d(df, x='x0', y='x1', z='x2', color=df['cluster_label'].astype(str), color_discrete_sequence=colors, height=500, width=1000)\nfig.update_layout(showlegend=False,\nscene_camera=dict(up=dict(x=0, y=0, z=1), \ncenter=dict(x=0, y=0, z=-0.1),\neye=dict(x=1.5, y=-1.4, z=0.5)),\nmargin=dict(l=0, r=0, b=0, t=0),\nscene=dict(xaxis=dict(backgroundcolor='white',\ncolor='black',\ngridcolor='#f0f0f0',\ntitle_font=dict(size=10),\ntickfont=dict(size=10)),\nyaxis=dict(backgroundcolor='white',\ncolor='black',\ngridcolor='#f0f0f0',\ntitle_font=dict(size=10),\ntickfont=dict(size=10)),\nzaxis=dict(backgroundcolor='lightgrey',\ncolor='black', \ngridcolor='#f0f0f0',\ntitle_font=dict(size=10),\ntickfont=dict(size=10))))\nfig.update_traces(marker=dict(size=3, line=dict(color='black', width=0.1)))\nfig.show()\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#seaborn","title":"Seaborn","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#setup-the-color-tone","title":"Setup the color tone","text":"<pre><code>import seaborn as sns\nPALETTE=['lightcoral', 'lightskyblue', 'gold', 'sandybrown', 'navajowhite',\n'khaki', 'lightslategrey', 'turquoise', 'rosybrown', 'thistle', 'pink']\nsns.set_palette(PALETTE)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Visualization%20-%20plotting%20a3c73bacb92948debeed202afd709466/#scatter-plot","title":"Scatter plot:","text":"<pre><code>plt.figure(figsize=(16,10))\nsns.scatterplot(\nx=\"tsne-2d-one\", y=\"tsne-2d-two\",\nhue=\"y\",\npalette=sns.color_palette(\"hls\", 10),\ndata=df_subset,\nlegend=\"full\",\nalpha=0.3\n)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Vscode%20Setup%2027a11191717743c9b9551954c9e5974b/","title":"Vscode Setup","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Vscode%20Setup%2027a11191717743c9b9551954c9e5974b/#python-debug-config","title":"Python debug config","text":"<p>Assume the module name is <code>schema</code>, this configuration allow the debug to recognize relative import correctly</p> <pre><code>{\n\"name\": \"Python: Module\",\n\"type\": \"python\",\n\"request\": \"launch\",\n\"module\": \"schema.${fileBasenameNoExtension}\",\n\"justMyCode\": true,\n\"envFile\": \"${workspaceFolder}/.env\"\n},\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Web%20related%202ea4054551364eefa3873b41cc4dbff4/","title":"Web related","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Web%20related%202ea4054551364eefa3873b41cc4dbff4/#deployment","title":"Deployment","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Web%20related%202ea4054551364eefa3873b41cc4dbff4/#gunicorn","title":"Gunicorn","text":"<ul> <li>Act as a process manager, manage multiple worker process to leverage multi core CPU</li> <li>it starts 1 or more worker process<ul> <li>e.g. Uvicorn worker process class</li> </ul> </li> <li>i.e. Gunicorn listen to the port &amp; ip \u2192 pass the message to the worker process</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/","title":"CSV file","text":"<p>You can read CSV directly with numpy, pandas, or native csv module, which depends on the usage </p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/#native-module-csv","title":"Native module: <code>csv</code>","text":"<ul> <li>Useful if wanna read row by row</li> </ul>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/#by-csvreader","title":"By csv.reader","text":"<ul> <li>Read each row as list</li> </ul> <pre><code>from csv import reader\npath = \"some.csv\"\n# open file in read mode\nwith open(path, 'r') as read_obj:\n# pass the file object to reader() to get the reader object\ncsv_reader = reader(read_obj)\nheader = next(csv_reader)\nprint(header)\n# Check file as empty\nif header != None:\n# Iterate over each row in the csv using reader object\nfor row in csv_reader:\n# row variable is a list that represents a row in csv\nprint(row)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/#by-csvdictreader","title":"By csv.DictReader","text":"<ul> <li>Read each row as a dict, with the csv header as dict keys</li> </ul> <pre><code>from csv import DictReader\npath = \"some.csv\"\n# open file in read mode\nwith open(path, 'r') as read_obj:\n# pass the file object to DictReader() to get the DictReader object\ncsv_dict_reader = DictReader(read_obj)\n# iterate over each line as a ordered dictionary\nfor row in csv_dict_reader:\n# row variable is a dictionary that represents a row in csv\nprint(row)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/#by-pandas","title":"By Pandas","text":"<ul> <li>Good if the subsequent tasks are Panads involved</li> <li>https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html</li> </ul> <pre><code>import pandas as pd\npd.read_csv('data.csv')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/CSV%20file%20e6500676a9c94f458abd847b626fdf5b/#by-numpy","title":"By numpy","text":"<ul> <li>Not common</li> <li>because difficult to set each field type</li> <li>https://numpy.org/doc/stable/user/basics.io.genfromtxt.html?highlight=csv</li> </ul> <pre><code>from numpy import genfromtxt\nmy_data = genfromtxt('my_file.csv', delimiter=',')\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/H5PY%207268064783ad44a4a47c8c83cb50323a/","title":"H5PY","text":"<p>It lets you store huge amounts of numerical data, and easily manipulate that data from NumPy. For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays. Thousands of datasets can be stored in a single file, categorized and tagged however you want. </p> <p>It is designed for flexible and efficient I/O and for high volume and complex data.</p> <p>https://github.com/h5py/h5py</p>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/PDF%20skills%205aeec149822c4d7c827cf1073f234972/","title":"PDF skills","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/PDF%20skills%205aeec149822c4d7c827cf1073f234972/#convert-pdf-to-image","title":"Convert PDF to image","text":"<pre><code>from pathlib import Path\nimport numpy as np\nimport cv2\nfrom pdf2image import convert_from_path\ndef pdf_to_image(file_path: Path, dpi: int):\ndef pil2cv(image):\n\"\"\" PIL -&gt; OpenCV \"\"\"\nimage_as_nparray = np.array(image, dtype=np.uint8)\nif image_as_nparray.ndim == 2:\npass\nelif image_as_nparray.shape[2] == 3:\nimage_as_nparray = cv2.cvtColor(image_as_nparray, cv2.COLOR_RGB2BGR)\nelif image_as_nparray.shape[2] == 4: \nimage_as_nparray = cv2.cvtColor(image_as_nparray, cv2.COLOR_RGBA2BGRA)\nreturn image_as_nparray\nlist_pil_images = convert_from_path(\nfile_path,\ndpi=dpi,\ngrayscale=True\n)\nlist_cv2_images = [pil2cv(pil_image) for pil_image in list_pil_images]\nreturn list_cv2_images\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/json%20file%20a3d18ba875af4ba19aaa5277ba9c78be/","title":"json file","text":""},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/json%20file%20a3d18ba875af4ba19aaa5277ba9c78be/#write-to-json","title":"Write to json","text":"<pre><code>import json\nimport os\njson_file_content = {\"test\": 11111}\njson_path = \"/aaaa/bbb/aaa.json\"\n# In case folder not exist...\nos.makedirs(os.path.dirname(json_path), exist_ok=True)\nwith open(json_path, 'w') as outfile:\njson.dump(json_file_content, outfile, ensure_ascii=False, indent=4)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/json%20file%20a3d18ba875af4ba19aaa5277ba9c78be/#read-from-json","title":"Read from json","text":"<pre><code>import json\nf_path = \"/xxx/xxx/xxx/wdw.json\"\nwith open(f_path) as f:\njson_obj = json.load(f)\n</code></pre>"},{"location":"Tech%20577253fab98740dc8047494b3ba29d2c/Python%200a10a99cd01b4ebbba143b2176835b8c/Path%20files%20related%204ba1501b734c4d88a6d3d21eab820d15/npz%20file%206ef3ea46fde447bc8e9fa5fec75e6486/","title":"npz file","text":"<pre><code>import os\nimport numpy as np\nfile_path = \"./xxxx.npz\"\n# Check existence\nif not os.path.exists(file_path):\nraise ValueError(\"File {} doesn't exist.\".format(file_path))\n## allow_pickle=True is needed, for accessing the elements\nwith  np.load(file_path, allow_pickle=True) as data:\nprint(\"Keys in npz file: \", data.files)\nx = data[\"x\"]\ny = data[\"y\"]\n</code></pre>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Daily%20Check%2058f97c83ff304bc7aded237d81c6429d/","title":"Daily Check","text":""},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Daily%20Check%2058f97c83ff304bc7aded237d81c6429d/#ml-related","title":"ML Related:","text":"<ul> <li>[Speed] Daily 1 question &amp; its explanation, very very interesting<ul> <li>https://today.bnomial.com/</li> </ul> </li> <li>Keep up the Coursera progress<ul> <li>ML specialization - https://www.coursera.org/learn/machine-learning/home/welcome</li> <li>DL specialization</li> </ul> </li> <li>Read 1 algorithm / page  / architecture<ul> <li>from scikit learn - https://scikit-learn.org/stable/user_guide.html</li> <li>Popular models architecture (CNN, RNN, \u2026etc)</li> </ul> </li> <li>Extra time - try Kaggle<ul> <li>read at least 1 notebook</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Pending%20material%20db2a0fefdcce4afeaeb1cae3f5aba78b/","title":"Pending material","text":""},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Pending%20material%20db2a0fefdcce4afeaeb1cae3f5aba78b/#long","title":"Long","text":"<ul> <li>LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale (2022)<ul> <li>goal - quantized the model to speed up large model (with drawback)</li> </ul> </li> <li>Neural Architectures for Named Entity Recognition(2016)</li> <li>Representation Learning for Information Extraction from Form-like Documents(2020)</li> <li>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE (2021)</li> <li>DETR : https://arxiv.org/abs/2005.12872 (2020)</li> <li>Machine learning type - Online learning - https://arxiv.org/abs/1802.02871 (2018)</li> <li>Paper to explain gradient descent, optimizer https://arxiv.org/pdf/1609.04747.pdf  (2017)</li> <li>Mercari MLOps - https://youtu.be/3fo5YyRqRII?t=570</li> <li>Minecraft VPT training - https://arxiv.org/abs/2206.11795 (2022)</li> <li>Check what topic of statistic need to be studied<ul> <li>from Chou : \u521d\u7b49\u7d71\u8a08\u5b78 \u62bd\u6a23\u8abf\u67e5 \u8ff4\u6b78\u5206\u6790</li> </ul> </li> <li>ViTDet - https://arxiv.org/abs/2203.16527 (2022)<ul> <li>https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet</li> </ul> </li> <li>*Imagic: Text-Based Real Image Editing with Diffusion Models* (2022)<ul> <li>https://arxiv.org/abs/2210.09276</li> <li>example - https://twitter.com/_akhaliq/status/1582175757153230849</li> </ul> </li> <li>Donut<ul> <li>https://github.com/clovaai/donut</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Pending%20material%20db2a0fefdcce4afeaeb1cae3f5aba78b/#short","title":"Short","text":"<ul> <li>Federated Learning<ul> <li>https://pair.withgoogle.com/explorables/federated-learning/</li> </ul> </li> <li>How large the data size should be - https://www.qualtrics.com/au/experience-management/research/determine-sample-size/?rid=ip&amp;prevsite=en&amp;newsite=au&amp;geo=JP&amp;geomatch=au</li> <li>GPT3 - https://openai.com/blog/gpt-3-apps/</li> <li>Image processing short tutorials<ul> <li>https://datacarpentry.org/image-processing/</li> </ul> </li> <li>metric learning https://towardsdatascience.com/the-why-and-the-how-of-deep-metric-learning-e70e16e199c0</li> <li>sklearn - stratify - https://scikit-learn.org/stable/modules/cross_validation.html#stratification</li> <li>Model architecture<ul> <li>CNN, Rnn, GAN</li> </ul> </li> <li>ViT (vision transformer)</li> <li>Line OCR source code<ul> <li>https://github.com/clovaai/donut</li> </ul> </li> <li>Diff - normalize vs standardize vs regularization<ul> <li>Regularization<ul> <li>Regularization is a way to avoid model from overfitting. It has different kind of techniques, such as L2, dropout\u2026etc https://www.reddit.com/r/learnmachinelearning/comments/w7yrog/what_regularization_does_to_a_machine_learning/?utm_medium=android_app&amp;utm_source=share</li> </ul> </li> </ul> </li> <li>adversarial validation</li> <li>A guy got 2nd on kaggle competition - step by step guide - https://twitter.com/marktenenholtz/status/1539578965920083968</li> <li>MLFlows \u2190 check how to use it</li> <li>wandb \u2190 check this</li> <li>weak supervision learning ?</li> <li>inductive learning<ul> <li>refers to a learning algorithm that learns from labeled training data and generalizes to new data, such as a test dataset.</li> </ul> </li> <li>transductive learning<ul> <li>refers to learning from labeled training data and generalizing to available unlabeled (training) data.</li> <li>https://machinelearningmastery.com/transduction-in-machine-learning/</li> </ul> </li> <li>MeanShift<ul> <li>the 2nd section here - https://www.christopherlovell.co.uk/blog/2016/07/04/image-pca-deckchair.html</li> </ul> </li> <li>Semi supervised learning example on mincraft game - https://openai.com/blog/vpt/<ul> <li>Check its github src - https://github.com/openai/Video-Pre-Training</li> <li>Video pretraining (vpt)</li> <li>Inverse dynamics model (idm)</li> </ul> </li> <li>Non maximum suppression ( CV)</li> <li>Gpu puzzle - an interactive notebook to learn gpu programming - https://github.com/srush/GPU-Puzzles</li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/","title":"Reading Material","text":""},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#book","title":"Book \ud83d\udcd8","text":"<p>Some thoughts: Prefer to read books of general knowledge, instead of introducing a specific framework/library. </p> <ul> <li>List of free ML book, grouped by topics, domains<ul> <li>https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md</li> </ul> </li> <li>Free ebook from https://machinelearningmastery.com/<ul> <li>the blog that daily bnomial always quote</li> </ul> </li> <li> <p>Python data science - https://www.oreilly.com/library/view/python-data-science/9781491912126/</p> </li> <li> <p>Deep Learning theory explanation - https://deeplearningtheory.com/</p> <ul> <li>Free book on arxiv</li> <li>Mainly maths</li> </ul> </li> </ul> <p>More topic specific :</p> <ul> <li>Semi-supervised learning<ul> <li>Semi-Supervised Learning, 2006.</li> <li>Introduction to Semi-Supervised Learning, 2009.</li> <li>Semi-Supervised Learning: Background, Applications and Future Directions, 2018.</li> <li>Graph-Based Semi-Supervised Learning, 2014.</li> </ul> </li> <li>Pattern Recognition and Machine Learning (PRML), 2006<ul> <li>https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/</li> <li>https://github.com/gerdm/prml</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#event","title":"Event \ud83c\udf03","text":"<ul> <li>Meet up events, many things to share<ul> <li>https://www.meetup.com/Machine-Learning-Tokyo/</li> </ul> </li> <li>MLops sharing of real company<ul> <li>https://mlops.connpass.com/</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#video-blog","title":"Video Blog \ud83d\udcf9","text":"<ul> <li>Google research youtube - https://www.youtube.com/c/GoogleResearch</li> <li>Google developer - https://www.youtube.com/googlecode</li> <li>Datacamp - https://www.youtube.com/c/Datacamp</li> <li>Full Stack Deep Learning Youtube channel - https://www.youtube.com/c/fullstackdeeplearning</li> <li>PyTorch youtube<ul> <li>https://www.youtube.com/c/PyTorch</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#regular-updated-blog","title":"Regular Updated Blog \ud83d\udcdd","text":"<ul> <li>bnomial blog<ul> <li>https://machinelearningmastery.com/blog/</li> </ul> </li> <li>Karpathy ML blog<ul> <li>http://karpathy.github.io/</li> </ul> </li> <li>Neptune blog<ul> <li>https://neptune.ai/blog</li> </ul> </li> <li>MLops blog<ul> <li>https://data.gunosy.io/entry/mlops</li> </ul> </li> <li>AWS MLU (ML University)<ul> <li>https://mlu-explain.github.io/</li> </ul> </li> <li>Open ai blog - https://openai.com/blog/</li> <li>Facebook ai blog - https://ai.facebook.com/</li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#newsletter","title":"Newsletter","text":"<ul> <li>https://read.deeplearning.ai/the-batch/</li> <li>https://dataelixir.com/</li> <li>https://www.deeplearningweekly.com/</li> <li>Reddit (r/MachineLearning)</li> <li>Twitter (following data scientists, engineers, project managers, etc)</li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#resource-lists","title":"Resource Lists :","text":"<ul> <li>List of ML blogs<ul> <li>https://github.com/josephmisiti/awesome-machine-learning/blob/master/blogs.md</li> <li>https://github.com/ujjwalkarn/Machine-Learning-Tutorials#blogs</li> </ul> </li> <li>List of resources<ul> <li>Machine learning resource Awesome list<ul> <li>https://github.com/ujjwalkarn/Machine-Learning-Tutorials#other</li> <li>machine learning and deep learning tutorials, articles and other resources</li> </ul> </li> <li>Python DataScience list<ul> <li>https://github.com/ujjwalkarn/DataSciencePython</li> <li>Generic algorithms</li> <li>common data analysis and machine learning tasks using python</li> </ul> </li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#tutorial-doc","title":"Tutorial / Doc","text":"<ul> <li> <p>The ML landscape</p> <p></p> </li> <li> <p>The Maths aspect in ML (image from reddit)</p> <p></p> </li> <li> <p>Recommended - By Jason Brownlee PhD - guide posts by various ML path</p> <ul> <li>https://machinelearningmastery.com/start-here/#getstarted</li> <li>PS: this platform <code>bnomial</code> always quote this blog</li> </ul> </li> <li>Recommended - 100 ML lectures(notes) in University of British Columbia<ul> <li>Author claims : It is meant to be followed in order</li> <li>Good roadmap! Recommended</li> <li>https://www.cs.ubc.ca/~schmidtm/Courses/LecturesOnML/</li> <li>Or download the all-included PDF in my drive (https://drive.google.com/file/d/1r-L2Bsazi13n_v8iMgtcaZ0b69ycg6Yo/view?usp=sharing) Note: private access</li> </ul> </li> <li>Recommended - kaggle course<ul> <li>https://www.kaggle.com/learn</li> <li>Text course, reading friendly</li> <li>E.g. ML, python</li> <li>Can get certificate</li> </ul> </li> <li>Recommended - Dive into Deep learning - https://d2l.ai/index.html<ul> <li>Super great interactive DL book with notebbok</li> </ul> </li> <li> <p>ML simple learning via drafting illustration</p> <ul> <li>https://illustrated-machine-learning.github.io/index.html</li> <li> <p>Like this:</p> <p></p> </li> </ul> </li> <li> <p>ML From Scratch - https://mlfromscratch.com/</p> <ul> <li>It is abandoned, so remove it after reading all interested posts</li> <li>Activation function - https://mlfromscratch.com/activation-functions-explained/</li> </ul> </li> <li>Scikit learn<ul> <li>Its user guide is too complex. But is a very great outline of algorithm that you should learn. Just use it as a target list.</li> </ul> </li> <li>Tensorflow - https://www.tensorflow.org/tutorials<ul> <li>Tensorflow training sample by using fashion MNiST.<ul> <li>https://www.tensorflow.org/tutorials/keras/classification</li> </ul> </li> <li>Strategy to prevent overfit or underfit<ul> <li>https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting</li> </ul> </li> <li>Optimize dataset for training<ul> <li>https://www.tensorflow.org/tutorials/keras/text_classification#configure_the_dataset_for_performance</li> <li>.cache - avoid store all data in memory, also faster than loadjng millions of small files</li> <li>.prefetch - allow to do preprocess while training at the same time.</li> </ul> </li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#video-lesson-course","title":"Video Lesson / Course","text":"<ul> <li>Coursera<ul> <li>Deep learning specialization (by deeplearning.ai) https://www.coursera.org/specializations/deep-learning</li> <li>GAN specialization (by deeplearning.ai) https://www.coursera.org/specializations/generative-adversarial-networks-gans</li> </ul> </li> <li>Codecademy - provide many project-wise lesson path, can build up portfolio seamlessly<ul> <li>https://www.codecademy.com/learn/paths/master-statistics-with-python</li> </ul> </li> <li>Fastai<ul> <li>Course, not the framework</li> <li>It's approach is from top level and learn practical first</li> </ul> </li> <li>Deeplearning ai<ul> <li>Opposite from fastai, start from low level like Maths, and then gradually deep dive to Tensorflow</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Reading%20Material%200344b3212c0d4e63b3fcaa27c346745c/#random-posts-pages","title":"Random Posts / Pages","text":"<ul> <li>MLCC Text Classification Guide<ul> <li>https://developers.google.com/machine-learning/guides/text-classification/</li> </ul> </li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Research%20Paper%204ad5ef86119b4fdebfe5c7f358d6d2b8/","title":"Research Paper","text":"<p>Note: Store the paper I read already. Put to Pending material if not read yet. </p> <p>A repo + Kaggle notebook explaining Key Concepts of various papers, e.g. BERT, Donut\u2026etc</p> <ul> <li>https://github.com/dair-ai/ML-Papers-Explained</li> </ul>"},{"location":"To%20Read%20548b5baeb46644c1b1aebb3b64440cb7/Research%20Paper%204ad5ef86119b4fdebfe5c7f358d6d2b8/#nlp","title":"NLP","text":"<ul> <li>Chargrid: Towards Understanding 2D Documents(2018)<ul> <li>Chargrid representation :<ul> <li>All characters are 1-hot encoded into a mapping integer</li> <li>i.e. the encoding is NOT contextualized</li> </ul> </li> <li>https://github.com/sciencefictionlab/chargrid-pytorch</li> </ul> </li> <li>BERTgrid: Contextualized Embedding for 2D Document Representation and Understanding(2019)<ul> <li>BERTgrid representation :<ul> <li>similar to Chargrid, but it is word level &amp; all words are embedded (an array of values)  instead of only representing the char as an integer.</li> <li>i.e. the embedding is contextualized</li> </ul> </li> </ul> </li> </ul>"}]}